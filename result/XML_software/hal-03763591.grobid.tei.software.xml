<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Study on Automatic 3D Facial Caricaturization: From Rules to Deep Learning</title>
				<funder ref="#_zYXaEcr">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_Tj56pz6">
					<orgName type="full">Association Nationale de la Recherche et de la Technologie</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
				<date type="published" when="2022-01-19">19 January 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Olivier</surname></persName>
							<email>nicolas.olivier@interdigital.com</email>
							<affiliation key="aff0">
								<orgName type="institution">InterDigital</orgName>
								<address>
									<settlement>Cesson-Sévigné</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Inria</orgName>
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">IRISA</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Glenn</forename><surname>Kerbiriou</surname></persName>
							<email>glenn.kerbiriou@interdigital.com</email>
							<affiliation key="aff0">
								<orgName type="institution">InterDigital</orgName>
								<address>
									<settlement>Cesson-Sévigné</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institut national</orgName>
								<orgName type="institution">sciences appliquées de Rennes</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ferran</forename><surname>Arguelaguet</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Inria</orgName>
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">IRISA</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quentin</forename><surname>Avril</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">InterDigital</orgName>
								<address>
									<settlement>Cesson-Sévigné</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabien</forename><surname>Danieau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">InterDigital</orgName>
								<address>
									<settlement>Cesson-Sévigné</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philippe</forename><surname>Guillotel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">InterDigital</orgName>
								<address>
									<settlement>Cesson-Sévigné</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ludovic</forename><surname>Hoyet</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Inria</orgName>
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">IRISA</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Franck</forename><surname>Multon</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Inria</orgName>
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">IRISA</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Santé (M2S)</orgName>
								<orgName type="institution">Laboratoire Mouvement, Sport</orgName>
								<address>
									<settlement>Bruz</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<settlement>Google</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Osaka University</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">John Dingliana</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Study on Automatic 3D Facial Caricaturization: From Rules to Deep Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-19">19 January 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">AF66153225EE8A7F2404300E86FCE20C</idno>
					<idno type="DOI">10.3389/frvir.2021.785104</idno>
					<note type="submission">This article was submitted to Technologies for VR, a section of the journal Frontiers in Virtual Reality Received: 28 September 2021 Accepted: 06 December 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-07T09:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>caricature</term>
					<term>style transfer</term>
					<term>machine learning</term>
					<term>geometry processing</term>
					<term>3D mesh</term>
					<term>perceptual study</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>Facial caricature is the art of drawing faces in an exaggerated way to convey emotions such as humor or sarcasm. Automatic caricaturization has been explored both in the 2D and 3D domain. In this paper, we propose two novel approaches to automatically caricaturize input facial scans, filling gaps in the literature in terms of user-control, caricature style transfer, and exploring the use of deep learning for 3D mesh caricaturization. The first approach is a gradient-based differential deformation approach with data driven stylization. It is a combination of two deformation processes: facial curvature and proportions exaggeration. The second approach is a GAN for unpaired face-scan-to-3D-caricature translation. We leverage existing facial and caricature datasets, along with recent domain-to-domain translation methods and 3D convolutional operators, to learn to caricaturize 3D facial scans in an unsupervised way. To evaluate and compare these two novel approaches with the state of the art, we conducted the first user study of facial mesh caricaturization techniques, with 49 participants. It highlights the subjectivity of the caricature perception and the complementarity of the methods. Finally, we provide insights for automatically generating caricaturized 3D facial mesh.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">INTRODUCTION</head><p>Caricatures have been used for centuries to convey humor or sarcasm. References can be found during the Antiquity with Aristotle referring to these artists as "grotesque," or in the works of Leonardo Da Vinci who was eagerly looking for people with deformities to use as models. Caricature can be defined as the art of drawing persons (usually faces) in a simplified or exaggerated way through sketching, pencil strokes, or other artistic drawings. Caricatures have been commonly used to entertain people, to laugh at politics or as a gift or souvenir sketched by street artists. These artists have the ability to capture distinct facial features, and then exaggerate those features <ref type="bibr" target="#b42">(Redman, 1984)</ref>. With the development of social VR networks or games, users may wish to use stylized avatars, including avatars preserving their identity <ref type="bibr" target="#b39">(Olivier et al., 2020)</ref> but with such exaggerated features. Hence, automatically generating such caricatured avatars becomes a key issue, as having artists manually creating caricatured avatars would not be feasible for such applications involving large numbers of users. Let us consider a 3D mesh representing the user's face (either using 3D scanning or computer vision methods to build 3D shape from a minimum set of images). An automatic caricature system should maintain the relative geometric location of facial components, while emphasizing the subject's facial features distinct from others. While different caricature experts would generate different styles of faces (more or less cartoonish style for example), they would all be exaggerating facial traits of the individual <ref type="bibr" target="#b6">(Brennan, 1985;</ref><ref type="bibr" target="#b29">Liang et al., 2002;</ref><ref type="bibr" target="#b37">Mo et al., 2004)</ref>. The ability of creating a variety of plausible caricatures for each single face is therefore a key challenge when automatically generating caricatures, as different artists would create visually different caricatures, which should also be taken into account when evaluating the subjective quality of the results.</p><p>Previous works for the generation of 3D caricatures can be separated into two main families: interactive and automatic methods. Interactive methods offer tools to caricature experts to design the resulting caricature <ref type="bibr" target="#b0">(Akleman, 1997;</ref><ref type="bibr" target="#b1">Akleman et al., 2000;</ref><ref type="bibr" target="#b9">Chen et al., 2002;</ref><ref type="bibr" target="#b19">Gooch et al., 2004)</ref>, while fully automatic methods use hand-crafted rules <ref type="bibr" target="#b6">(Brennan, 1985;</ref><ref type="bibr" target="#b29">Liang et al., 2002;</ref><ref type="bibr" target="#b37">Mo et al., 2004)</ref>, often derived from the drawing procedure of artists. However, these approaches are typically restricted to a particular artistic style, e.g., sketch or a certain cartoon, and predefined templates of exaggeration. From the works in the literature in other domains, two different solutions could be envisioned to automatically generate caricatures. First, in the context of exaggerating distinct features, <ref type="bibr" target="#b43">Sela et al. (2015)</ref> proposed a generic method to exaggerate the differences between the 3D scan of an object and an average template model of such type of object. However, this method has never been formally evaluated for human faces. Second, deep learning methods could be considered. As mentioned above, automatic methods mainly use hand-crafted rules that may fail to capture some complex choices made by caricature experts. In contrast, generative adversarial networks (GANs) are a promising mean to attempt to learn these choices based on a set of examples made by experts, without being limited to hand-crafted rules, but it has been never applied for the generation of 3D caricatures. The main goal of this paper is to propose and evaluate novel methods for the automatic generation of 3D caricatures from real 3D facial scans, first with a rule-based method, in order to keep tunable and interpretable parameters, and a deep learning method, to leverage real caricature data and hence generate caricatures closer to real ones. The main hypotheses we wish to address in this paper are:</p><p>H1: the specialization of generic exaggeration methods for human faces should allow to produce convincing caricatures. To this end, we adapted the generic method proposed by <ref type="bibr" target="#b43">Sela et al. (2015)</ref> in order to generate caricatures by exaggerating facial features from a 3D face scan (see Figure <ref type="figure" target="#fig_0">1</ref>). This method has two main stages, one based on a curvature EDFM (Exaggerating the Difference From the Mean), and another based on a nearestneighbors search in a 3D caricature dataset, to apply the proportion exaggeration.</p><p>H2: deep learning should allow to overcome some of the limitations of rule-based methods by their ability to generalize based on a set of examples. Thus, we designed a method leveraging advances in the field of GAN-based style transfer, which has shown great success in the 2D domain, for instance on drawn caricatures <ref type="bibr" target="#b8">(Cao et al., 2019)</ref>.</p><p>H3: both methods should reach and overcome the state-ofthe-art results when trying to automatically generate caricatures from a human face 3D scan. To assess the advantages and disadvantages of the proposed methods, we conducted a perceptual study considering the base method proposed by <ref type="bibr" target="#b43">Sela et al. (2015)</ref> and an additional EDFM method <ref type="bibr" target="#b2">(Akleman and Reisch, 2004)</ref>.</p><p>The results of the study support hypotheses H1 and H2, as the perceptual study demonstrated no significant preference of the subjects for any of the tested methods, for the proposed human faces. Although this result shows that the two proposed methods reached state of the art performance (H3), the perceptual study did not show a clear winner, highlighting the difficulty to simulate and evaluate such artistic caricatures for which a large variety of styles and solutions exists. The remainder of the paper is structured as follows. First, Section 2 reviews the state of the art, and identifies the gaps between existing techniques. Section 3 and Section 4 present the proposed rule-based and deep learningbased caricaturization methods respectively. Then, Section 5 presents the perceptual evaluation of the proposed methods with state-of-the-art methods. Finally, we discuss the results and provide insights on the automatic caricature generation in Section 6. Computer assisted caricature generation has been a topic of interest for researchers since the beginning of Computer Graphics <ref type="bibr" target="#b6">(Brennan, 1985)</ref>. Typically, techniques from drawing guides, such as Redman's practical guide 1984) on how to draw caricatures, are exploited. This guide sets the fundamental rules of caricatures and proposes some concepts that are massively used. Among them, the "mean face assumption" implies the existence of an average face, and the process of "Exaggerating the Difference From the Mean" (EDFM) consists in emphasizing the features that make a person unique, i.e., different from the average face. Existing methods for automatic caricature generation split into two main categories: rule-based and learning-based methods.</p></div>
<div><head n="2.1">Rule-Based Methods</head><p>Rule-based methods use a priori known procedures to caricaturize a shape. They can be further divided into two branches depending if their domain of application is on human faces or other shapes.</p><p>Face rule-based methods follow caricature drawing guidelines (e.g., EDFM) to generate deformed faces with emphasized features. <ref type="bibr" target="#b6">Brennan (1985)</ref> first proposed an implementation of EDFM in two dimensions. They built an interactive system where a user can select facial feature points which are matched against the average feature points, then the distance between them is exaggerated. This algorithm was later extended by <ref type="bibr" target="#b2">Akleman and Reisch, 2004)</ref>. Their software relies on a low-level procedure which requires the user to decide whether the exaggeration of a feature increases likeness or not. In the same spirit, <ref type="bibr" target="#b17">Fujiwara et al. (2002)</ref> developed a piece of software named <software ContextAttributes="used">PICASSO</software> for automatic 3D caricature generation. They used a set of feature points to generate simplified 3D faces before performing EDFM. EDFM was also used by <ref type="bibr" target="#b4">Blanz and Vetter (1999)</ref> in an application example of their morphable model. They learn a principal component analysis (PCA) space from 200 3D textured faces. Their system allows caricature generation by increasing the distance to the statistical mean in terms of geometry and texture. Statistical dispersion has been taken into account by <ref type="bibr" target="#b37">Mo et al. (2004)</ref> who showed that features should be emphasized proportionally to their standard deviation to preserve likeness. <ref type="bibr" target="#b10">Chen et al. (2006)</ref> created 3D caricatures by fusing 2D caricatures generated using EDFM from different views. Redman's guide <ref type="bibr" target="#b42">(Redman, 1984)</ref> not only introduces EDFM but also high levels concepts such as the five head types (oval, triangular, squared, round and long) and the dissociation between local and global exaggeration. These concepts have been exploited by <ref type="bibr" target="#b34">Liu et al. (2012)</ref> to perform photo to 3D caricature translation. They applied EDFM with respect to the shape of the head (global scale) and to the distance ratios of a set of feature points (local scale). Face rule-based methods can generate a caricature from an input photograph or a 3D model but fail at reproducing artistic styles. Different caricaturists would make different caricatures from the same person. To avoid this issue, they usually provide user control at a relatively low-level of comprehension, which often requires artistic knowledge.</p><p>Non face specific rule-based methods rely on intrinsic or extracted features of geometrical shapes. They generalize the concept of caricature beyond the domain of human faces. <ref type="bibr" target="#b16">Eigensatz et al. (2008)</ref> developed a 3D shape editing technique based on principal curvatures manipulation. With no reference model, their method can enhance or reduce the sharpness of a 3D shape. The link between saliency and caricature has been explored by <ref type="bibr" target="#b13">Cimen et al. (2012)</ref>. They introduced a perceptual method for caricaturing 3D shapes based on their saliency using free form deformation technique. A computational approach for surface caricaturization has been presented by <ref type="bibr" target="#b43">Sela et al. (2015)</ref>. They locally scale the gradient field of a mesh by its absolute Gaussian curvature. A reference mesh can be provided to follow the EDFM rule, and the authors show that their method is invariant to isometries, i.e., invariant to poses. General shape rule-based methods can also caricature a 2D or 3D shape without any reference model. As they do not take into account any statistical information nor the concept of artistic style, they try to link low-level geometry information to high-level caricature concepts, e.g., the fact that the most salient area should be more exaggerated <ref type="bibr" target="#b13">(Cimen et al., 2012)</ref>. As a result, they do not take into account the semantic of faces nor the art of human face caricature.</p><p>Since this work only tackles human face caricaturization, we refer to "face rule-based methods" as simply "rule-based methods".</p></div>
<div><head n="2.2">Learning Based Methods</head><p>Existing learning-based methods for caricature generation can use both paired and unpaired data as training material.</p><p>Supervised data-driven methods would automatically find rules by relying on pairs of exemplars to learn a mapping between the domain of normal faces and the domain of caricatures. <ref type="bibr" target="#b52">Xie et al. (2009)</ref> proposed a framework that learns a PCA model over 3D caricatures and a Locally Linear Embedding (LLE) model over 2D caricatures, both made by artists. The user can manually create a deformation that is projected into the PCA subspace and refined using the LLE model. <ref type="bibr" target="#b40">Li et al. (2008)</ref> and <ref type="bibr" target="#b31">Liu et al. (2009)</ref> both focused on learning a mapping between the LLE representation of photographs and their corresponding LLE representation of 3D caricatures modeled by artists. In the same vein, but only in the 3D domain, <ref type="bibr" target="#b56">Zhou et al. (2016)</ref> regressed a set of locally linear mappings from sparse exemplars of 3D faces and their corresponding 3D caricature. As far as we know, <ref type="bibr" target="#b14">Clarke et al. (2011)</ref> are the only authors that proposed a physics-oriented caricature method. They capture the artistic style of 2D caricatures by learning a pseudo stress-strain model which describes physical properties of virtual materials. All these data-driven approaches are based on paired datasets which require the work of 2D or 3D artists. Such datasets are costly to produce, therefore techniques of this kind are hardly applicable.</p><p>Unsupervised learning based methods learn how to caricature from unpaired face and caricature exemplars. <ref type="bibr" target="#b23">Chen et al. (2001)</ref> and <ref type="bibr" target="#b29">Liang et al. (2002)</ref> generated 2D caricatures by learning a nonlinear mapping between photos and corresponding caricatures made by artists. Derived from the image synthesis literature, where they have been used for unpaired one-to-one translation <ref type="bibr" target="#b32">(Liu et al., 2017;</ref><ref type="bibr" target="#b49">Taigman et al., 2017;</ref><ref type="bibr" target="#b54">Yi et al., 2017;</ref><ref type="bibr" target="#b57">Zhu et al., 2017)</ref>, or unpaired many-to-many translation <ref type="bibr" target="#b25">(Huang et al., 2018b;</ref><ref type="bibr" target="#b33">Liu et al., 2019;</ref><ref type="bibr" target="#b12">Choi et al., 2020)</ref>, Generative Adversarial Networks (GANs) have also shown impressive results on mesh synthesis and mesh-to-mesh translation <ref type="bibr" target="#b20">(Goodfellow et al., 2014)</ref>. Other approaches achieve 2D stylization using 3D priors and a differentiable renderer <ref type="bibr" target="#b50">(Wang et al. (2021)</ref>.) <ref type="bibr" target="#b8">Cao et al. (2019)</ref> proposed a photo to 2D caricature translation framework CariGANs based on a large dataset of over 6,000 labeled 2D caricatures <ref type="bibr" target="#b26">(Huo et al., 2018)</ref>, and two GANs, namely CariGeoGAN for geometry exaggeration using landmark warping, and CariStyGAN for stylization. CariStyGAN allows to use a reference graphic style, or else, it will generate a random style. This framework was first extended by <ref type="bibr" target="#b44">Shi et al. (2019)</ref> with a feature point-based warping for geometric exaggeration, then by <ref type="bibr">Gu et al. (</ref> <ref type="formula">2021</ref>) which provides a random set of deformation styles in addition to the random set of graphics styles, offering consequent user control. In the 3D domain, <ref type="bibr" target="#b51">Wu et al. (2018)</ref> then <ref type="bibr" target="#b7">Cai et al. (2021)</ref> proposed robust methods for 3D caricature reconstruction from meshes, enlarging the set of available in-the-wild 3D caricatures, when used in combination with WebCaricature <ref type="bibr" target="#b26">(Huo et al., 2018)</ref>. <ref type="bibr" target="#b22">Guo et al. (2019)</ref> showed an approach for producing expressive 3D caricatures from photos using a VAE-CycleGAN. <ref type="bibr" target="#b53">Ye et al. (2020)</ref> proposed an end-to-end 3D caricature generation from photos method, using a GAN-based architecture with two symmetrical generators and discriminators. A step of texture stylization is performed with CariStyGAN. The recent works for caricature generation in 3D domain allow to reproduce the style of artists but they do not feature much user control. <ref type="bibr" target="#b53">Ye et al. (2020)</ref> introduced Facial Shape Vectors so the user can choose the facial proportions on the caricature, but this is a quite low-level interaction and thus should be done by an artist. These works also show a weakness from the use of CariStyGAN for texture stylization. CariStyGAN tends to emphasize the shadows and light spots of the photos in order to make the reliefs sharper. In the case of textured 3D models, the shadows and light spots should be induced by the geometry and the lighting conditions, not by the texture albedo. If lighting information is entangled within texture information, changing the lighting condition can make the 3D model appear to be enlightened by non-existent lights.</p><p>Adopting a 3D mesh representation requires application of mesh convolutions defined on non-Euclidean domains (i.e., geometric deep learning methodologies). Over the past few years, the field of geometric deep learning has received significant attention <ref type="bibr" target="#b30">(Litany et al., 2017;</ref><ref type="bibr" target="#b35">Maron et al., 2017)</ref>. Methods relevant to this paper are auto-encoder structures such as used by <ref type="bibr" target="#b41">Ranjan et al. (2017)</ref> and <ref type="bibr" target="#b18">Gong et al. (2019)</ref>, that showcase the efficiency of recent 3D convolutional operators at capturing the distribution of 3D facial meshes. Several approaches resort to mapping 3D faces to a 2D domain, and using 2D convolution operators <ref type="bibr" target="#b38">(Moschoglou et al., 2020)</ref>. Projecting a 3D surface to a 2D plane for 2D convolutions requires locally deforming distances, which translates to higher computing and memory costs compared to recent 3D convolution approaches, and some high-frequency information loss <ref type="bibr" target="#b18">(Gong et al., 2019)</ref>.</p><p>Deep learning based approaches, leveraging recent advancements in the field, could produce caricatures more similar to the kind produced by professionals, and allow global style control using handmade caricatures as style examples. On the opposite side, a user-controlled rule-based approach enabling a local control of the facial mesh deformation would allow for fine-tuned local control. We develop both approaches in Section 3 and Section 4. Finally, there is no overall perception user study of this specific field, limiting any qualitative comparison between approaches. We present the first study of this kind in Section 5, in order to evaluate the strengths and drawbacks of these two novel methods in comparison to two state-of-the-art approaches.</p></div>
<div><head n="3">RULE-BASED USER-CONTROLLED CARICATURIZATION</head><p>We present a novel method featuring short computation time and providing meaningful user control over the generated caricatures. It is based on two main modules depicted in Figure <ref type="figure" target="#fig_1">2</ref> (in green and in yellow). First, a curvature exaggeration module (in green) enhances the facial lines by applying EDFM technique to the main PCA scores of the mesh gradients of the input face. This emphasizes only the 3D surface details such as ridges, peaks, and folds, and does not affect the global shape of the face (such as eyes, nose, and mouth relative positions). Second, a proportion exaggeration module (in yellow) leverages compositions of real artists (see Section 3.1) to caricature the general shape of the face. It projects the input face into a 3D caricature shape space thanks to a kNN regressor. This process applies a smooth and large scale deformation to the input face while preserving its local features. The curvature exaggeration and proportion exaggeration modules are thus complementary. They are combined to provide the user with a bilateral control (small scale versus large scale) over the resulting caricature. Lastly, an optional texture blurring and contrast enhancement module (in pink) makes the resulting caricature less realistic and more graphic. The reason behind this step is to make the result more acceptable for human observers. As shown by <ref type="bibr" target="#b55">Zell et al. (2015)</ref>, we use texture blurring because it increases the appeal and lowers the eeriness of a virtual character. The increase in contrast is meant to make the caricatures less realistic, but one could have used another technique to this end. In addition to these modules, our usercontrolled method features semantic mesh segmentation in four regions (see Section 3.2). In total, the method exposes ten knobs to the user.</p></div>
<div><head n="3.1">Datasets</head><p>Realistic 3D faces were sampled from the LSFM dataset <ref type="bibr" target="#b5">(Booth et al., 2016)</ref> which contains nearly 10k distinct 3D faces. In order to have textured meshes, we completed this set with 300 in-house 3D face scans. Their topologies are unified through automatic facial landmarking and geometry fitting <ref type="bibr" target="#b15">(Danieau et al., 2019)</ref>. To build our 3D caricatured mesh dataset, we run the 2D to 3D caricature inference method of <ref type="bibr" target="#b7">Cai et al. (2021)</ref> on the WebCaricature dataset <ref type="bibr" target="#b26">(Huo et al., 2018)</ref>, which enables to extract the 3D caricatured face mesh from each 2D image. The WebCaricature dataset contains over 6k 2D caricatures. When Cai's algorithm did not successfully estimate the faces, due to extreme drawing composition (quick sketch, incomplete drawings, drafts, cubism etc.) the generated output remains the same default caricature mesh. All faces were then registered, in order to have a fixed topology <ref type="bibr" target="#b48">(Sumner and Popović, 2004)</ref>.</p></div>
<div><head n="3.2">Facial Segmentation</head><p>In face modeling, cartoonization and caricaturing, semantic segmentation is a popular technique for increasing expressivity and user interaction <ref type="bibr" target="#b4">(Blanz and Vetter, 1999;</ref><ref type="bibr" target="#b31">Liu et al., 2009;</ref><ref type="bibr" target="#b56">Zhou et al., 2016)</ref>. In the proposed system, the 3D faces are segmented using the scheme proposed by <ref type="bibr" target="#b4">Blanz and Vetter (1999)</ref> i.e. in four regions: the eyes, the nose, the mouth, and the rest of the face. This semantic segmentation allows the user to choose whether to emphasize or not a facial part. In total, the method exposes ten knobs to the user: one scalar is used for the strength of the gradient EDFM and another one for the amount of deformation from the kNN regressor to be added. Those two weights are tunable for each of the five regions (four masks and full face). Segmenting the domain also allows to break the inherent linearity of PCA by learning different subspaces.</p></div>
<div><head n="3.3">Curvature Exaggeration</head><p>To emphasize the small scale features of the input 3D face, the curvature exaggeration module performs EDFM on the mesh gradient. In the process, we use PCA as a mean to reduce high frequencies (Figure <ref type="figure" target="#fig_2">3</ref>).</p><p>• Offline preprocessing. The edge-based gradient operator E (see Supplementary Material) is used to compute the gradients g of each face mesh s of our custom 3D face dataset (Section 3.1). Following the results of <ref type="bibr" target="#b37">Mo et al. (2004)</ref> showing that lowvariance features should be more taken into account, the gradients G are standardized: G std G-g σG . Then, a PCA is performed on the standardized gradients leading to the principal components W and each PCA scores t such that t g std •W.</p><p>• Runtime curvature exaggeration. The input face mesh s is standardized then projected into the PCA space learnt offline. EDFM technique is applied with a factor f grad given by the user. To prevent noise, we weight the result by the normalized standard  The exaggerated PCA scores are obtained as t t • max(f grad • σ, 1). The exaggerated gradient is then recovered as ĝ g + σ G • ( t • W T ). The gradients' exaggerated mesh ŝ is eventually reconstructed at the least squares sense by setting the border vertices fixed (the border of the eyes, the nostrils, the inner lips, and the contour of the head), as described in Supplementary Material.</p></div>
<div><head n="3.4">Proportion Exaggeration</head><p>The proportion exaggeration module leverages the 3D caricatures (see Section 3.1) to sample a deformation that matches the input face difference from the mean using a kNN. Thus, it can be seen as an example-based version of EDFM. We argue that the sampled deformation contains mainly low frequencies and adding it to an input face will modify very little its surface curvatures. We observed that the 3D caricatures have more diverse global shapes than our 3D faces while being much smoother. In addition, the kNN regression also contributes to smooth out the deformation by averaging the k nearest neighbors. The process works as follows:</p><p>• Offline preprocessing. The 3D caricatures are first standardized using the standard deviation of our 3D faces to make the low-variance areas more important <ref type="bibr" target="#b37">(Mo et al., 2004)</ref>. Then, we fit a kNN regressor using a cosine distance metric, as we mainly seek to find directions of deformation rather than amplitudes of deformation. The amplitude tuning is reserved for the user.</p><p>• Runtime proportion exaggeration. The input face is standardized then projected into the 3D caricature space with the kNN regressor using barycentric weights. The obtained deformation δ std is weighted by the 3D face standard deviation σ S and by a user-defined scalar f prop for amplitude tuning. Eventually, we add this deformation to the curvature exaggerated face to get the vertex positions of the resulting caricature c:</p><formula xml:id="formula_0">c ŝ + δ with δ f prop • δ std • σ S (1)</formula></div>
<div><head n="3.5">Results</head><p>In this section, the results of both the curvature exaggeration module and the proportion exaggeration module are presented and compared to those of their most similar existing approaches. We compare the curvature exaggeration module to <ref type="bibr" target="#b43">Sela et al. (2015)</ref> because they fix the positions of border vertices and therefore tend to preserve the proportions of the caricatured faces. Our proportion exaggeration module is compared to the baseline 3D position EDFM introduced in the seminal work of <ref type="bibr" target="#b4">Blanz and Vetter (1999)</ref>.</p><p>• Curvature exaggeration module. The benefit of the PCAbased denoising mechanism is visible in Figure <ref type="figure" target="#fig_2">3</ref> between column b), and column c) and d). Without PCA, the EDFM technique magnifies the existing high frequencies of the face's difference from the mean. With PCA, the noise is removed but the exaggeration of facial lines remains. The use of a segmented model not only enables to provide more user-control, but also to emphasize the curvatures more locally. This effect can be noticed when comparing the results c) and d) in Figure <ref type="figure" target="#fig_2">3</ref>. <ref type="bibr" target="#b43">Sela et al. (2015)</ref>'s method successfully preserves the position of the eyes, the nostrils, the inner lips and the contour of the face. However other parts such as the nose, the lips and the chin seem greatly inflated and displaced which should not belong to facial lines enhancement. Conversely, our curvature exaggeration module modifies the vertex positions such that it only enhances the fine curvature details.</p><p>• Proportion exaggeration module. Figure <ref type="figure" target="#fig_3">4</ref> shows the effect of modifying k on the results of our proportion exaggeration module. Visually, the parameter k of the kNN regressor has less impact than we expected. However, it appears that a small value of k ( ≤ 5) tends to introduce high-frequencies and vertex entanglement while larges values of k ( ≥ 1000) seem to produce less vivid results. We fixed k 40 in our experiments.</p><p>The semantic segmentation has also an impact on our proportion exaggeration module. In Figure <ref type="figure" target="#fig_4">5</ref>, the results with segmentation (column c) seem more caricatural but also more expressive than without segmentation (column b). Expressiveness is not intended by the proposed method since the focus is on neutral expression caricature generation. Nevertheless, we decided to conserve the segmentation scheme for the proportion exaggeration module. We also compare the proportion exaggeration algorithm to the baseline PCA-based EDFM on 3D coordinates proposed by <ref type="bibr" target="#b4">Blanz and Vetter (1999)</ref> (column d). Our method clearly generates more diverse and inhomogeneous shapes than Blanz and Vetter (1999)'s approach. It is also noticeable that less high-frequency details are added than with the baseline method, which is what we aim at.</p></div>
<div><head n="4">DEEP LEARNING BASED AUTOMATIC CARICATURIZATION</head><p>Rule-based methods allow the use of controllable and interpretable parameters, but are limited to capture information about caricature styles. Supervised learning based methods require a large paired mesh-to-caricature dataset, that are highly consuming in terms of both time and means to build. Instead, we consider the case of an unpaired learning-based approach, taking advantage of our 3D datasets of both neutral and caricatured faces <ref type="bibr" target="#b7">(Cai et al., 2021</ref>) (cf. Section 3.1). Our network architecture is based on the shared content space assumption of <ref type="bibr" target="#b33">Liu et al. (2019)</ref>, that we adapt to the context of 3D data through the use of 3D convolutions of <ref type="bibr" target="#b18">Gong et al. (2019)</ref>, which define 3D convolution neighborhoods.</p></div>
<div><head n="4.1">Framework Overview</head><p>Let us consider meshes of different styles (e.g. scans and caricatures), all sharing the same mesh topology. We represents our faces with raw 3D coordinates, and encode them using a recent 3D convolutional operator <ref type="bibr" target="#b18">(Gong et al., 2019)</ref>. Given a mesh x ∈ X and an arbitrary style y ∈ Y, our goal is to train a single generator G that can generate diverse meshes of each style y that corresponds to the mesh x. We generate stylespecific vectors in the learned space of each style and train G to reflect these vectors. Figure <ref type="figure" target="#fig_5">6</ref> illustrates an overview of our framework, which consists of three modules described below.</p><p>Generator. Our generator G translates an input mesh x into an output mesh G (x, s) reflecting a style-specific style code s, which is provided by the style encoder E. We use adaptive instance normalization (AdaIN) <ref type="bibr">(Huang and Belongie, 2018a)</ref> to inject s into G. We observe that s can represent any style, which removes the necessity of providing y to G and allows G to synthesize meshes of all domains.</p><p>Style encoder. Given a mesh x, our encoder E extracts the style codes s E(x). Similar to <ref type="bibr" target="#b33">Liu et al. (2019)</ref>, our style encoder benefits from the multi-task learning setup. E can produce diverse style codes using different reference meshes. This allows G to synthesize an output mesh reflecting the style code s of a reference mesh x.  Discriminator. Our discriminator D is a multitask discriminator <ref type="bibr" target="#b36">(Mescheder et al., 2018;</ref><ref type="bibr" target="#b33">Liu et al., 2019;</ref><ref type="bibr" target="#b12">Choi et al., 2020)</ref>, which consists of multiple output branches. Each branch D y learns a binary classification determining whether a mesh x is a mesh from the dataset of style y or a fake mesh G (x, s) produced by G.</p></div>
<div><head n="4.2">Training Objectives</head><p>Given a mesh x ∈ X and its original style y ∈ Y, we train our framework using the following objectives:</p><p>• Adversarial objective. During training, we sample a mesh a and generate its style code s E(a). The generator G takes a mesh x and s as inputs and learns to generate an output mesh G (x, s) that is indistinguishable from real meshes of the style y, via a classical adversarial loss <ref type="bibr" target="#b3">(Arjovsky et al., 2017)</ref>:</p><formula xml:id="formula_1">L adv E x,y logD y x ( ) + E x, ỹ log 1 -D ỹ G x, s ( ) ( )</formula><p>where D y (•) denotes the output of D corresponding to the style y.</p><p>• Reconstruction and cycle losses. To guarantee that the generated mesh G (x, s) properly preserves the style-invariant characteristics (e.g. identity) of its input mesh x, we employ the cycle consistency loss <ref type="bibr" target="#b27">(Kim et al., 2017;</ref><ref type="bibr" target="#b57">Zhu et al., 2017;</ref><ref type="bibr" target="#b11">Choi et al., 2018</ref>)</p><formula xml:id="formula_2">L cyc E x,y, ỹ x -G G x, s ( ), ŝ<label>(</label></formula><p>)</p><formula xml:id="formula_3">1</formula><p>where ŝ E y (x) is the estimated style code of the input mesh x, ỹ and s are the style and estimated style codes of another mesh than x. By encouraging the generator G to reconstruct the input mesh x with its estimated style code ŝ, G learns to preserve the original characteristics of x while changing its style faithfully. In a similar goal of preserving style invariant characteristics, we use a reconstruction loss</p><formula xml:id="formula_4">L r E x,y x -G x, ŝ ( ) 1</formula><p>where ŝ E y (x) is the estimated style code of the input mesh x.</p><p>• Full objective. Our objective function can be summarized as follows:</p><formula xml:id="formula_5">min G,F,E max D L adv + λ cyc • L cyc + λ r • L r</formula><p>where λ r and λ cyc are hyper parameters for each term. We use the Adam Optimizer <ref type="bibr" target="#b28">(Kingma and Ba, 2015)</ref>.</p></div>
<div><head n="4.3">Results</head><p>We trained the network for 50k iterations on a Titan X Pascal (4h, 8Go). Results of the approach are visible in Figure <ref type="figure" target="#fig_6">7</ref>. The original faces (top row) are encoded using the network illustrated in Figure <ref type="figure" target="#fig_5">6</ref> along with a random caricature of the dataset, producing the caricatured face (bottom row). Facial proportions are hence exaggerated according to the distribution of the neutral and caricatured faces learned during the training stage.</p></div>
<div><head n="5">USER STUDY</head><p>In order to assess the subjective quality of the caricatures generated by the previously described methods, we have conducted a perceptual study. The goal of the perceptual study was to subjectively rank the generated caricatures based on the perceived quality of the caricatures. In addition to the two methods described in Section 3 and Section 4, we also considered two baseline methods, the method from <ref type="bibr" target="#b43">Sela et al. (2015)</ref> and a EDFM method <ref type="bibr" target="#b4">(Blanz and Vetter (1999)</ref>).</p></div>
<div><head n="5.1">Participants</head><p>Forty-nine participants took part in the experiment (9 females). They were between 18 and 63 years old (mean and STD age: 31.0 ± 11.3), and were recruited from our laboratory among students and staff. They were all naive to the purpose of the experiment, had normal or correct-to-normal vision, and gave written and informed consent. The study conformed to the declaration of Helsinki. Participants were not compensated for their participation and none of the participants knew the human faces used in the study.</p></div>
<div><head n="5.2">Stimuli</head><p>The top part of Figure <ref type="figure" target="#fig_6">7</ref> presents the 12 human face scans (Identity factor) used in the study (4 females, eight males). They were caricatured using five different approaches (Method factor): the learning-based approach (Deep) presented in Section 4, two variations of the rule-based approach presented in Section 3 (see Table <ref type="table">1</ref>), and two state-of-the-art caricaturization methods-EDFM <ref type="bibr" target="#b4">(Blanz and Vetter, 1999)</ref> and Sela <ref type="bibr" target="#b43">(Sela et al., 2015)</ref>. For each face (original and caricatured), we used the cartoonization module presented in Section 3. The texture blurring is expected to reduce the mismatch of realism between the shape and the texture and therefore make the caricature more acceptable to human observers <ref type="bibr" target="#b55">(Zell et al., 2015)</ref>. The stimuli were rendered with a rotation of 30 °around the vertical axis, with a fixed view. The angle was chosen as a common viewpoint between a frontal and profile view. We considered only the facial mask, hence other facial attributes such as eyes and hair were not displayed.</p></div>
<div><head n="5.3">Protocol</head><p>The perceptual study consisted of two parts. The first part of the study assessed the results produced by each method for each face, according to participant's preferences. For each human facial scan, participants were presented with the original face and the caricatures generated with the five methods. They were asked to rank all five caricatures from the best to the worst caricature. The order of the scans and the presentation of the caricatures was randomized independently for each participant and each facial scan was only presented once, for a total of 12 trials. The second  TABLE 1 | Parameters sets of the two variations of our rule-based method used in the user study (Section 5). The first variation targets the proportions more while the second strongly exaggerates the curvatures. These parameter sets aim at exploring the range of user control provided to the user. A number of other variations could have been proposed, but we meet complexity restrictions for the user study. part of the study aimed at evaluating globally each of the five methods. For each method (in a random order), the caricaturization results (12 facial scans) were displayed at once. Participants were asked to indicate how much they agreed to three statements using 5-point Likert scales. The statements were "They preserve the identity of the person," "They correspond to what would be expected of a caricature," "I like the results". There was no time limit for any of the two parts, and the evaluation was conducted online using the PsyToolkit software <ref type="bibr" target="#b47">(Stoet, 2010</ref><ref type="bibr" target="#b46">(Stoet, , 2017))</ref>. We include a sample view of the ranking task in Supplementary Figure <ref type="figure" target="#fig_1">22</ref>. A render of all 12 caricatures for each method can be seen on Supplementary Figures <ref type="figure" target="#fig_6">17</ref><ref type="figure" target="#fig_7">18</ref><ref type="figure" target="#fig_8">19</ref>, 21.<ref type="bibr" target="#b46">(Stoet, , 2017))</ref>. We include a sample view of the ranking task in Supplementary Figure <ref type="figure" target="#fig_1">22</ref>. A render of all 12 caricatures for each method can be seen on Supplementary Figures <ref type="figure" target="#fig_8">19</ref>, 21.</p></div>
<div><head n="5.4">Results</head><p>We present in this section the statistical results of the user study.</p></div>
<div><head n="5.4.1">Average Rankings</head><p>To analyze ranking distributions (Figures 9-Figure <ref type="figure" target="#fig_0">10</ref>), we first performed a Friedman test with the within-subject factor Method (using the average rank between all 12 scans). We found an effect of the Method on average ranking (χ 2 12.21; p &lt; 0.05). The effect is then explored further using a Wilcoxon post-hoc test for pair-wise comparisons. We found significant differences only between EDFM and Deep, Geo.1, Sela (all p &lt; 0.05). We found that per method, average rankings vary between 2.81 (EDFM) and 3.12 (Deep) 10. In order to determine whether ranking distributions per method differed with identities, we used a Friedman test with within-subject factors Method and Identity. Out of 12 distinct identities, 6 <ref type="bibr">(identities 2, 5, 6, 7, 11, 12)</ref> showed significantly different rankings between methods. This is in most cases (5 out of 6) due to worse than average performance from a set of methods, usually Deep or Sela.</p></div>
<div><head n="5.4.2">Top Rankings</head><p>We measured Top-1, Top-2, and Top-3 rank differences per method, using Friedman tests, Top-X rankings being the number of times the techniques were ranked X or lower (lower is better, Figure <ref type="figure" target="#fig_9">11</ref>). We found no significant differences for Top-1 (χ 2 4.14; p 0.38) rankings, but an effect was found for both Top-2 (χ 2 9.74; p &lt; 0.05) and Top-3 rankings (χ 2 34.60; p &lt; 0.001). The effect for Top-2 and Top-3 rankings is then explored using a Wilcoxon post-hoc test. For Top-2 rankings, we found that EDFM was chosen significantly more often as first or second choice than Deep (p &lt; 0.05) and Sela (p &lt; 0.01). For Top-3 rankings, we found a similar preference for EDFM over Deep, Geo.1, and Sela (p &lt; 0.05), as well as a significant lower preference for Sela over all others (p &lt; 0.05).</p></div>
<div><head n="5.4.3">Variations Between Participants</head><p>We looked into participant-wise preferences for caricature methods using a Friedman test on ranking choices of each participant, individually. Out of 49 participants, separate Friedman tests on their Top-1 rankings showed that only 12 had a significant preference towards a set of methods, and out of these only 4 towards a specific one. These numbers are too low to show anything conclusive in that regard.</p></div>
<div><head n="5.4.4">Subjective Scores</head><p>Subjective ratings results were analyzed separately using a oneway ANOVA with within-subject factor Method on the data of each question. All subjective results differences between methods were found to be significant (p values of 5.7e -6, 7.35e -6, and 2.28e -5). We conducted separate post-hoc analyses using Wilcoxon. For the statement "They preserve the identity of the person" (Figure <ref type="figure" target="#fig_10">12</ref>), significantly different groups of method were Deep, Sela (mean 3), and Geo.1, EDFM (mean 2.3). The method Geo.2 (mean 2.6) was not significantly different from others. For the statements "They correspond to what would be expected of a caricature" (Figure <ref type="figure" target="#fig_11">13</ref>) and "I like the results" (Figure <ref type="figure" target="#fig_12">14</ref>), the only significant differences were between the group of Geo.1, Geo.2, EDFM, and Sela, Deep being in between.</p></div>
<div><head n="6">DISCUSSION</head><p>In this paper, we have proposed two novel caricaturization methods. One leveraging the capabilities of deep style transfer The proposed methods, and two additional methods from the literature were evaluated through a user study considering 12 different facial scans and the corresponding caricature generated from these different methods. Overall, the results FIGURE 10 | Boxplot of the average rankings over participants, per method. Rankings range from 1 to 5. Overall, all methods achieve similar performances, averages being between 2.81 and 3.12 (lower is better).   showed that all methods achieved similar performances, average ratings going from 2.82 to 3.12 (lower is better). An observation from the results is that in general, there was not a method which was significantly superior to the others. The results considering only the method (see Figure <ref type="figure" target="#fig_10">12</ref>) show a fairly distributed results, although Deep and Sela approaches seem to generate a higher number of "badly ranked caricatures" (fourth and fifth ranks). This observation matches with the global appreciation from participants, as EDFM, Geo.1 and Geo.2 got slightly higher scores. While this result could suggest that some of the methods worked better from some facial scans than others, the results split by Identity do not totally support this hypothesis (see Figure <ref type="figure" target="#fig_9">11</ref>). Looking at the top five worst ranked caricatures (Figure <ref type="figure" target="#fig_4">15</ref>), we can identify several cases in which the method considered could have generated undesired results. The facial features of face six interpenetrate each other when using Sela, and the borders of face seven are spread too widely using the same method. On face 5, eye size difference is too greatly exaggerated with the method Deep. These generated faces rated significantly worse than others on average can be easily identified, opening possibilities of a manual or automatic filtering protocol. Nevertheless, these results seem to evidence that some methods had a particularly bad performance on some of the facial scans. Yet, this did not happen consistently. Each caricaturization method had a pre-defined set of meta-parameters. The chosen configuration could have suited better some faces than others, generating caricatures of different qualities. The top five best ranked caricatures can be seen on Figure <ref type="figure" target="#fig_7">8</ref>.</p><p>Another potential explanation for the results is that the task was too hard and subjective, choices ending up being random. Using faces with no hair or eyes might have even increased the complexity of the task. Indeed, some participants explicitly stated that the task was difficult, especially as they were judging textured facial masks instead of full faces. Nevertheless, this potential user preference does not seem to be linked with any particular caricaturization method. Looking at participant preferences, only 12 participants out of 49 showed a significant rating variation between methods ranked first. Looking at results on subjective questions, the two worse rated (Deep and Sela) methods rank-wise (being also those with the worst rated specific caricatures) were rated higher both at "They correspond to what would be expected of a caricature" and "I like the result," where caricatures of each method were presented globally, suggesting that without their bad results on specific faces-which might be less visible when presented amongst all the others-they could actually have ranked higher than other methods. The conception of a perceptual metric reliably judging the quality of a caricature could help guide its creation, but the high variation of participant preferences in our study suggest that it would require a considerably larger study to be defined.</p><p>Considering these findings, we issue the following guidelines for choosing a method to generate caricatures automatically.</p><p>• If the main goal is to generate caricatures with a given set of parameters, no specific style, and as little variance as possible in quality, an EDFM-based method is the most suitable. • If there is still no specific style required, but more tolerance to variance in quality (for instance if it is possible to tune the generated faces when they are unsatisfying), we recommend the approach of Sela, rated very similarly to EDFM on average in the rankings task, and significantly more on the subjective questionnaire.</p><p>• If a specific caricature style is required, the Deep approach will offer results comparable with Sela both in the ranking task and the questionnaire. • Finally, if there is a need to target a specific user, the best solution is to use the panel of available methods, and leave the choice to them. Caricatures provide a style whose notion can be understood as an "accentuation of facial features," allowing manually defined rules to achieve comparable performance to learning-based approaches. Other stylistic facial domains, such as aliens or anthropomorphic animals could have more to gain from learning. Such non-realistic 3D facial data is although currently very scarce.</p></div>
<div><head n="7">CONCLUSION</head><p>In this paper we have introduced two novel approaches to automatically generate caricatures from 3D facial scans. T he first method mixes EDFM-based curvature deformation and data driven proportion deformation, while the second method is based on a domain-to-domain translation deep neural network. Then, we present and discuss a perceptual study aiming to assess the quality of the generated caricatures. Overall, the results showed that the different evaluated methods performed in a similar way, although their performance could vary with respect to the facial scan used. This result illustrates both the subjectivity of evaluating caricaturization performance, along with the complementarity of using different approaches, producing different styles of caricatures. Future work could involve looking into automatic detection of the worse cases of automatic caricaturization, to apply a correction or a filter, or exploring learned-based automatic caricaturization by learning on different caricature styles, and setting up a network able to generate faces of a given style. We believe this study of the extended state of the art have helped grow and precise the landscape of automatic caricaturization approaches, and 3D facial stylization in general, and that our work provides interesting insights and guidelines for the automatic generation of caricatures that will help practitioners and inspire future research.</p></div><figure xml:id="fig_0"><head>FIGURE 1 |</head><label>1</label><figDesc>FIGURE 1 | Results of our novel user-controlled rule-based approach. Each pair (A, B, C, and D) presents the input facial scan (wired on the left) and its automatically generated caricature on the right.</figDesc><graphic coords="3,69.96,67.75,455.49,116.62" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>FIGURE 2 |</head><label>2</label><figDesc>FIGURE 2 | Overview of our user-controlled method presented in Section 3. Arrows and diamond shapes represent algorithms while boxes represent data. Offline and online processing are represented by the blue and orange colors, respectively. Green, yellow, and pink highlights show the different modules which compose the core of the user-controlled caricature system. For simplification purposes, the face segmentation is not shown.</figDesc><graphic coords="6,70.75,67.75,453.81,158.80" type="bitmap" /></figure>
<figure xml:id="fig_2"><head>FIGURE 3 |</head><label>3</label><figDesc>FIGURE 3 | Different curvature exaggeration techniques: (A) Original 3D Mesh. (B) Naive gradient EDFM without segmentation (f grad 5). (C) Gradient EDFM with PCA denoising, without segmentation and (D) with segmentation (f grad 5). (E) Sela et al. (2015)'s method, without reference model (c 0.3) and (F) with the mean face as reference model (β 4). More examples in Supplementary Material, at Supplementary Figure 23.</figDesc><graphic coords="6,70.13,299.91,455.02,88.10" type="bitmap" /></figure>
<figure xml:id="fig_3"><head>FIGURE 4 |</head><label>4</label><figDesc>FIGURE 4 | A comparison of results with different values of k for the kNN algorithm of the proportion exaggeration module. The first column shows the original facial mesh. Here, the caricatures are generated with f proportions 2.</figDesc><graphic coords="8,97.57,67.75,400.28,171.33" type="bitmap" /></figure>
<figure xml:id="fig_4"><head>FIGURE 5 |</head><label>5</label><figDesc>FIGURE 5 | A comparison between proportion exaggeration techniques on two facial meshes. (A) Original facial mesh. (B) Our proportion exaggeration algorithm without segmentation and (C) with segmentation. (D) Baseline PCA-based 3D positions EDFM<ref type="bibr" target="#b4">(Blanz and Vetter, 1999)</ref>.</figDesc><graphic coords="8,105.73,313.91,383.75,243.21" type="bitmap" /></figure>
<figure xml:id="fig_5"><head>FIGURE 6 |</head><label>6</label><figDesc>FIGURE 6 | Overview of UNGT. A facial scan's identity is encoded along with the style of a caricature mesh, in order to produce the caricatured face. Textures are not processed, and presented for illustration purpose only. E represent the Style Encoder, G the Generator, and D the Discriminator.</figDesc><graphic coords="9,69.96,67.75,455.49,177.39" type="bitmap" /></figure>
<figure xml:id="fig_6"><head>FIGURE 7 |</head><label>7</label><figDesc>FIGURE 7 | Deep learning based caricatures for a number of facial scan examples.</figDesc><graphic coords="10,70.53,67.75,454.31,116.05" type="bitmap" /></figure>
<figure xml:id="fig_7"><head>FIGURE 8 |</head><label>8</label><figDesc>FIGURE 8 | The five best caricatures (with the best mean ranks; identities 7, 6, 9, 12, 2).</figDesc><graphic coords="10,69.85,239.07,455.73,86.06" type="bitmap" /></figure>
<figure xml:id="fig_8"><head>FIGURE 9 |</head><label>9</label><figDesc>FIGURE 9 | Average rankings, per Method and Identity. R1 to R5 are the ranks 1 to rank 5. Note the high variance per face and method.</figDesc><graphic coords="11,93.71,67.75,407.98,154.32" type="bitmap" /></figure>
<figure xml:id="fig_9"><head>FIGURE 11 |</head><label>11</label><figDesc>FIGURE 11 | Caricature ranking distribution across all participants, per method. Top-1 to Top-5 rankings respectively shown in light blue, green, yellow, orange, and blue.</figDesc><graphic coords="12,118.94,280.01,357.58,133.34" type="bitmap" /></figure>
<figure xml:id="fig_10"><head>FIGURE 12 |</head><label>12</label><figDesc>FIGURE 12 | Average Likert ratings for the statement "They preserve the identity of the person". Deep and Sela are significantly different to Geo.1 and EDFM.</figDesc><graphic coords="12,50.00,477.24,234.47,133.34" type="bitmap" /></figure>
<figure xml:id="fig_11"><head>FIGURE 13 |</head><label>13</label><figDesc>FIGURE 13 | Average Likert ratings for the statement "I like the results". Geo.1, Geo.2, and EDFM are significantly different to Sela.</figDesc><graphic coords="12,310.79,477.24,234.47,133.34" type="bitmap" /></figure>
<figure xml:id="fig_12"><head>FIGURE 14 |</head><label>14</label><figDesc>FIGURE 14 | Average Likert ratings for the statement "They correspond to what would be expected of a caricature". Geo.1, Geo.2, and EDFM are significantly different to Sela.</figDesc><graphic coords="13,50.00,67.75,234.47,133.34" type="bitmap" /></figure>
			<note place="foot" xml:id="foot_0"><p>Frontiers in Virtual Reality | www.frontiersin.org January 2022 | Volume 2 | Article 785104</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We wish to thank all the reviewers for their comments, and the participants in our experiment. This project has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation program</rs> under grant agreement No <rs type="grantNumber">952 147</rs>. This project has received funding from the <rs type="funder">Association Nationale de la Recherche et de la Technologie</rs> under CIFRE agreement No <rs type="grantNumber">2018/1656</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zYXaEcr">
					<idno type="grant-number">952 147</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation program</orgName>
				</org>
				<org type="funding" xml:id="_Tj56pz6">
					<idno type="grant-number">2018/1656</idno>
				</org>
			</listOrg>

			<div type="availability">
<div><head>DATA AVAILABILITY STATEMENT</head><p>The raw data supporting the conclusion of this article will be made available by the authors, without undue reservation.</p></div>
			</div>

			<div type="annex">
<div><head>ETHICS STATEMENT</head><p>Ethical review and approval was not required for the study on human participants in accordance with the local legislation and institutional requirements. Written informed consent for participation was not required for this study in accordance with the national legislation and the institutional requirements.</p></div>
<div><head>AUTHOR CONTRIBUTIONS</head><p>NO contributed to this work during his PhD, GK during his Master. They were supervised by FA, QA, FD, PG, LH, and FM.</p></div>
<div><head>SUPPLEMENTARY MATERIAL</head><p>The Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/frvir.2021.785104/ full#supplementary-material Conflict of Interest: Authors NO, GK, QA, FD, and PG were employed by the company InterDigital.</p><p>The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p><p>Publisher's Note: All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Making Caricatures with Morphing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Akleman</surname></persName>
		</author>
		<idno type="DOI">10.1145/259081.259231</idno>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Making Extreme Caricatures with a New Interactive 2d Deformation Technique with Simplicial Complexes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Akleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Logan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Vis</title>
		<imprint>
			<biblScope unit="page" from="100" to="105" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling Expressive 3d Caricatures</title>
		<author>
			<persName><forename type="first">E</forename><surname>Akleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Reisch</surname></persName>
		</author>
		<idno type="DOI">10.1145/1186223.1186299</idno>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wasserstein Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno>ArXiv: 1701.07875</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Morphable Model for the Synthesis of 3D Faces</title>
		<author>
			<persName><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<idno type="DOI">10.1145/311535.311556</idno>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH</title>
		<imprint>
			<biblScope unit="page" from="187" to="194" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A 3D Morphable Model Learnt from 10,000 Faces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ponniah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dunaway</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2016.598</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Caricature Generator: The Dynamic Exaggeration of Faces by Computer</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Brennan</surname></persName>
		</author>
		<idno type="DOI">10.2307/1578048</idno>
	</analytic>
	<monogr>
		<title level="j">Leonardo</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="170" to="178" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Landmark Detection and 3d Face Reconstruction for Caricature Using a Nonlinear Parametric Model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.gmod.2021.101103</idno>
	</analytic>
	<monogr>
		<title level="j">Graphical Models</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">101103</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CariGANs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3272127.3275046</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title />
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
		<idno type="DOI">10.1145/641007.641040</idno>
	</analytic>
	<monogr>
		<title level="j">PicToon. ACM Multimedia</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generation of 3d Caricature by Fusing Caricature Images</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chiang</surname></persName>
		</author>
		<idno type="DOI">10.1109/icsmc.2006.384498</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Systems, Man and Cybernetics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-To-Image Translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00916</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, Utah</addrLine></address></meeting>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8789" to="8797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">StarGAN V2: Diverse Image Synthesis for Multiple Domains</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00821</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (Virtual Event (originally Seattle): CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8185" to="8194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Perceptual Caricaturization of 3d Models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cimen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bulbul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ozguc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Capin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4471-4594-3_21</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Inf. Sci</title>
		<imprint>
			<biblScope unit="page" from="201" to="207" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatic Generation of 3d Caricatures Based on Artistic Deformation Styles</title>
		<author>
			<persName><forename type="first">L</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Min Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mora</surname></persName>
		</author>
		<idno type="DOI">10.1109/tvcg.2010.76</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="808" to="821" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic Generation and Stylization of 3D Facial Rigs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Danieau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gubins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lopez</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR.2019.8798208</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Virtual Reality and 3D User Interfaces</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="784" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Curvature-domain Shape Processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eigensatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pauly</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2008.01121.x</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="241" to="250" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Method for 3d Face Modeling and Caricatured Figure Generation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fujiwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Koshimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fujimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Noguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ishikawa</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICME.2002.1035531</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Multimedia Expo</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="137" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spiralnet++: A Fast and Highly Efficient Mesh Convolution Operator</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCVW.2019.00509</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision Workshop</title>
		<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<publisher>ICCVW</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4141" to="4148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human Facial Illustrations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gooch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gooch</surname></persName>
		</author>
		<idno type="DOI">10.1145/966131.966133</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="27" to="44" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<idno>ArXiv: 1406.2661</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Carime: Unpaired Caricature Generation with Multiple Exaggerations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2021.3086722</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">3D Magic Mirror: Automatic Video to 3D Caricature Translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno>ArXiv: 1906.00544</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">544</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Example-based Facial Sketch Generation with Non-parametric Sampling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hong Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Ying-Qing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Heung-Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan-Ning</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-N</forename></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2001.937657</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Arbitrary Style Transfer in Real-Time with Adaptive Instance Normalization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.167</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>ICCV</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal Unsupervised Image-To-Image Translation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01219-9_11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018b</date>
			<biblScope unit="page" from="179179" to="196196" />
		</imprint>
	</monogr>
	<note>: ECCV)</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Webcaricature: a Benchmark for Caricature Recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<idno>ArXiv: 1703.03230</idno>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to Discover Cross-Domain Relations with Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.5555/3305381.3305573</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<meeting><address><addrLine>Sydney, NSW</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
	<note>: ICML)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>ArXiv: 1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Example-based Caricature Generation with Exaggeration</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ying-Qing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heung-Yeung</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename></persName>
		</author>
		<idno type="DOI">10.1109/pccga.2002.1167882</idno>
	</analytic>
	<monogr>
		<title level="m">Pacific Conference on Computer Graphics and Applications</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep Functional Maps: Structured Prediction for Dense Shape Correspondence</title>
		<author>
			<persName><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Remez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.603</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semisupervised Learning in Reconstructed Manifold Space for 3d Caricature Generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-8659.2009.01418.x</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2104" to="2116" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised Image-To-Image Translation Networks</title>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="DOI">10.5555/3294771.3294838</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<publisher>NIPS</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Few-shot Unsupervised Image-To-Image Translation</title>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.01065</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10550" to="10559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Three-dimensional Cartoon Facial Animation Based on Art Rules</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00371-012-0756-2</idno>
	</analytic>
	<monogr>
		<title level="j">Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1135" to="1149" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Surfaces via Seamless Toric Covers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Aigerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dym</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073616</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Which Training Methods for gans Do Actually Converge?</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno>ArXiv: 1801.04406</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Machine Learn. (Icml)</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="3478" to="3487" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved Automatic Caricature by Feature Normalization and Exaggeration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="DOI">10.1145/1186223.1186294</idno>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3dfacegan: Adversarial Nets for 3d Face Representation, Generation, and Translation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-020-01329-8</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="2534" to="2551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The Impact of Stylization on Face Recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hoyet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Danieau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Argelaguet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Avril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lecuyer</surname></persName>
		</author>
		<idno type="DOI">10.1145/3385955.3407930</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Symposium on Applied Perception</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3d Caricature Generation by Manifold Learning</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yiqiang Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Junfa Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Guanhua</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<idno type="DOI">10.1109/icme.2008.4607591</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An All-In-One Convolutional Neural Network for Face Analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="DOI">10.1109/FG.2017.137</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">How to Draw Caricatures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Redman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Computational Caricaturization of Surfaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aflalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2015.05.013</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Vis. Image Understanding</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">WarpGAN: Automatic Caricature Generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2019.01102</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Laplacian Mesh Processing</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sorkine</surname></persName>
		</author>
		<idno type="DOI">10.2312/egst.20051044</idno>
	</analytic>
	<monogr>
		<title level="s">Eurographics 2005 -State of the Art Reports</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">PsyToolkit</title>
		<author>
			<persName><forename type="first">G</forename><surname>Stoet</surname></persName>
		</author>
		<idno type="DOI">10.1177/0098628316677643</idno>
	</analytic>
	<monogr>
		<title level="j">Teach. Psychol</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="24" to="31" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">PsyToolkit: A Software Package for Programming Psychological Experiments Using Linux</title>
		<author>
			<persName><forename type="first">G</forename><surname>Stoet</surname></persName>
		</author>
		<idno type="DOI">10.3758/brm.42.4.1096</idno>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Methods</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1096" to="1104" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deformation Transfer for triangle Meshes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Popović</surname></persName>
		</author>
		<idno type="DOI">10.1145/1186562.1015736</idno>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised Cross-Domain Image Generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno>ArXiv: 1611.02200</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cross-Domain and Disentangled Face Manipulation with 3D Guidance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<idno>ArXiv: 2104.11228</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Alive Caricature from 2d to 3d</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.1109/cvpr.2018.00766</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Interactive 3d Caricature Generation Based on Double Sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1145/1631272.1631403</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">3d-carigan: An End-To-End Solution to 3d Caricature Generation from Face Photos</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv: 2003.06841</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository</title>
		<imprint>
			<publisher>CoRR</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">DualGAN: Unsupervised Dual Learning for Image-To-Image Translation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<idno type="DOI">10.1109/iccv.2017.310</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">To Stylize or Not to Stylize?</title>
		<author>
			<persName><forename type="first">E</forename><surname>Zell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aliaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jarabo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zibrek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mcdonnell</surname></persName>
		</author>
		<idno type="DOI">10.1145/2816795.2818126</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">3d Cartoon Face Generation by Local Deformation Mapping</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00371-016-1265-5</idno>
	</analytic>
	<monogr>
		<title level="j">Vis. Comput</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="717" to="727" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.244</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>