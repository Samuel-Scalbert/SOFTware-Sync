<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" />
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">1E50FA256004A2E93295D4599E99DC33</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-21T10:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract />
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1.">INTRODUCTION</head><p>In recent years, deep learning-based single-channel speech enhancement methods have greatly improved speech enhancement systems' speech quality and intelligibility. These methods are often trained in a supervised setting and can be divided into time-domain and frequency-domain methods. The time-domain methods <ref type="bibr">[1]</ref><ref type="bibr">[2]</ref><ref type="bibr">[3]</ref> use the neural network to map noisy speech waveform to clean speech waveform directly. The frequency-domain methods <ref type="bibr">[4]</ref><ref type="bibr">[5]</ref><ref type="bibr">[6]</ref><ref type="bibr">[7]</ref> typically use the noisy spectral feature (e.g., complex spectrum, magnitude spectrum) as the input of a neural model. Learning target is the spectral feature of clean speech or a certain mask (e.g., Ideal Binary Mask <ref type="bibr">[8]</ref>, Ideal Ratio Mask <ref type="bibr">[9]</ref>, complex Ideal Ratio Mask (cIRM) <ref type="bibr">[10]</ref>). In general, due to the high dimension and the lack of apparent geometric structure for the time domain signal, the frequency domain methods still dominate the vast majority of speech enhancement methods. In this paper, we focus on real-time singlechannel speech enhancement in the frequency domain.</p><p>In our previous work <ref type="bibr">[11]</ref>, a sub-band-based method was proposed for single-channel speech enhancement. Unlike the traditional full-band-based methods, the method performed in a sub-band style:</p><p>The input of the model consists of one frequency, together with several context frequencies. The output is a prediction of the clean speech target for the corresponding frequency. All the frequencies are processed independently. This method is designed on the following grounds. (1) It learns the frequency-wise signal stationarity to discriminate between speech and stationary noise. It is known that speech is non-stationary, while many types of noise are relatively stationary. The temporal evolution of frequency-wise STFT magnitude reflects the stationarity, which is the foundation for the conventional noise power estimators <ref type="bibr">[12,</ref><ref type="bibr">13]</ref> and speech enhancement methods <ref type="bibr">[14,</ref><ref type="bibr">15]</ref>. (2) It focuses on the local spectral pattern presented in the current and context frequencies. The local spectral pattern has been proved to be informative for discriminating between speech and other signals. This method was submitted to the DNS challenge <ref type="bibr">[16]</ref> in INTERSPEECH 2020 and ranked the fourth place out of the 16 real-time track submissions.</p><p>The sub-band model meets the DNS challenge's real-time requirement, and the performance is also very competitive. However, since it cannot model the global spectral pattern and exploit the longdistance cross-band dependencies. Especially for the sub-band with an extremely low signal-to-noise ratio (SNR), the sub-band model can hardly recover the clean speech, while it will be possible with the help of full-band dependency. On the other hand, the full-band models <ref type="bibr">[4,</ref><ref type="bibr">5]</ref> are trained to learn the regression between the highdimensional input and output, lacking a mechanism dedicated to the sub-band information, such as the signal stationarity.</p><p>This paper proposes a full-band and sub-band fusion model named <software ContextAttributes="used">FullSubNet</software> to address the above problems. Based on plenty of preliminary experiments, the <software ContextAttributes="used">FullSubNet</software> is designed as a series connection of the full-band model and sub-band model. In short, the full-band model's output is input to the sub-band model. Through effective joint training, these two models are jointly optimized. The <software ContextAttributes="used">FullSubNet</software> can capture the global (full-band) context while retaining the ability to model signal stationarity and attend the local spectral pattern. Like the sub-band model, the <software ContextAttributes="used">FullSubNet</software> still meets the real-time requirement and can exploit future information within a reasonable latency. We evaluate the <software ContextAttributes="used">FullSubNet</software> on the DNS challenge (INTERSPEECH 2020) dataset. Experimental results show that the <software ContextAttributes="used">FullSubNet</software> prominently outperforms both the sub-band model <ref type="bibr">[17]</ref> and a pure full-band model with a larger amount of parameters with the <software ContextAttributes="used">FullSubNet</software>, which indicates that the sub-band information and the full-band information are complementary. The proposed fusion model is effective for integrating them. Besides, we also compare the performance with the top-ranked methods in the DNS challenge, and the results show that our objective performance measures are better than them.</p></div>
<div><head n="2.">METHOD</head><p>We use the representation of speech signal in the short-time fourier transform (STFT) domain:</p><formula xml:id="formula_0">X(t, f ) = S(t, f ) + N (t, f ).</formula><p>(1)</p><p>where X(t, f ), S(t, f ) and N (t, f ) respectively represent the complex-valued time-frequency (T-F) bin of noisy speech, noisefree speech (the reverberant image signal received at the microphone) and interference noise at time frame t and frequency bin f</p><formula xml:id="formula_1">with t = 1, • • • , T and f = 0, • • • , F -1.</formula><p>T and F denote the total number of frames and frequency bins, respectively. This paper focuses only on the denoising task, and the target is to suppress noise N (t, f ) and recover the reverberant speech signal S(t, f ). We propose a full-band and sub-band fusion model to accomplish this task, including a pure full-band model Gfull and a pure sub-band model Gsub. The basic workflow is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Next, we will introduce each part in detail.</p></div>
<div><head n="2.1.">Input</head><p>Previous works <ref type="bibr">[4,</ref><ref type="bibr">5,</ref><ref type="bibr">11,</ref><ref type="bibr">17]</ref> have proved that magnitude spectral feature can provide crucial clues about the global spectral pattern at full-band, while the local spectral pattern and signal stationarity at sub-band. Therefore, we use the noisy full-band magnitude spectral features</p><formula xml:id="formula_2">X(t) = [|X(t, 0)|, • • • , |X(t, f )|, • • • , |X(t, F -1)|] T ∈ R F . (2)</formula><p>We use its sequence</p><formula xml:id="formula_3">X = (X(1), • • • , X(t), • • • , X(T ))<label>(3)</label></formula><p>as the input of the full-band model Gfull. Then, Gfull can capture the global contextual information and outputs a spectral embedding with the size being the same as X, which is expected to provide complementary information to the following sub-band model Gsub.</p><p>The sub-band model Gsub predicts the frequency-wise cleanspeech target according to the signal stationarity and local spectral mode encoded in the noisy sub-band signal, as well as the full-band model's output. In detail, we take a time-frequency point |X(t, f )| and its adjacent 2 × N time-frequency points as a sub-band unit. N is the number of neighbor frequencies considered on each side. For boundary frequencies, with f -N &lt; 0 or f + N &gt; F -1, circular Fourier frequencies are used. We concatenate the sub-band unit and the output of the full-band model, denoted as Gfull(|X(t, f )|), as the input of the sub-band model Gsub,</p><formula xml:id="formula_4">x(t, f ) =[|X(t, f -N )|, • • • , |X(t, f -1)|, |X(t, f )| (4) |X(t, f + 1)|, • • • , |X(t, f + N )|, Gfull(|X(t, f )|)] T ∈ R 2N +2 .</formula><p>For the frequency f , the input sequence of Gsub is</p><formula xml:id="formula_5">x(f ) = (x(1, f ), • • • , x(t, f ), • • • , x(T, f )).<label>(5)</label></formula><p>In this sequence, the temporal evolution along with time axis reflect the signal stationarity, which is an efficient cues to discriminate between speech and relatively stationary noise Since the full-band spectral feature X(t) contains F frequencies, we eventually generate F independent input sequences for Gsub with a dimension of 2N + 2 for each.</p></div>
<div><head n="2.2.">Learning target</head><p>There is no doubt that the precise estimation of phase can provide more hearing perception quality improvement, especially in low signal-to-noise ratio (SNR) conditions. However, the phase is wrapped in -π ∼ π and has chaotic data distribution, which makes it not easy to estimate. Instead of estimating the phase directly, like the previous works <ref type="bibr">[11,</ref><ref type="bibr">17]</ref>, we adopt the complex Ideal Ratio Mask (cIRM) as our model's learning target. Follow [10], we use hyperbolic tangent to compress cIRM in training and use inverse function to uncompressed mask in inference (K = 10, C = 0.1). We denote cIRM as y(t, f ) ∈ R 2 for one T-F bin. The sub-band model takes as input sequence x(f ) for the frequency f and then predicts the cIRM sequence</p><formula xml:id="formula_6">y(f ) = (y(1, f ), • • • , y(t, f ), • • • , y(T, f )).</formula><p>(6)</p></div>
<div><head n="2.3.">Model architecture</head><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows the architecture of the <software ContextAttributes="used">FullSubNet</software>. The full-band and sub-band models in the FullSuNet have the same model structure, including two stacked unidirectional LSTM layers and one linear (fully connected) layer. The LSTM of the full-band model contains 512 hidden units in each layer and uses ReLU as the output layer's activation function. The full-band model outputs a F -dimensional vector at each time step, with one element for each frequency. The sub-band units are then concatenated with this vector frequency by frequency to form F independent input samples for the sub-band model following Equation <ref type="formula">4</ref>. According to our previous experiments, the sub-band model is not necessary to be as large as the full-band model, and thus 384 hidden units are used in each layer of LSTM. According to the settings in [10], the output layer of the sub-band model does not use activation functions. It is important to note that all the frequencies share one unique sub-band network (and its parameters). During training, considering the limited LSTM memory capacity, the input-target sequence pairs are generated with a constant-length sequence.</p><p>To make the model easier to optimize, the input sequence must be normalized to equalize the input levels. For the full-band model, we empirically calculate the mean value µfull of the magnitude spectral features on the full-band sequence X and normalize the input sequence as X µ full . The sub-band model process the frequencies independently. For frequency f , we caculate the mean value µsub(f ) on the input sequence x(f ) and normalize the input sequence as x(f ) µ sub (f ) . In the real-time inference stage, we usually use the cumulative normalization method <ref type="bibr">[18,</ref><ref type="bibr">19]</ref>, i.e., at each time, the mean value used for normalization is computed using all the available frames. However, in the practical real-time speech enhancement system, the speech signal is usually silent initially, which means that the speech signal's beginning part is mostly invalid. In this work, to better show the <software ContextAttributes="used">FullSubNet</software>'s performance regardless of the normalization problem, we directly use µfull and µsub(f ) computed on the entire test clip to perform normalization during inference.</p><p>Same as the method mentioned in [17], our proposed method supports output delay, which enables the model to explore future information within a reasonably small delay. As shown in the Fig. <ref type="figure" target="#fig_0">1</ref>, to infer y(t -τ ), the future time steps, i.e. x(t -τ + 1), • • • , x(t), are provided in the input sequence.</p></div>
<div><head n="3.">EXPERIMENTAL SETUP</head></div>
<div><head n="3.1.">Datasets</head><p>We evaluated the <software ContextAttributes="used" /> on the DNS Challenge (INTER-SPEECH 2020) dataset <ref type="bibr">[16]</ref>. The clean speech set includes over 500 hours of clips from 2150 speakers. The noise dataset includes over 180 hours of clips from 150 classes. To make full use of the dataset, we simulate the speech-noise mixture with dynamic mixing during model training. In detail, before the start of each training epoch, 75% of the clean speeches are mixed with randomly selected room impulse responses (RIR) from (1) the Multichannel Impulse Response Database <ref type="bibr">[20]</ref> with three reverberation times (T60) 0.16 s, 0.36 s, and 0.61 s. (2) the Reverb Challenge dataset [21] with three reverberation times 0.3 s, 0.6 s and 0.7 s. After that, the speech-noise mixtures are dynamically generated by mixing the clean speech (75% of them are reverberant) and noise with a random SNR in between -5 and 20 dB. The total data "seen" by the model is over 5000 hours after ten epochs of training. The DNS Challenge provides a publicly available test dataset, including two categories of synthetic clips, i.e., without and with reverberations. Each category has 150 noisy clips with SNR levels distributed in between 0 dB to 20 dB. We use this test dataset for evaluation.</p></div>
<div><head n="3.2.">Implementation</head><p>The signals are transformed to the STFT domain using a 512-sample (32 ms) Hanning window with a frame step of 256 samples. We use <software ContextAttributes="used">PyTorch</software> to implement the <software ContextAttributes="used">FullSubNet</software>. Adam optimizer is used with a learning rate of 0.001. The sequence length for training is set to T = 192 frames (about 3 s). According to the real-time requirement of the DNS Challenge (INTERSPEECH 2020), we set τ = 2, which exploits two future frames to enhance the current frame, and uses a 16 × 2 = 32ms look ahead. As in <ref type="bibr">[17]</ref>, we set 15 neighbor frequencies for each side of the input frequency of the sub-band model in the <software ContextAttributes="used">FullSubNet</software>.</p></div>
<div><head n="3.3.">Baselines</head><p>To testify the full-band and sub-band fusion method's effectiveness, we compare with the following two models, which use the same experimental settings and learning target (cIRM) as the <software>FullSubNet</software>.</p><p>• Sub-band model [17]: The sub-band model has achieved very competitive performance in the DNS-Challenge (the fourth place of the real-time track). To compare performance fairly, like to train the <software>FullSubNet</software>, we use dynamic mixing during training.</p><p>• Full-band model: We construct a pure full-band model, which contains three LSTM layers with 512 hidden units for each layer. The full-band model's architecture, i.e., a stack of LSTM layers, is actually widely used for speech enhancement, such as in <ref type="bibr">[6,</ref><ref type="bibr">26]</ref>. This model is slightly larger than the proposed fusion model, and thence the comparison would be fair enough.</p><p>In addition to these two models, we also compared with the topranked methods in the DNS challenge <ref type="bibr">(INTERSPEECH 2020), including NSNet [22]</ref>, <ref type="bibr">DTLN [23]</ref>, , <ref type="bibr">DCCRN [19]</ref> and <ref type="bibr">PoCoNet [25]</ref>.</p></div>
<div><head n="4.">RESULTS</head></div>
<div><head n="4.1.">Comparison with the baselines</head><p>In the last three rows of Table <ref type="table" target="#tab_0">1</ref>, we compare the performance of the sub-band model, the full-band model, and the <software ContextAttributes="used">FullSubNet</software>. "# Para" and "Look Ahead" in the table respectively represent the parameter amount of the model and the length of used future information. "With Reverb" means that the noisy speeches in the test dataset have not only noise but also a certain degree of reverberation, which significantly increases the difficulty for speech enhancement. "Without Reverb" means that the noisy speeches in the test dataset have only noise. For a fair comparison, these three models use the same training target (cIRM), experimental settings, and look ahead.</p><p>From the table, we can find that most of the full-band model's evaluation scores are better than the ones of the sub-band model, as the full-band model exploits the wide-band information using a larger network. It is interesting to find that, relative to the full-band model, the sub-band model seems more effective for the "With Reverb" data, as the superiority of the full-band model for "With Reverb" is smaller than the one for "Without Reverb." This indicates that the sub-band model effectively models the reverberation effect by focusing on the temporal evolution of the narrow-band spectrum. The possible reason is that the cross-band dependency of the reverberation effect is actually much lower than the one of signal spectra. Regarding the <software>FullSubNet</software>: (1) Although the sub-band model's performance is already very competitive, after integrating the fullband model (stacked by two LSTM layers and one linear layer), the model performance has been dramatically improved. This improvement shows that the global spectral pattern and the long-distance cross-band dependencies are essential for speech enhancement. (2) The performance of the <software ContextAttributes="used">FullSubNet</software> also significantly exceeds the full-band model. We must first point out that this improvement does not come from using more parameters. In fact, the <software ContextAttributes="used">FullSubNet</software> (two layers of full-band LSTM plus two layers of sub-band LSTM) has even fewer parameters than the full-band model (three layers of fullband LSTM). After integrating the sub-band model, the FullSub-Net inherits the sub-band model's unique ability, namely exploiting signal stationarity and local spectral patterns, and the capability of modeling the reverberation effect. The apparent superiority of the <software ContextAttributes="used">FullSubNet</software> over the full-band model demonstrates that the information exploited by the sub-band model is indeed not learned by the full-band model, which is complementary to the full-band model. Overall, these results testify that the proposed fusion model successfully integrates the virtues of full-band and sub-band techniques.</p></div>
<div><head n="4.2.">Comparison with the state-of-the-art methods</head><p>In Table <ref type="table" target="#tab_0">1</ref>, in addition to showing that the <software ContextAttributes="used">FullSubNet</software> can effectively integrate two complementary models, we also compare its performance with the top-ranked methods in DNS Challenge <ref type="bibr">(INTER-SPEECH 2020)</ref>. The "Rank" column in the table indicates whether to support real-time processing and the challenge's ranking. e.g., "RT-8" means the eighth place of the real-time (RT) track. "NRT-1" means the first place of the non-real-time (NRT) track.</p><p>In Table <ref type="table" target="#tab_0">1</ref>, NSNet is the official baseline method of the DNS challenge, which uses a compact RNN to enhance the noisy shorttime speech spectra in a single-frame-in, single-frame-out manner. We use the DNS challenge recipe provided in the asteroid toolkit 1 to implement and train NSNet. The training data are generated using the method mentioned in <ref type="bibr">[17]</ref>. In the table, no matter which metric, our proposed method greatly surpasses NSNet with all metrics.</p><p>DTLN, Conv-TasNet, DCCRN, and PoCoNet are the top-ranked methods in the DNS challenge's subjective listening test. To ensure the fairness of comparison, we directly quote performance scores from their original papers. The vacant place in the table means that the corresponding score was not reported in the original paper. <ref type="bibr">DTLN [23]</ref> is capable of real-time processing. It combines the STFT operation and a learned analysis and synthesis basis into 1 https://github.com/mpariente/asteroid/tree/mas ter/egs/dns_challenge a stacked-network with less than one million parameters. <ref type="bibr">[24]</ref> proposed a low-latency Conv-TasNet. Conv-TasNet [18] is a widelyused time-domain audio separation network, which has a large computational complexity. Consequently, the low-latency Conv-TasNet does not satisfy the real-time requirement. <ref type="bibr">DCCRN [19]</ref> simulates the complex-valued operation inside the convolution recurrent network. It won the first place of the real-time track. PoCoNet [25] is a convolutional neural network with frequency-positional embeddings employed. Besides, a semi-supervised method is adopted to increase conversational training data by pre-enhancing the noisy datasets. It won the first place of the non-real-time track. These methods cover a large range of advanced deep learning-based speech enhancement techniques and represent the state-of-the-arts to an extent. The original paper of these methods provided the evaluation results on the same test set used in this work but with not all the metrics used in this work. It can be seen that the proposed fusion model achieves considerably better objective scores than all of them on this limited dataset. The performance of PoCoNet is close to ours, but it is a non-real-time model with a much larger network (about 50 M parameters). The proposed <software ContextAttributes="used">FullSubNet</software> provides a new model dedicated to the full-band/sub-band fusion, which is likely not conflicting with the advanced techniques employed in these state-of-the-art models. Therefore, it worth expecting that speech enhancement capability can be further improved by properly combining them.</p><p>Regarding the computational complexity, the one STFT frame (32 ms) processing time of the proposed model (<software>PyTorch</software> implementation) is 10.32 ms tested on a virtual quad-core CPU (2.4 GHz) based on Intel Xeon E5-2680 v4, which obviously meets the realtime requirement. Later, we will open-source the code and pretrained models, and show some enhanced audio clips at https: //github.com/haoxiangsnr/<software ContextAttributes="used">FullSubNet</software>.</p></div>
<div><head n="5.">CONCLUSION</head><p>In this paper, we propose a full-band and sub-band fusion model, named as <software ContextAttributes="used">FullSubNet</software>, for real-time single-channel speech enhancement. This model is designed to integrate the advantages of the full-band and the sub-band models, that is, it can capture the global (full-band) spectral information and the long-distance cross-band dependencies, meanwhile retaining the ability to modeling signal stationarity and attending the local spectral pattern. On the DNS challenge <ref type="bibr">(INTERSPEECH 2020)</ref> test dataset, we demonstrated that the sub-band information and the full-band information are complementary, and the <software ContextAttributes="used">FullSubNet</software> can effectively integrate them. We also compared the performance with some top-ranked methods for the DNS challenge, and the results show that the <software ContextAttributes="used">FullSubNet</software> outperforms these methods.</p></div><figure xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Diagram of the proposed FullSubNet. The second line in the rectangle describes the dimensions of the data at the current stage, e.g., "1 (F )" represents one F -dimensional vector. "F (2N + 1)" represents F independent (2N + 1)-dimensional vectors.</figDesc><graphic coords="3,55.84,72.00,240.95,281.37" type="bitmap" /></figure>
<figure xml:id="fig_1"><head /><label /><figDesc>. The noisy sub-band spectra (composed of 2N + 1 frequencies) and its temporal dynamics provides the local spectral pattern, which can be learned by the dedicated sub-band model. While the signal stationarity cues and the local pattern are actually present in the input of the full-band model Gfull as well, however, they are not especially learned by the full-band model Gfull. Consequently, the sub-band model Gsub still learns some extra/different information relative to the full-band model Gfull. Meanwhile, the output of the full-band model Gfull provide some complementary information not seen by the sub-band model Gsub.</figDesc></figure>
<figure type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The performance in terms of WB-PESQ [MOS], NB-PESQ [MOS], STOI [%], and SI-SDR [dB] on the DNS challege test dataset.</figDesc><table><row><cell>Method</cell><cell># Para (M)</cell><cell>Look Ahead (ms)</cell><cell>Rank</cell><cell cols="6">With Reverb WB-PESQ NB-PESQ STOI SI-SDR WB-PESQ NB-PESQ STOI SI-SDR Without Reverb</cell></row><row><cell>Noisy</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>1.822</cell><cell>2.753</cell><cell>86.62 9.033</cell><cell>1.582</cell><cell>2.454</cell><cell>91.52 9.071</cell></row><row><cell>NSNet [22]</cell><cell>5.1</cell><cell>0</cell><cell>-</cell><cell>2.365</cell><cell>3.076</cell><cell>90.43 14.721</cell><cell>2.145</cell><cell>2.873</cell><cell>94.47 15.613</cell></row><row><cell>DTLN [23]</cell><cell>1.0</cell><cell /><cell>RT-8</cell><cell /><cell>2.70</cell><cell>84.68 10.53</cell><cell /><cell>3.04</cell><cell>94.76 16.34</cell></row><row><cell>Conv-TasNet [24]</cell><cell>5.08</cell><cell>33</cell><cell>NRT-5</cell><cell>2.750</cell><cell /><cell /><cell>2.73</cell><cell /><cell /></row><row><cell>DCCRN-E [19]</cell><cell>3.7</cell><cell>37.5</cell><cell>RT-1</cell><cell /><cell>3.077</cell><cell /><cell /><cell>3.266</cell><cell /></row><row><cell>PoCoNet [25]</cell><cell>50</cell><cell /><cell>NRT-1</cell><cell>2.832</cell><cell /><cell /><cell>2.748</cell><cell /><cell /></row><row><cell cols="2">Sub-band Model [17] 1.3</cell><cell>32</cell><cell>RT-4</cell><cell>2.650</cell><cell>3.274</cell><cell>90.53 14.673</cell><cell>2.369</cell><cell>3.052</cell><cell>94.24 16.153</cell></row><row><cell>Full-band Model</cell><cell>6.0</cell><cell>32</cell><cell>-</cell><cell>2.681</cell><cell>3.344</cell><cell>90.64 13.580</cell><cell>2.731</cell><cell>3.256</cell><cell>95.71 16.190</cell></row><row><cell>FullSubNet</cell><cell>5.6</cell><cell>32</cell><cell>-</cell><cell>2.969</cell><cell>3.473</cell><cell>92.62 15.750</cell><cell>2.777</cell><cell>3.305</cell><cell>96.11 17.290</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl />
			</div>
		</back>
	</text>
</TEI>