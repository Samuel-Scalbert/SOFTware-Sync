<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" />
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">3E3E1753F1028F297DFBF368DC145F4E</idno>
					<idno type="DOI">10.1007/s11042-020-09901-7</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-21T10:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract />
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head>Duration modelling and evaluation for Arabic statistical parametric speech synthesis</head><p>Imene Zangar • Zied Mnasri • Vincent Colotte • Denis Jouvet the date of receipt and acceptance should be inserted later Abstract Sound duration is responsible for rhythm and speech rate. Furthermore, in some languages phoneme length is an important phonetic and prosodic factor. For example, in Arabic, gemination and vowel quantity are two important characteristics of the language. Therefore, accurate duration modelling is crucial for Arabic TTS systems. This paper is interested in improving the modelling of phone duration for Arabic statistical parametric speech synthesis using DNN-based models. In fact, since a few years, DNN have been frequently used for parametric speech synthesis, instead of HMM. Therefore, several variants of DNN-based duration models for Arabic are investigated. The novelty consists in training a specific DNN model for each class of sounds, i.e. short vowels, long vowels, simple consonants and geminated consonants. The main idea behind this choice is the improvement that we already achieved in the quality of Arabic parametric speech synthesis by the introduction of two specific features of Arabic, i.e. gemination and vowel quantity into the standard HTS feature set. Both objective and subjective evaluations show that using a specific model for each class of sounds leads to a more accurate modelling of the phone duration in Arabic parametric speech synthesis, outperforming the state-of-the-art duration modelling systems.</p></div>
<div><head n="1">Introduction</head><p>Text-To-Speech synthesis (TTS) has been historically used as help for people with visual impairments, for reading ebooks and messages. Now it is being used worldwide in a large range of applications varying from consumer electronics, talking devices and especially as a cost-effective alternative to human speakers, in charge of broadcasting routine messages like news bulletins, weather forecasts and traffic alerts. More recently, it started to be also used for language learning and online translation.</p><p>Overview of Text-To-Speech systems Several TTS techniques have been developed since more than half a century: formant synthesis [1], articulatory synthesis <ref type="bibr">[2]</ref>, concatenative methods, i.e. diphone synthesis <ref type="bibr">[3,</ref><ref type="bibr">4]</ref> and unit selection synthesis <ref type="bibr">[5]</ref>, and parametric speech synthesis based either on hidden Markov models (HMM) or on deep neural networks (DNN). In fact, this timeline order depends on the progress of storage abilities and of computational resources. Hence, the first methods, i.e. formant synthesis and diphone synthesis need only a small database, whereas unit selection synthesis has been feasible only when it had been possible to store huge databases of speech segments. Finally, statistical parametric synthesis methods, based on HMM and DNN, were promoted by the development of complex algorithms able to deal with the corresponding computational load. Therefore, each technique has its advantages and drawbacks that depend on the background approach: whereas formant synthesis and diphone synthesis do not require high memory, the output sound quality is not quite satisfactory. On the opposite side, unit selection synthesis provides a high quality sound, at the cost of using a huge database. Finally, parametric speech synthesis represents a trade-off between relatively small memory and high quality of sound.</p></div>
<div><head>Parametric speech synthesis</head><p>The first parametric speech synthesis technique was based on HMM to model and predict the speech parameters, i.e. duration, fundamental frequency (F0) and spectrum. Decision trees were used to share context-dependent parameters, and thus to select the best-fitting HMM parameters according to the available contextual information <ref type="bibr">[6]</ref>. The generated parameters were fed into a vocoder, like MLSA (Mel Log Spectrum Approximation) <ref type="bibr">[7]</ref>, to generate the output speech signal. Therefore, it was possible to develop this technique using less speech data than for unit selection. However, the naturalness of its output speech has been so far less appreciated than that of unit selection synthesis, mainly due to oversmoothing, acoustic modelling and vocoding <ref type="bibr">[8]</ref>. DNN-based speech synthesis is another parametric speech synthesis approach. Instead of relying on HMMs and associated decision trees to predict the speech parameters, DNNs map the input contextual features to the corresponding speech parameters <ref type="bibr">[9]</ref>. Then, in the same way, the output speech is generated by a vocoder. Furthermore, recent relevant developments in sequence-to-sequence DNN-based TTS such as <software ContextAttributes="used">Tacotron</software> <ref type="bibr">[10,</ref><ref type="bibr">11]</ref> have given good subjective evaluation results. These models outperform state-of-the-art results through combining alignment, duration and acoustic parameters modelling. It should be noted that in the recent benchmark evaluations of TTS techniques, parametric speech synthesis, either based on HMM or DNN, has always been ranked amongst the best quality techniques, notwithstanding the development of novel techniques, mainly based on deep learning <ref type="bibr">[56]</ref>. Such a good rating can be explained by the particularity of parametric speech synthesis, based on modelling prosody specifically for each target language.</p></div>
<div><head>Prosody modelling</head><p>Prosody modelling is a key component in any TTS system, especially in parametric speech synthesis. Prosody refers to intonation, rhythm and loudness <ref type="bibr">[12]</ref>. Phonetically speaking, these aspects are expressed by F0, duration and energy. Except unit-selection TTS, where signal modification is avoided [5], most concatenative speech synthesis systems rely on signal modification to adapt the values of the prosodic parameters of the database units to the predicted ones, in order to ensure intelligibility and naturalness of the output speech. In the case of parametric speech synthesis, duration and acoustic parameters (F0 and spectrum) are modelled either by HMM or by neural networks. These models rely on label features that cover most of contextual, linguistic and phonological information that may influence the output parameters, i.e. duration, F0 and spectrum. A medium size database, e.g. 450 sentences in <ref type="bibr">[6]</ref>, was used to train the context-dependent HMM models, nevertheless larger databases lead to better speech synthesis quality. The context-dependent HMMs are clustered using a decision tree, whose binary questions refer to the label features. The binary tree is constructed by iteratively selecting the most relevant question, that is the question that leads to the largest log-likelihood gain of the training data. The size of the decision tree depends on some threshold of the training criterion, i.e. MDL (Minimum Description Length) value <ref type="bibr">[13]</ref> or cross-validation <ref type="bibr">[6]</ref>.</p></div>
<div><head>Prosody modelling for parametric TTS</head><p>The main advantage of parametric speech synthesis consists in its ability to model the acoustic parameters for a wide range of contexts. In fact, thanks to decision trees clustering where parameters distributions are shared, the effect of unseen contexts and data sparsity is reduced. Though parametric speech synthesis has been proved to produce accurate prosody models in many languages, such as Japanese [6], English [8], Arabic <ref type="bibr">[14]</ref>, etc., it did not succeed to reach the unit-selection speech quality as reported in <ref type="bibr">[15]</ref>. This is due to several reasons, namely vocoding, acoustic modelling and oversmoothing. As far as vocoding is concerned, since the output speech is generated using MLSA filter <ref type="bibr">[7]</ref>, based on a pulse train and white noise excitation, a buzz effect is usually perceived. To cope with this issue, many vocoders were tried out, to enhance the quality of the output speech, such as <software ContextAttributes="used">STRAIGHT</software> <ref type="bibr">[15]</ref> and more recently <software ContextAttributes="used">WORLD</software> <ref type="bibr">[16]</ref>. For acoustic modelling, though decision trees are able to match the contextual features to the adequate HMM model, the whole system suffers from the simple frame-wise acoustic parameter modelling <ref type="bibr">[9]</ref>. Actually, speech and more particularly prosody parameters generation is a continuous process, which might be considered as recurrent rather than simply overlapping. To resolve this problem, many modifications were brought to the HMM structure, to enhance their ability to model acoustic parameters taking into account their inherent dependencies <ref type="bibr">[9]</ref>.<ref type="bibr">[9]</ref>. The last issue, i.e. oversmoothing, is due to using statistical averaging in the training phase, to reduce the data sparsity. Besides, in the synthesis phase, dynamic constraints are used. Both techniques yield in over-smoothed trajectories of the acoustic parameters (F0 and spectrum), so that a muffling effect is perceived by the listener, because the micro-prosodic variations cannot be captured <ref type="bibr">[9]</ref>. To overcome this effect, simple solutions like post-filtering or explicitly using the training data as constraints in the parameter prediction algorithm were suggested <ref type="bibr">[9]</ref>. Another solution is to use multiple-level statistics, like global variance (GV) when generating speech parameters trajectories </p></div>
<div><head>Duration modelling for parametric TTS</head><p>Several approaches have been developed to model segmental duration since the beginning of TTS technology. These approaches can be classified into two main families: rule-based approaches and data-driven ones. In the first family, an explicit formulation of segment duration is found out, where phone and/or syllable duration is calculated using analytic formulas [1], or assuming the existence of a common compression/extension factor for all phone durations within a syllable <ref type="bibr">[17]</ref>. In <ref type="bibr">[18]</ref>, the duration of a segment is calculated as the sum of products of a hypothetic intrinsic duration multiplied by some contextual factors like phoneme class, stress and place of articulation. However, such empiric rules do not provide high accuracy, and were criticized for the lack of fundamental and consistent proofs <ref type="bibr">[12]</ref>. In the second family, segment duration is obtained through training. Particularly, in HMM-based speech synthesis, state duration distributions are estimated. Each state duration is explicitly modelled by a Gaussian distribution. Context-dependent decision trees are used to cluster the duration distributions. In the synthesis part, the state duration is obtained by mapping the contextual feature vector to the corre-sponding state duration HMM. Finally, the phone duration is obtained by summing its state durations <ref type="bibr">[6]</ref>. Nevertheless, the main critic of HMM-based speech synthesis, and in particular HMM-based phone duration modelling is the averaged output, since HMM models are mostly relying on Gaussian distribution <ref type="bibr">[8]</ref>. Therefore, a better way to model the micro-variations of prosody, including phone duration, is looked for through replacing HMM by DNN in parametric speech synthesis.</p></div>
<div><head>Arabic parametric TTS</head><p>Modern Standard Arabic (MSA), which is widely used among all Arab-speaking countries as the official and literary language, has 28 consonants and three vowels, /a/, /u/ and /i/ <ref type="bibr">[19]</ref>. Most consonants could be geminated (doubled) which is indicated in writing through adding the specific diacritic sign (shadda) whereas each vowel has a short and a long version. One of the main components of prosody is segmental duration. In fact, rhythm, speech rate, and partly intonation, depend on units durations. Moreover, in some languages like Arabic, a change in vowel quantity (long vs. short) and in gemination alters the meaning of the word, e.g. /aradha/ "he presented", /a:radha/ (he objected) and /arradha/(he exposed).</p><p>Application of TTS to the Arabic language has started since the beginning of TTS technology. Unit selection TTS was successfully adapted to Arabic <ref type="bibr">[20]</ref> as well as the HMM-based TTS toolkit (HTS) <ref type="bibr">[14]</ref>. Also in [14], the synthesis filter was modified to improve its quality: the set of acoustic parameters consisted of spectrum amplitude and multi-band voicing decision, and the MLSA filter has been replaced by a multi-band excitation vocoder <ref type="bibr">[21]</ref>. Prosody and more particularly duration modelling has been studied specifically for Arabic to be integrated into Arabic TTS using diphone synthesis techniques like PSOLA and MBROLA. Such duration models were based either on rule-based methods, i.e. explicit formulas for phoneme duration in <ref type="bibr">[22]</ref>, or data-driven techniques like artificial neural networks (ANN) in <ref type="bibr">[23,</ref><ref type="bibr">24]</ref>. However, prosody modelling for Arabic can be improved by implementing specific models, taking care of its characteristics. In fact, such an approach has been proved to be efficient in improving the overall quality of Arabic speech synthesis, in our previous work <ref type="bibr">[25]</ref>.</p></div>
<div><head>Paper's scope and organization</head><p>Since the prosodic module is one of the key components of any parametric speech synthesis system, the work presented in this paper aims to improve prosody modelling in general, and phone duration modelling in particular. This goal is met by finding out DNN-based duration models that allow enhancing the quality of HMM-based parametric speech synthesis for Arabic. DNN-based duration models are developed using additional linguistic features of Arabic, i.e. vowel quantity and gemination, which have recently been successfully introduced into HTS system for Arabic TTS <ref type="bibr">[25]</ref>. The novelty of the proposed approach consists in establishing a specific duration model for each class of Arabic phonemes. The duration predicted externally by the specific DNN models is then introduced into the HTS system for generating F0 and spectrum features, and then the speech output. Both objective and subjective evaluations show that using a specific model for each class of sounds leads to a more accurate modelling of the phone duration. The objective evaluation results have already been briefly presented in <ref type="bibr">[26]</ref>, whereas in this paper it will be extended and a subjective assessment will be added.</p><p>The rest of the paper is organized as follows, Section 2 presents duration modelling for parametric speech synthesis, starting from the baseline HMM model to current DNN ones, including other external models tried out for parametric speech synthesis. Section 3 describes DNN and recurrent neural network (RNN) duration models that we investigated for Arabic, with a special care to recall the modifications of the input features set to meet Arabic language requirements, that have been recently proposed for Arabic parametric speech synthesis using HMM <ref type="bibr">[25]</ref>. Section 4 details the objective evaluation measures and subjective listening test results. Finally, a discussion and a conclusion end the paper.</p></div>
<div><head n="2">Duration modelling for parametric speech synthesis</head><p>In HMM-based parametric speech synthesis, prosody and acoustic modules rely on HMMs to generate duration and excitation features, i.e. log(F0) and the spectral parameters (and their dynamic counterparts ∆ and ∆∆). However, in order to improve naturalness, other approaches have been investigated in a bid to replace the HMM duration model.</p></div>
<div><head>Data-driven duration modelling for parametric speech synthesis</head></div>
<div><head>HSMM duration model for parametric speech synthesis</head><p>For the modelling of the state durations in parametric speech synthesis systems, explicit state duration models have been introduced. As it is conventional in context-dependent HMMs, context-dependent parameters are clustered through the use of decision trees. This is also the case for the state duration distributions; and the decision trees are built using the contextual features (previous and following phonemes, position in the syllable, in the word, in the phrase, etc.). During synthesis, the duration Gaussian distributions used are those returned by the decision tree, according to the contextual features, and the state durations are determined in order to maximise the duration probabilities.</p><p>Hidden-semi Markov models <ref type="bibr">[27,</ref><ref type="bibr">28]</ref> based on explicit duration Gaussian models were used to model duration in the HMM-based speech synthesis system (HTS) <ref type="bibr">[29]</ref>. However, they did not considerably improve the naturalness of parametric speech synthesis output speech in comparison to unit selection synthesis, as reported in <ref type="bibr">[8]</ref>.</p><p>Other data-driven duration models for parametric speech synthesis One of the most perceived weaknesses of parametric speech synthesis consists is bland prosody, partly due to over-averaging, particularly in duration, which results from the use of Gaussian distributions <ref type="bibr">[30]</ref>. To enhance the ability of parametric speech synthesis to generate more natural rhythm and speech rate, many duration models were proposed to refine or to replace the original HSMM one. For the refinement methods, the HSMM-based state duration modelling was extended to the phoneme and/or the syllable level in <ref type="bibr">[31,</ref><ref type="bibr">32]</ref>. In the same way, the diagonal Gaussian distribution of the HSMM model was replaced by a full-covariance Gaussian distribution in <ref type="bibr">[33]</ref>, or by a gamma distribution, with fine tuning of the gamma parameter <ref type="bibr">[34]</ref>. The second option consists in simply replacing the HSMM duration model by a better performing one. Several data-driven duration modelling techniques have shown high performance in modelling segmental duration for diphone synthesis, such as artificial neural networks (ANN) <ref type="bibr">[17]</ref>, classification and regression trees (CART) <ref type="bibr">[35]</ref>, support vector regression (SVR) <ref type="bibr">[36]</ref> and multi-adaptive regression splines (MARS) <ref type="bibr">[37]</ref>. Therefore, many machine learning based models were externally trained to provide more accurate state, phone or syllable durations for parametric speech synthesis <ref type="bibr">[38]</ref>.</p></div>
<div><head>State duration modelling using MLP instead of decision trees</head><p>In <ref type="bibr">[38]</ref>, external SVR and Multi-layer perceptron (MLP) models explicitly provide state or phone duration to the parametric speech synthesis system. Another approach consists in replacing the context-based decision tree of duration HMMs by an MLP, in a hybrid . This is done in a two-stage process, where the state durations (in number of frames) are obtained by training monophone HMMs and Viterbi state alignment. In the second stage, the obtained state durations are trained as outputs of the MLP, whose inputs are the contextual, linguistic and phonological features, which used to be fed into the decision tree. Objective evaluation of phone duration prediction has shown a decrease in RMSE (ms) while measured for the overall phonemes, and partial decrease with respect to phoneme identity, mainly for vowels and semi-vowels. Listening tests have shown rather a preference for the hybrid model, though the "no preference" answer has been dominating.</p></div>
<div><head>DNN-based duration modelling for parametric speech synthesis</head><p>Thanks to their proved ability to approximate continuous functions, DNN have become the state-of-the-art tool in regression problems. Particularly, duration modelling for speech synthesis has been achieved using several approaches with DNN.</p><p>DNN-based state duration distribution vs. segmental duration models DNN-based duration modelling for parametric speech synthesis can be used in two ways. In the first case, DNN are used instead of decision trees in the HSMM duration model to map the label features to the pre-trained state duration HMMs; whereas in the second case, raw segmental durations, i.e. state, phone or syllable durations, are directly estimated from the label features using DNN.</p><p>-DNN-based state duration distribution modelling Since the work of [9], DNN have been preferred to decision trees to model state duration HMMs for parametric speech synthesis due to some limitations of decision trees. Actually, decision trees are not well-fitted to model complicated functions like XOR, d-bit parity function or multiplex problems <ref type="bibr">[9]</ref>. Furthermore, decision trees have a poor generalization power, since they process input data by partitioning the input space into regions, each associated with a terminal node. This yields in neglecting some "weak" features [40]; whereas using DNN involves training all the input data while updating weights, which means a better generalization. On the other side, the outputs are the parameters of duration HMM, i.e. the mean value and the variance. Actually, predicting these statistic parameters and then generating durations using the Gaussian distribution ensure average values. It should be also mentioned that in this internal scheme, only state durations can be predicted, like in HSMM-based duration modelling, then the phone duration is obtained as the sum of the durations of its states.</p></div>
<div><head>-DNN-based segmental duration modelling</head><p>In this model, raw segmental duration, i.e. state, phone or syllable duration is directly predicted from the label features using DNN. In the synthesis stage, the predicted durations are included to the parametric speech synthesis system. However, the way phone durations are predicted may depend on the DNN architecture. In some works, like [41], discrete phone durations are directly predicted from label features using DNN, whereas in [42] phone durations were predicted by adding an explicit constraint, which consists in the mean square error between the phone duration and the sum of the phone's state durations (L ec ) (cf. ( <ref type="formula" target="#formula_0">1</ref>)):</p><formula xml:id="formula_0">L ec = N j=1 ( 5 s=1 dj,s -d j ) 2 , (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where N is the number of phonemes, dj,s and d j are respectively the predicted duration of state s within phone j and the target duration of phone j.</p></div>
<div><head>DNN architectures for duration modelling</head><p>Duration modelling may also take advantage from the different architectures of DNN, i.e. feedforward, recurrent or multi-task learning, which have all been already applied for this purpose.</p><p>- In feedforward-DNN, features are processed without recurrence. This means that the sequential aspect of speech, in particular affecting rhythm and speech rate, is neglected. However, this aspect is primordial in speech production. Recurrent neural networks (RNN) are based on forward or backward time propagation of the input features. Amongst RNN variants, longshort-term memory (LSTM) have been particularly praised for addressing the vanishing gradient problem in standard <ref type="bibr">RNN's [44]</ref>. In LSTM, the output at each layer is generated from its input and a memory cell, which is obtained from the previous time memory cell and a set of gates (input, output and forget gates) in addition to the current layer's input itself <ref type="bibr">[43]</ref>. Hence, at each layer, each feature passes through the input, forget and output gates in addition to the memory cell, before the output is obtained. This ensures that the previous phonemes are taken into account while calculating the current phoneme duration.</p></div>
<div><head>-BLSTM neural networks</head><p>Furthermore, forward and backward information propagation in LSTM can be combined. Such variant is called bi-directional LSTM (BLSTM).</p><p>In this scheme, the hidden activation functions are defined as positivedirection or negative-direction, respectively for forward or backward direction. The same for the weight matrices and the recurrent matrices. LSTM and BLSTM were both successfully utilized to model state durations in <ref type="bibr">[45,</ref><ref type="bibr">46]</ref> and raw phone duration in <ref type="bibr">[41,</ref><ref type="bibr">47,</ref><ref type="bibr">48]</ref> for parametric speech synthesis.</p><p>Continuous vs. discrete duration modelling using DNN</p><p>In the case of continuous phone duration modelling, the optimization criterion is the mean square error L msei expressed for each utterance i having n i phonemes (cf. ( <ref type="formula" target="#formula_2">2</ref>)):</p><formula xml:id="formula_2">L msei = 1 n i ni j=1 (d i,j -di,j ) 2 ,<label>(2)</label></formula><p>where d i,j and di,j are the reference and the predicted phone durations respectively of the phoneme sequence</p><formula xml:id="formula_3">(v i,1 , v i,2 , ..., v i,ni ) of the i th utterance [41].</formula><p>Another strategy consists in the estimation of the discrete probability distribution of the phone duration. The interest of this method lies in estimating the probability of the current phone duration given the phoneme sequence history (v i,1 , v i,2 , ..., v i,j ) to be expressed by the cross-entropy criterion L cei for each utterance u i (cf. (3)):</p><formula xml:id="formula_4">L cei = -log(p( di |v i )) = - ni j=1 log(p( di,j |v i,1 , ..., v i,j )),<label>(3)</label></formula><p>where p( di,j |v i,1 , ..., v i,j ) is the duration probability distribution of phoneme v i,j given the sequence v i,1 , ..., v i,j <ref type="bibr">[41]</ref>. Though both criteria, i.e. L mse for continuous duration and L ce for discrete duration probability density are equivalent from the perspective of probability distribution [41], we have relied in this work on continuous duration modelling, since we use continuous values, instead of discrete probability densities of phone duration.</p><p>3 Proposed DNN-based duration modelling for Arabic parametric speech synthesis</p><p>Based on the aforementioned approaches, in particular those applied for DNNbased parametric speech synthesis, and aiming at taking care of Arabic speech requirements, we introduce a set of DNN-based models, specifically designed for Arabic parametric speech synthesis, along with the speech materials and the feature set. </p></div>
<div><head>Workflow of the proposed approach</head><p>The main idea behind the proposed approach consists in designing a dedicated DNN-based duration model for Arabic, and to introduce it into the standard HTS parametric speech synthesis toolkit <ref type="bibr">[53]</ref>. As illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, the novelty consists in (a) adding some novel features to the standard HTS feature set [8], namely vowel quantity and gemination, (b) training a specific DNN duration model for each class of Arabic phonemes, namely short vowels, long vowels, simple consonants and geminated consonants, instead of using one duration model for all phonemes. The ultimate goal, and the real added value of this work consists in exploiting the results of the proposed DNN-based duration model as an external input to the HTS system, instead of the internal HMM-based duration model of HTS, in order to improve the performance of HTS, specifically for Arabic (cf. Figure <ref type="figure" target="#fig_1">2</ref>).</p></div>
<div><head>Introduction of DNN-based duration model into HMM-based parametric speech synthesis system</head><p>Since our aim is to improve the quality of parametric speech synthesis for Arabic, using characteristic features of Arabic, i.e. gemination and vowel quantity, and since it has been proven that the aforementioned DNN models outperform all the state-of-the-art duration prediction techniques, we propose to improve the duration prediction performance of HMM-based parametric speech synthesis for Arabic, by using external DNN models, dedicated for duration prediction, instead of relying on the classical duration HMM's used in HTS system. This is feasible since HTS allows imposing parameters values, whether for excitation, i.e log(F 0 ), spectrum or duration, so that the values provided by the internal HMM models will be replaced by those introduced from external sources. Figure <ref type="figure" target="#fig_1">2</ref> shows the whole HTS system, where only the duration HMM module is replaced by an external DNN. In the following, the steps of the experimental protocol will be detailed. </p></div>
<div><head>Speech material</head><p>In order to train the DNN-based duration models, a set of 1597 utterances was used. The utterances were taken from an Arabic speech corpus <ref type="bibr">[55,</ref><ref type="bibr">52]</ref> </p></div>
<div><head>Features extraction</head><p>The baseline contextual feature set is similar to the one used in Arabic speech synthesis using HTS <ref type="bibr">[14]</ref>, where the features can be classified into three main subsets:</p><p>-Positional features, e.g. position of the current phoneme in the current syllable (forward and backward), number of phonemes in the current syllable, etc. -Linguistic features, e.g. identity of the current phoneme, guess-part-ofspeech (content-word/non content-word), etc. -Phonological features, e.g. lexical stress of the current syllable, etc.</p><p>Besides, two Arabic-language-specific features, i.e. gemination and vowel quantity were proved to enhance the quality of parametric speech synthesis for MSA <ref type="bibr">[25]</ref>. Therefore, both were added to the feature set. The original duration were provided by automatic segmentation, using forced alignment <ref type="bibr">[49]</ref>. In addition, a portion of the corpus was segmented manually by experts, to evaluate the quality of the automatic segmentation, and to verify whether the corpus needs a totally manual segmentation <ref type="bibr">[49]</ref>.</p></div>
<div><head>Benchmarking and proposed duration models</head><p>This work aims to improve the prediction of the phoneme durations for parametric synthesis, using DNN taking into account some characteristic features of Arabic (simple consonants/geminated consonants and short vowels/long vowels). Five duration models were trained and evaluated, all using the same set of contextual features and the same output reference durations: Since learning performance is mainly adjusted empirically, several structures of the proposed models were tried out by varying the number of hidden layers, the number of their nodes, and the activation functions. As the task is to predict continuous values of phone duration, the RMSE minimisation criterion was adopted for training. The RMSprop optimizer was adopted in all the experiments. To avoid over-learning, early stopping was used. Thus, if L mse evaluated on the development set does not improve after a certain number of epochs, set to 20 in our case, the training process is stopped. Table <ref type="table" target="#tab_2">2</ref> shows, for each class of sounds, the DNN structure leading to the best duration prediction results on the development set.</p><p>The class-specific model is defined as the model which gives the best accuracy for the given class of sounds, independently of its performance on the other classes of sounds.</p></div>
<div><head n="4">Evaluation protocol</head><p>A twofold evaluation process was carried out to assess the quality of the duration prediction, through an objective evaluation by measuring the differences between the reference durations and the predicted durations, and a subjective evaluation based on listening tests.</p></div>
<div><head>Objective evaluation</head><p>The objective evaluation process consists in comparing the performance of the class-specific DNN modelling to state-of-the-art models, i.e. HMM model as Then, they are selected as the best models on the development set (c.f. Table <ref type="table" target="#tab_2">2</ref>) because they give the best scores and finally they were kept to be evaluated on the test set. The standard DNN model trained on all phonemes is built using three LSTM layers having 1024, 512 and 512 nodes respectively. The DNN model as used in <software ContextAttributes="used">MERLIN</software> is composed by 6 hidden feedforward layers with 1024 units each and tangent hyperbolic (tanh) as activation function. This model relies on the same set of features as HTS. The MLP model from <ref type="bibr">[24]</ref>, contains two hidden layers with 26 units each, and uses sigmoid and tanh as activation functions. This model does not use the same set of linguistic features as HTS. Evaluation is carried out by focusing on the comparison of the performance of the models on each class of phonemes and on pauses; a global comparison is also reported that takes into account all the sounds.</p><p>Table <ref type="table" target="#tab_3">3</ref> presents the evaluation of the prediction of the phone durations on the test set data, as measured by the following criteria: root mean square error (RMSE), mean absolute error (MAE) and correlation coefficient (corr) between original and predicted duration, as calculated respectively by (4a),  Table <ref type="table" target="#tab_3">3</ref> shows that the proposed class-specific DNN models, lead to the best performance amongst all the evaluated models, i.e. baseline HMM model (from HTS), the DNN model trained on all phonemes (DNN-all-phone), the baseline DNN model from <software ContextAttributes="used">MERLIN</software> and the former MLP model, for almost every measure, RMSE, MAE and correlation coefficient, and almost every class of sounds, i.e. short/long vowels, simple/geminated consonants, and pauses. Table <ref type="table" target="#tab_5">4</ref> presents the accuracy values computed on all the phonemes, as well as the accuracy computed on all sounds, i.e., on all the phonemes plus pauses; it shows that using class specific models improves the prediction duration accuracy, in comparison to state-of-the-art models. Figure <ref type="figure" target="#fig_5">3</ref> displays the original and predicted duration distributions of phone durations for each class of sounds on the test set. A good match is observed for simple consonants, for short vowels and for long vowels. However, for the geminated consonants, the predicted duration distribution is sharper, showing a lower standard deviation with higher mean values, which means that the predicted duration values are more concentrated around the average duration than are the reference duration values. Table <ref type="table" target="#tab_2">2</ref> shows that for each phoneme class, a class specific DNN modeling enhances the prediction accuracy of the phoneme duration, on the test set data, as measured by the various criteria: root mean square error (RMSE), mean absolute error (MAE) and correlation coefficient between original and predicted duration. Results show that for each class of sounds, the novel class-specific DNN modeling performs better than the HMM modeling from HTS, the DNN modeling from <software ContextAttributes="used">MERLIN</software>, and the former ANN model: the root mean square error and the mean absolute error are lower, and the correlation between predicted and reference duration values is higher. When the accuracy are computed globally, i.e., on all the Figure <ref type="figure" target="#fig_0">1</ref> shows the distributions of phoneme durations for the original and predicted values on the test set, for each class of phonemes. Results show a good match for simple consonants, short vowels and long vowels. For the geminated consonants, the distribution of the predicted values is sharper, i.e. has a lower standard deviation, and slightly shifted towards higher durations. This means that the model slightly over-estimates the durations of long vowels.</p><formula xml:id="formula_5">RM SE = 1 N N j=1 (d j -dj ) 2 ,<label>(4a)</label></formula><formula xml:id="formula_6">M AE = 1 N N j=1 |d j -dj |,<label>(4b) corr</label></formula><formula xml:id="formula_7">= cov(d, d) σ d σ d ,<label>(4c)</label></formula></div>
<div><head n="4.">Discussion and conclusions</head><p>This paper has investigated the modeling of the duration of the Arabic sounds for text-to-speech synthesis. Various DNNbased architectures have been developed and evaluated. Each model have been trained on various subsets of the training data corresponding to classes of sounds, as for example training on all phoneme segments, training on vowel segments only, training on short vowel segments only, etc. The various modeling architectures trained on various subsets of sounds have been compared on the development data. It appears that it is not the same model and training data, which leads to the best prediction accuracy on the various classes of phonemes (short vowels, long vowels, simple consonants and geminated consonants). This led us to define a class-specific modeling approach, which for each sound, uses the model that performs the best on the validation set. This class-specific modeling approach has been compared on the Arabic test set, to several state of the art modeling approaches, as the HMM-based modeling from the HTS toolkit and the DNN-based modeling from the <software>MERLIN</software> toolkit. Objective evaluations show that the </p></div>
<div><head>Subjective evaluation</head><p>In addition to the objective evaluation, listening tests were conducted to assess on one side the global quality of the uttered sounds, and on the other side the perception of phone durations. In both tests, 22 Arabic-native-speaking listeners, who are not speech specialists, were asked to rate the speech stimuli -Natural, this is the reconstructed signal obtained after processing the original signal with the <software>STRAIGHT</software> vocoder (hence, corresponds to original durations). -Class-specific-DNN, which is the signal synthesized after predicting the sound durations with the class-specific models, as proposed above.</p><p>-DNN-all-phone, which is the signal reconstructed using the durations predicted by the best DNN model trained over all the phonemes. -HMM, which corresponds to the signal generated by HTS using the HMMbased duration model from HTS. This model was used as a baseline above.</p><p>It should be emphasized that the acoustic parameters (F0 and spectral parameters (MGC)) are generated from HMM models in HTS. The duration is generated externally from DNN duration models in the case of DNN-all-phone and Class-specific-DNN and from original duration in the case of Natural. In addition, it should be also mentioned, that all signals were generated with the HTS toolkit version 2.3 using the <software ContextAttributes="used">STRAIGHT</software> vocoder. Also, it is worth noting that the subjective evaluation has been conducted following standard methodologies used for speech quality assessment, as detailed in <ref type="bibr">[57,</ref><ref type="bibr">58]</ref>.</p></div>
<div><head>Assessment of global quality</head><p>To assess this aspect, the 22 native Arabic listeners were asked to rate the overall quality of each stimuli through a MOS evaluation, and then through a preference test.</p></div>
<div><head>MOS Test</head><p>Figure <ref type="figure" target="#fig_6">4</ref> shows that the class-specific-DNN duration model reaches the same MOS value as the natural duration reference. It is also interesting to note that the DNN-all-phone is ranked last, which means that to outperform HMM in duration modelling, the DNN should be used class-wise. For a further analysis, the MOS values were finely investigated (cf. Figure <ref type="figure">5</ref>), to show that this outstanding result for the class-specific-DNN model is due to the fact that it received a high MOS score (i.e. MOS ≥ 4) in more than 40% of cases. However, it is interesting to observe that HMM was the model which received the most of "Very good" scores (though the difference with Class-specific-DNN is slight). </p></div>
<div><head>0%</head></div>
<div><head>Assessment of duration perception</head><p>The second subjective test was conducted to assess the perception of phone duration. The 22 listeners were asked to rate the overall quality of the duration of the uttered phones through a MOS evaluation, and then to choose which stimuli is best with respect to phone duration in a preference test.</p></div>
<div><head>MOS test</head><p>In this test, the listeners were asked to evaluate how they appreciate the duration of the pronunciation of the phones. The listeners had to choose one answer among five ranging from "phone durations are not respected" up to "phone durations are all respected". The MOS results are shown in Figure <ref type="figure" target="#fig_7">7</ref>.</p><p>It looks that HMM duration model was slightly preferred to the class-specific-DNN model. However, the difference is not significant when compared to the 95% confidence interval; and it is very relevant to note that both have a MOS result close to the one of the natural duration reference. In addition, Figure <ref type="figure">8</ref> shows a finer analysis of the given MOS scores. It is interesting to note that the class-specific-DNN model received as many high scores (MOS ≥ 4) as the HMM model, or the natural durations. Finally, the DNN-all-phone model was again less appreciated than all other models.</p><p>Preference test The participants were also asked to listen to pairs of stimuli, where for each pair, the same utterance is pronounced using a different duration model. They were asked to answer how they judge the duration of the pronunciation of the phones of the second stimuli in comparison to the first one. Figure <ref type="figure">9</ref> shows the results of the preference test. It appears that the HMM model and the class-specific-DNN model, are the most preferred, which matches with the overall quality assessment for duration pronunciation (cf. Figure <ref type="figure">6</ref>). </p></div>
<div><head>Interpretation and discussion</head><p>The results reported in the objective and the subjective evaluation can be analyzed further, from quantitative and qualitative viewpoints. </p></div>
<div><head>Quantitative analysis</head><p>The objective evaluation (cf. Table <ref type="table" target="#tab_4">3</ref> and<ref type="table" target="#tab_5">Table 4</ref>) shows that the proposed class-specific DNN model yields the least error measure, i.e. RMSE and MAE, and the highest correlation to the original duration values. This can be interpreted as a proof that each class of phonemes, i.e. short/long vowels, simple/geminated consonants has its own margin of duration. This is to recall a previous assumption, made by [59] who had stipulated that each phoneme has its own intrinsic duration, that can be stretched by an elasticity coefficient, whose value depends on the context. However, such an assumption has not been proved so far. Besides, a deeper cross-language investigation is necessary to reveal other contributory features affecting phone duration in other languages, so that the standard HTS features could be personalized for each target language.</p></div>
<div><head>Qualitative analysis</head><p>The analysis of MOS and preference test results (cf. Figure <ref type="figure" target="#fig_6">4</ref> to Figure <ref type="figure">9</ref>) shows that speech reconstructed with the proposed specific-class DNN duration model is perceived as equal as that reconstructed with natural duration. Beyond this positive feedback, that was made by native Arabic speakers, a further analysis by listeners specialized in linguistics and phonetics should be carried out to judge the quality of each single type of phonemes. Actually, gemination is still a controverted phenomenon in Arabic phonetics, since phonetically, it is in the frontiers between a double consonant and a standalone unit. Also, vowel quantity is still a relative notion, since in Arabic, some long vowels might be pronounced as short ones, especially at the end of the word. Another point, which is not less important, consists in the necessity to study the effect such a phoneme-type-related modelling has on the other prosodic parameters, such as F0 and spectrum, in correlation with the quality of parametric speech synthesis.</p></div>
<div><head n="5">Conclusion</head><p>In this paper, a comprehensive investigation of duration modelling for Arabic sounds for TTS is described. A variety of DNN-based structures have been developed and evaluated. Taking into account some characteristics of Arabic, i.e. vowel quantity (short vs. long vowels) and gemination, the proposed DNN models were trained on different sets of phonemes, namely DNN-all-phone model that was trained on all the phonemes, whereas class-specific-DNN models were trained separately on each phoneme class, i.e. short/long vowels, simple/geminated consonants and pauses. The validation phase allowed selecting for each phoneme class the model that performed the best on the development set for evaluation on the test set. The main finding of the objective evaluation consists in observing that for each class of phonemes, the class-specific-DNN model performs better than the generic DNN model trained on all phonemes, i.e. DNN-all-phone model. This novel class-specific modelling approach was also compared to state-of-the-art models, i.e. baseline HMM from HTS toolkit and DNN from <software>MERLIN</software> toolkit. Again, the best accuracy values were obtained by the proposed class-specific DNN models, which results outperformed those of the baseline DNN from <software ContextAttributes="used">MERLIN</software> in all cases, and were better or equal to those of HTS.</p><p>The subjective evaluation consisted in the assessment of the global quality of the generated sound and in the appreciation of the predicted sound durations. Both tests included a MOS and a preference rating. For performing these listening tests, the predicted duration obtained using the class-specific DNN model and the generic DNN model applied for all phonemes (DNN-all-phone) were processed with the HTS toolkit to generate the synthetic speech signals.</p><p>In terms of global quality, the preference of listeners to the class-specific-DNN sounds confirms the objective evaluation results. Actually, class-specific-DNN was rated as high as natural, i.e. the original-duration-reconstructed sounds. This proves that the class-specific approach has succeeded to finely model the phone durations by fitting each DNN model to the corresponding class of phonemes, instead of the classical approach, i.e. applying one single DNN model for all phonemes. This proves also the benefit of handling specifically the various classes of Arabic sounds, through the addition of gemination and vowel quantity as special features for Arabic to the standard feature set of HTS, as done in previous studies, and / or through dedicated models as done in this current study for Arabic phone duration prediction.</p></div><figure xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Workflow of the proposed class-specific duration modelling approach: Each class of Arabic phonemes is processed independently to train its own DNN-based duration model.</figDesc><graphic coords="12,72.97,89.45,334.80,97.54" type="bitmap" /></figure>
<figure xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Overview of the modified HMM-based parametric speech synthesis system with external duration DNN modelling</figDesc></figure>
<figure xml:id="fig_2"><head /><label /><figDesc>1. HMM baseline model (from HTS toolkit [53]), 2. DNN model (from MERLIN toolkit [54]), 3. Feedforward-DNN model, 4. DNN-LSTM model, 5. DNN-BLSTM model. Besides, feedforward-DNN, DNN-LSTM and DNN-BLSTM models were trained with various training subsets corresponding to either all the phonemes present in the training set, or only short vowels for the short vowel dedicated model, only long vowels for the long vowels dedicated model, only simple consonants for the simple consonant dedicated model, only geminated consonants for the geminated consonant dedicated model, and only pauses for the pause dedicated model.</figDesc></figure>
<figure xml:id="fig_3"><head /><label /><figDesc>where cov(d, d)  is the covariance of duration vectors d and d; and σ d , σ d are the standard deviations of d and d respectively.</figDesc></figure>
<figure xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of the distributions of the phoneme durations between original and predicted values.</figDesc><graphic coords="18,47.22,275.63,402.05,231.75" type="bitmap" /></figure>
<figure xml:id="fig_5"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Comparison of the probability distribution of reference (continuous line)and predicted (dotted line) duration for each class of phonemes</figDesc></figure>
<figure xml:id="fig_6"><head>Fig. 4</head><label>4</label><figDesc>Fig. 4 MOS score results for global quality of generated speech</figDesc></figure>
<figure xml:id="fig_7"><head>Fig. 7</head><label>7</label><figDesc>Fig. 7 MOS scores for duration perception</figDesc></figure>
<figure type="table" xml:id="tab_1"><head /><label /><figDesc>In addition, [51]  suggests that using an adequate normalizing transformation of input data may dramatically reduce the estimation errors and the calculating time during the training phase. Therefore the numeric contextual features were normalized in the interval [0, 1] by dividing their values on the maximum value in the training set. Since the main contribution of this work concerns the modelling of phone duration by phoneme class, the reference durations have been statistically analysed by phoneme class. Table1reports, for each class of sounds, the number of phone occurrences, the mean duration, and its associated standard deviation value.</figDesc><table><row><cell cols="5">Table 1 Statistics of phone durations by phoneme class for the training, validation and</cell></row><row><cell>test sets</cell><cell /><cell /><cell /><cell /></row><row><cell>Dataset</cell><cell>Class</cell><cell cols="2">Number of Mean</cell><cell>Standard</cell></row><row><cell /><cell /><cell>occurences</cell><cell cols="2">value (ms) deviation (ms)</cell></row><row><cell>Training</cell><cell>Simple consonant</cell><cell>37872</cell><cell>91</cell><cell>37</cell></row><row><cell /><cell>Geminated consonant</cell><cell>4040</cell><cell>180</cell><cell>43</cell></row><row><cell /><cell>Short vowel</cell><cell>23670</cell><cell>71</cell><cell>36</cell></row><row><cell /><cell>Long vowel</cell><cell>11565</cell><cell>120</cell><cell>49</cell></row><row><cell /><cell>Pauses</cell><cell>2458</cell><cell>340</cell><cell>156</cell></row><row><cell cols="2">Validation Simple consonant</cell><cell>5850</cell><cell>95</cell><cell>43</cell></row><row><cell /><cell>Geminated consonant</cell><cell>831</cell><cell>202</cell><cell>38</cell></row><row><cell /><cell>Short vowel</cell><cell>3825</cell><cell>85</cell><cell>44</cell></row><row><cell /><cell>Long vowel</cell><cell>1929</cell><cell>116</cell><cell>59</cell></row><row><cell /><cell>Pauses</cell><cell>1132</cell><cell>445</cell><cell>211</cell></row><row><cell>Test</cell><cell>Simple consonant</cell><cell>3658</cell><cell>93</cell><cell>38</cell></row><row><cell /><cell>Geminated consonant</cell><cell>450</cell><cell>202</cell><cell>46</cell></row><row><cell /><cell>Short vowel</cell><cell>2466</cell><cell>82</cell><cell>38</cell></row><row><cell /><cell>Long vowel</cell><cell>1004</cell><cell>138</cell><cell>61</cell></row><row><cell /><cell>Pauses</cell><cell>548</cell><cell>454</cell><cell>126</cell></row></table><note><p><p><p>, representing news bulletin in MSA (Modern Standard Arabic), read by a native-Arabic male speaker. The signals were recorded at 48 KHz sampling rate, with 16-bit precision. The corpus was divided into three subsets, approximately 70% of utterances for training, 20% for development and 10% for test. The label features and output target duration have undertaken the required preprocessing, i.e. label features coding and output durations normalization. Categorical label features were coded in binary, e.g. like stressed/not stressed syllables, or in discrete values, e.g. phoneme identity, whereas unlimited-value features like phoneme's position in the syllable were coded numerically. The label features were coded into a 445-coefficient vector. On the other side, output duration distribution was analysed and a log-transform was applied to normalize it. In fact, duration distribution normalization was suggested in</p>[50]  </p>to increase the prediction ability of neural networks.</p></note></figure>
<figure type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Description of the model structure leading to the best accuracy on the development set for each phoneme class and for pauses</figDesc><table><row><cell>Class</cell><cell>Model</cell><cell>Training</cell><cell>Model description</cell><cell># of</cell></row><row><cell>of sounds</cell><cell /><cell>set</cell><cell /><cell>epochs</cell></row><row><cell>simple</cell><cell>DNN-</cell><cell>simple</cell><cell>2 feedforward layers</cell><cell>94</cell></row><row><cell cols="2">consonants BLSTM(1)</cell><cell cols="2">consonants with 512 units each and</cell><cell /></row><row><cell /><cell /><cell /><cell>activation function tanh,</cell><cell /></row><row><cell /><cell /><cell /><cell>plus 2 BLSTM layers</cell><cell /></row><row><cell /><cell /><cell /><cell>with 128 units each</cell><cell /></row><row><cell>geminated</cell><cell>DNN-</cell><cell>geminated</cell><cell>2 feedforward layers</cell><cell>74</cell></row><row><cell cols="2">consonants BLSTM(2)</cell><cell cols="2">consonants with 16 units each and</cell><cell /></row><row><cell /><cell /><cell /><cell>activation function tanh,</cell><cell /></row><row><cell /><cell /><cell /><cell>plus 2 BLSTM layers</cell><cell /></row><row><cell /><cell /><cell /><cell>with 16 units each</cell><cell /></row><row><cell>short</cell><cell>DNN-</cell><cell>short</cell><cell>2 feedforward layers</cell><cell>89</cell></row><row><cell>vowels</cell><cell>BLSTM(1)</cell><cell>vowels</cell><cell>with 512 units each and</cell><cell /></row><row><cell /><cell /><cell /><cell>activation function tanh,</cell><cell /></row><row><cell /><cell /><cell /><cell>plus 2 BLSTM layers</cell><cell /></row><row><cell /><cell /><cell /><cell>with 128 units each</cell><cell /></row><row><cell>long</cell><cell cols="2">Feedforward-long</cell><cell>2 feedforward layers with</cell><cell>97</cell></row><row><cell>vowels</cell><cell>DNN</cell><cell>vowels</cell><cell>512 and 256 units resp. and</cell><cell /></row><row><cell /><cell /><cell /><cell>activation function tanh</cell><cell /></row><row><cell>pauses</cell><cell>DNN-</cell><cell>all the</cell><cell>3 LSTM layers with</cell><cell>95</cell></row><row><cell /><cell>LSTM</cell><cell>phonemes</cell><cell>1024, 512 and 512 units resp.</cell><cell /></row></table><note><p><p><p>used in HTS, a standard DNN model trained on all phonemes, DNN model as used in MERLIN, and the former MLP model developed for Arabic</p>[24]</p>. To select the best class-specific DNN models, many architectures were tried out.<p />. To select the best class-specific DNN models, many architectures were tried out.</p></note></figure>
<figure type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Comparison of RMSE, MAE and correlation between predicted duration and reference duration on the test set for each phoneme class and for the various modelling approaches</figDesc><table><row><cell>Class</cell><cell>Duration</cell><cell>RMSE</cell><cell>MAE</cell><cell>Corr</cell></row><row><cell>of sound</cell><cell>model</cell><cell>(ms)</cell><cell>(ms)</cell><cell /></row><row><cell>simple</cell><cell>HMM from HTS</cell><cell>25</cell><cell>18</cell><cell>0.76</cell></row><row><cell>consonants</cell><cell>Class-specific DNN</cell><cell>25</cell><cell>17</cell><cell>0.77</cell></row><row><cell /><cell>DNN-all-phone</cell><cell>28</cell><cell>20</cell><cell>0.72</cell></row><row><cell /><cell>DNN-MERLIN</cell><cell>26</cell><cell>18</cell><cell>0.75</cell></row><row><cell /><cell>MLP from [24]</cell><cell>35</cell><cell>25</cell><cell>0.50</cell></row><row><cell>geminated</cell><cell>HMM from HTS</cell><cell>43</cell><cell>31</cell><cell>0.43</cell></row><row><cell>consonants</cell><cell>Class-specific DNN</cell><cell>42</cell><cell>32</cell><cell>0.51</cell></row><row><cell /><cell>DNN-all-phone</cell><cell>48</cell><cell>37</cell><cell>0.34</cell></row><row><cell /><cell>DNN from MERLIN</cell><cell>54</cell><cell>40</cell><cell>0.15</cell></row><row><cell /><cell>MLP from [24]</cell><cell>62</cell><cell>50</cell><cell>0.42</cell></row><row><cell>short</cell><cell>HMM from HTS</cell><cell>22</cell><cell>16</cell><cell>0.82</cell></row><row><cell>vowels</cell><cell>Class-specific DNN</cell><cell>22</cell><cell>16</cell><cell>0.84</cell></row><row><cell /><cell>DNN-all-phone</cell><cell>23</cell><cell>17</cell><cell>0.82</cell></row><row><cell /><cell>DNN from MERLIN</cell><cell>26</cell><cell>19</cell><cell>0.81</cell></row><row><cell /><cell>MLP from [24]</cell><cell>26</cell><cell>19</cell><cell>0.78</cell></row><row><cell>long</cell><cell>HMM from HTS</cell><cell>49</cell><cell>34</cell><cell>0.68</cell></row><row><cell>vowels</cell><cell>Class-specific DNN</cell><cell>40</cell><cell>28</cell><cell>0.77</cell></row><row><cell /><cell>DNN-all-phone</cell><cell>48</cell><cell>35</cell><cell>0.65</cell></row><row><cell /><cell>DNN from MERLIN</cell><cell>54</cell><cell>38</cell><cell>0.66</cell></row><row><cell /><cell>MLP from [24]</cell><cell>68</cell><cell>52</cell><cell>0.07</cell></row><row><cell>pauses</cell><cell>HMM from HTS</cell><cell>109</cell><cell>73</cell><cell>0.54</cell></row><row><cell /><cell>Class-specific DNN</cell><cell>109</cell><cell>70</cell><cell>0.54</cell></row><row><cell /><cell>DNN-all-phone</cell><cell>109</cell><cell>70</cell><cell>0.54</cell></row><row><cell /><cell>DNN from MERLIN</cell><cell>146</cell><cell>110</cell><cell>0.60</cell></row><row><cell /><cell>MLP from [24]</cell><cell>188</cell><cell>158</cell><cell>0.56</cell></row><row><cell>(4b) and (4c):</cell><cell /><cell /><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of RMSE, MAE and correlation between predicted durations and reference durationson test set, for the various modeling approaches.</figDesc><table><row><cell>Phoneme</cell><cell>Duration</cell><cell>RMSE</cell><cell>MAE</cell><cell>Corr</cell></row><row><cell>Class</cell><cell>modeling</cell><cell>(ms)</cell><cell>(ms)</cell><cell /></row><row><cell /><cell>HMM from HTS</cell><cell>30</cell><cell>20</cell><cell>0.83</cell></row><row><cell>All</cell><cell>Class-specific DNN</cell><cell>28</cell><cell>19</cell><cell>0.85</cell></row><row><cell>phonemes</cell><cell>DNN from MERLIN</cell><cell>33</cell><cell>22</cell><cell>0.80</cell></row><row><cell /><cell>ANN from [17]</cell><cell>41</cell><cell>28</cell><cell>0.66</cell></row><row><cell>All</cell><cell>HMM from HTS</cell><cell>40</cell><cell>24</cell><cell>0.93</cell></row><row><cell>phonemes</cell><cell>Class-specific DNN</cell><cell>39</cell><cell>22</cell><cell>0.93</cell></row><row><cell>+pauses</cell><cell>DNN from MERLIN</cell><cell>50</cell><cell>28</cell><cell>0.92</cell></row><row><cell /><cell>ANN from [17]</cell><cell>63</cell><cell>37</cell><cell>0.87</cell></row></table></figure>
<figure type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Comparison of RMSE, MAE and correlation between predicted duration and reference duration on the test set for the various modelling approaches</figDesc><table><row><cell /><cell>Duration</cell><cell>RMSE</cell><cell>MAE</cell><cell>Corr</cell></row><row><cell /><cell>model</cell><cell>(ms)</cell><cell>(ms)</cell><cell /></row><row><cell>All</cell><cell>HMM from HTS</cell><cell>30</cell><cell>20</cell><cell>0.83</cell></row><row><cell>phones</cell><cell>Class-specific DNN</cell><cell>28</cell><cell>19</cell><cell>0.85</cell></row><row><cell /><cell>DNN-all-phone</cell><cell>32</cell><cell>22</cell><cell>0.80</cell></row><row><cell /><cell>DNN-MERLIN</cell><cell>33</cell><cell>22</cell><cell>0.80</cell></row><row><cell /><cell>MLP from [24]</cell><cell>41</cell><cell>28</cell><cell>0.66</cell></row><row><cell>All</cell><cell>HMM from HTS</cell><cell>40</cell><cell>24</cell><cell>0.93</cell></row><row><cell>phones</cell><cell>Class-specific DNN</cell><cell>39</cell><cell>22</cell><cell>0.93</cell></row><row><cell>+pauses</cell><cell>DNN-all-phone</cell><cell>42</cell><cell>25</cell><cell>0.92</cell></row><row><cell /><cell>DNN from MERLIN</cell><cell>50</cell><cell>28</cell><cell>0.92</cell></row><row><cell /><cell>MLP from [24]</cell><cell>63</cell><cell>37</cell><cell>0.87</cell></row><row><cell cols="5">using a score ranging from 1 (Very poor) to 5 (Very good), i.e. a Mean Opinion</cell></row><row><cell cols="5">Score (MOS) test, and to compare pairs of stimuli, i.e. a preference test. Each</cell></row><row><cell cols="5">participant has listened to 24 stimuli, 6 from each phone duration model,</cell></row><row><cell cols="5">randomly selected amongst 96 stimuli (24 from each phone duration model).</cell></row><row><cell cols="5">The subjective evaluation concerns the best duration model according to the</cell></row><row><cell cols="5">objective evaluation above, plus two other duration modelling approaches and</cell></row><row><cell cols="2">the reference duration signal:</cell><cell /><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_6"><head /><label /><figDesc>Analysis of MOS scores for global quality of generated speechPreference Test The MOS test results are confirmed by the preference test (cf. Figure6), where listeners were asked to choose the sound having the best quality, for each pair of stimuli synthesized with different duration models. The main result is the clear preference of Class-specific-DNN above DNN-allphone, and above HMM. Preference test results for global quality of generated speech</figDesc><table><row><cell>Class-specific-DNN</cell><cell>DNN-all-phone</cell></row><row><cell>HMM</cell><cell>DNN-all-phone</cell></row><row><cell cols="2">20% Poor 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 40% 60% 80% Natural HMM Class-specific-DNN DNN-all-phone Very poor Fair Good Very good HMM First No preference Second Fig. 5 Class-specific-DNN 100% Fig. 6</cell></row></table></figure>
<figure type="table" xml:id="tab_7"><head /><label /><figDesc>Analysis of the MOS scores for duration perception</figDesc><table><row><cell>DNN-all-phone</cell><cell /><cell /><cell /><cell /></row><row><cell>Class-specific-DNN</cell><cell /><cell /><cell /><cell /></row><row><cell>HMM</cell><cell /><cell /><cell /><cell /></row><row><cell>Natural</cell><cell /><cell /><cell /><cell /></row><row><cell>0%</cell><cell cols="5">10% 20% 30% 40% 50% 60% 70% 80% 90% 100%</cell></row><row><cell /><cell>Very poor</cell><cell>Poor</cell><cell>Fair</cell><cell>Good</cell><cell>Very good</cell></row><row><cell>Fig. 8</cell><cell /><cell /><cell /><cell /></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgments</head><p>This research work was conducted in the framework of PHC-Utique Program, financed by CMCU (Comité mixte de coopération universitaire), grant No.15G1405.</p></div>
			</div>			<div type="references">

				<listBibl />
			</div>
		</back>
	</text>
</TEI>