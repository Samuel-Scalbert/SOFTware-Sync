<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TwistSLAM: Constrained SLAM in Dynamic Environment</title>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mathieu</forename><surname>Gonzalez</surname></persName>
							<email>mathieu.gonzalez@b-com.com</email>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Marchand</surname></persName>
							<email>eric.marchand@irisa.fr</email>
							<affiliation key="aff1">
								<orgName type="department">Eric Marchand is</orgName>
								<orgName type="laboratory">IRISA</orgName>
								<orgName type="institution" key="instit1">with Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amine</forename><surname>Kacete</surname></persName>
							<email>amine.kacete@b-com.com</email>
						</author>
						<author>
							<persName><forename type="first">Jerome</forename><surname>Royan</surname></persName>
							<email>jerome.royan@b-com.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute of Research and Technology b&lt;&gt;com</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TwistSLAM: Constrained SLAM in Dynamic Environment</title>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">E494D97A9E5C878B545727B52A232543</idno>
					<note type="submission">Manuscript received: February, 24, 2022; Revised April, 28, 2022; Accepted May, 19, 2022.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-07T09:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>SLAM</term>
					<term>Localization</term>
					<term>Mapping</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>Classical visual simultaneous localization and mapping (SLAM) algorithms usually assume the environment to be rigid. This assumption limits the applicability of those algorithms as they are unable to accurately estimate the camera poses and world structure in real life scenes containing moving objects (e.g. cars, bikes, pedestrians, etc.). To tackle this issue, we propose TwistSLAM: a semantic, dynamic and stereo SLAM system that can track dynamic objects in the environment. Our algorithm creates clusters of points according to their semantic class. Thanks to the definition of inter-cluster constraints modeled by mechanical joints (function of the semantic class), a novel constrained bundle adjustment is then able to jointly estimate both poses and velocities of moving objects along with the classical world structure and camera trajectory. We evaluate our approach on several sequences from the public KITTI dataset and demonstrate quantitatively that it improves camera and object tracking compared to state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head>I. INTRODUCTION</head><p>V ISUAL Simultaneous Localization And Mapping (SLAM)   is an important problem for robotics that has been heavily studied in the past decade. Its goal is to estimate the pose of a camera moving in a scene while building a map of the environment. Some algorithms such as <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> can efficiently solve this problem, however they rely on the static scene assumption. This hypothesis assumes that the world is a single rigid body and thus that no object can move within it. This assumption, which is rarely met in most real world scenes, as they can contain moving objects (e.g. cars, bikes, pedestrians for urban scenes of the KITTI dataset), limits the scenarios in which a SLAM algorithm can be used. Classical SLAM systems such as <ref type="bibr" target="#b0">[1]</ref> try to alleviate this assumption using robust estimators, allowing them to flag moving parts as outliers. However as soon as the number of moving points is too important, the estimated camera pose accuracy decreases. This makes this approach unsuitable for some scenes (e.g. crowded or urban scenes). Some systems <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> have been proposed to detect and mask out dynamic objects in images, thus making the static scene assumption valid. However some recent approaches Fig. <ref type="figure">1</ref>. Our approach allows us to track objects in the scene such as cars. Here we can see: (a) the frame with semantic points and tracked clusters (orange cars) with their estimated speed. (b) a map top view with tracked clusters (orange cars), clusters trajectories (black spheres and lines), clusters twists (blue and purple lines), road points and plane (in green) and camera trajectory (green and blue frustums). (c) Map side view. The rotation part of the twists (blue lines) is perpendicular to the road plane, the translation part (purple lines) is parallel to the plane. <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> argue that moving objects represent valuable information that can be necessary for some applications. Most recent approaches trying to solve both SLAM and object tracking have used semantics as an additional source of information. Semantic knowledge can indeed be beneficial to SLAM <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> as it contains information about the class dynamicity <ref type="bibr" target="#b2">[3]</ref> which is higher level information than simple 3D points.</p><p>In this paper we present a stereo SLAM system called TwistSLAM, as we estimate objects' twists to track them and consider that objects are linked to each other through mechanical joints, similarly to joints linking different parts of a robot. An illustration of our algorithm is visible in figure <ref type="figure">1</ref>: the camera pose is estimated simultaneously with all moving objects in the scene and the map structure (here the plane of the road) constrains the movement of objects. Our approach is based on ORB-SLAM2 <ref type="bibr" target="#b0">[1]</ref> and S 3 LAM <ref type="bibr" target="#b10">[11]</ref>. In our work we use semantic information to build a map of clusters corresponding to objects in the scene. The clustering of the scene allows us to estimate the pose of the camera using static clusters only such as road or house. The other clusters that can be dynamic are tracked and their pose is updated in the map through the estimation of twists that represent their velocity. Most SLAM systems that can track dynamic objects directly estimate their pose through the minimization of a reprojection error function <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref> or with 3D points registration <ref type="bibr" target="#b6">[7]</ref>. Doing so, the estimated pose of an object has 6 degrees of freedom. This does not correspond to reality. For example a car has only 3 degrees of freedom, 2 translations in the road plane and 1 rotation around its normal, hence its pose should be constrained. Our goal is thus to remove degrees of freedom corresponding to physically unfeasible movements. To do so we choose to represent those constraints as mechanical joints which makes our approach highly generic. A mechanical joint between clusters constrains the estimated twist of a dynamic cluster by blocking some of its degrees of freedom thus reducing the effect of noise on the estimation. Once an object twist has been estimated it can be used to update the object pose which enables object tracking. The object poses can then be tightly refined with camera poses and 3D points within a bundle adjustment that also applies mechanical joints constraints.</p><p>Our contributions presented in this paper are:</p><p>• A semantic SLAM system that can robustly estimate the pose of a camera in static as well as dynamic scenes. • A stereo SLAM framework that can track multiple moving objects in the scene. • A new formulation for both the tracking and bundle adjustment that takes into account the characteristics of mechanical joints between objects in the scene to constrain their movements. We also evaluate our approach on several sequences from the public KITTI dataset and compare our approach quantitatively with respect to ORB-SLAM2 <ref type="bibr" target="#b0">[1]</ref>, <software ContextAttributes="used">DynaSLAM</software> <ref type="bibr" target="#b2">[3]</ref>, VDO-SLAM <ref type="bibr" target="#b5">[6]</ref> and <software ContextAttributes="used">DynaSLAM</software> II <ref type="bibr" target="#b4">[5]</ref>, which demonstrates the benefits of our method in terms of object poses and velocities and camera pose estimation accuracy. The rest of the paper is described as follows. First we describe related work on dynamic classical and semantic SLAM. Then we give an overview of mathematical concepts that will be used in this paper. Following, we describe our approach to build a semantic map of clusters, estimate the pose of a camera in a dynamic scene, track moving clusters within the scene while using joint constraints to improve object tracking and refine all estimations with a bundle adjustment. Finally we demonstrate the benefits of our approach on multiple sequences from a public dataset.</p></div>
<div><head>II. RELATED WORK: DYNAMIC SLAM</head><p>In this section we first present some classical <ref type="bibr" target="#b0">[1]</ref> and dynamic SLAM systems <ref type="bibr" target="#b11">[12]</ref> that remove dynamic outliers. Then we focus on semantic SLAM systems that tackle the problem of dynamic objects by masking out <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> or tracking <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref> objects in the scene. Our approach belongs to this last class of systems. For a more in-depth survey of SLAM we refer the reader to <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>ORB-SLAM2 <ref type="bibr" target="#b0">[1]</ref>, which follows the work of <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> uses three parallel threads to estimate the pose of a camera through relocalization or tracking, build a map, refine both using a bundle adjustment <ref type="bibr" target="#b16">[17]</ref> and close looping trajectories. ORB-SLAM2 manages to mitigate the influence of dynamic objects through the use of a RANSAC scheme and a robust cost function based estimation. To tackle this problem some methods propose to roughly estimate the pose of the camera and robustly find outliers in the scene or in the image. Outliers are then removed or downweighted and the camera pose is refined. For example <ref type="bibr" target="#b11">[12]</ref> proposes a direct approach based on the alignment of depth edges using an ICP scheme. For each point they robustly estimate a staticity confidence score which downweights dynamic objects and an intensity assisted ICP robsulty refines the pose using those weights. However those approaches fail when the amount of dynamic parts in the image is too high and are not able to track dynamic objects. DS-SLAM <ref type="bibr" target="#b3">[4]</ref> applies a geometrical moving consistency check on segmented areas. This score allows them to know which areas correspond to moving objects, which are then discarded for robust camera pose estimation and mapping. <software ContextAttributes="used">DynaSLAM</software> <ref type="bibr" target="#b2">[3]</ref> uses semantic information to segment a priori moving objects, which are not used for tracking and mapping. The segmentation is refined using depth information. This approach improves camera localization in dynamic scenes but deteriorates it when a priori moving objects are in reality static such as parked cars. Those approaches, similarly to the previous ones, are not able to track dynamic objects. MaskFusion <ref type="bibr" target="#b6">[7]</ref> is one of the first semantic dynamic SLAM that can track objects. Inspired from <ref type="bibr" target="#b17">[18]</ref> it makes use of 2D instance segmentation to detect objects in the scene and tracks them using both photometric and geometric information from an RGB-D camera. <software ContextAttributes="used">DynaSLAM</software> II <ref type="bibr" target="#b4">[5]</ref> uses semantic information to detect objects. Object 3D points are represented in the object reference frame and used to estimate the object pose at all time by minimizing their reprojection error. ClusterVO <ref type="bibr" target="#b4">[5]</ref>.<ref type="bibr" target="#b7">[8]</ref> is similar to <ref type="bibr" target="#b4">[5]</ref>, but they consider object detection (i.e. 2D bounding boxes) as input which is much faster to infer than dense masks. They also apply a cleaning procedure to improve dynamic keypoints matching and make sure that 2D points do not come from the background of the bounding box. VDO-SLAM <ref type="bibr" target="#b5">[6]</ref> proposes to use optical flow to track features extracted more densely than other systems, which allows them to obtain a more precise object pose estimation. Furthermore the optical flow and the object and camera motions are tightly refined. CubeSLAM <ref type="bibr" target="#b12">[13]</ref> is different from the previously cited papers as it is an object based SLAM. A 3D bounding box is fitted for each object detected by a CNN (such as <ref type="bibr" target="#b18">[19]</ref>), which allows them to know the object 6 DoF pose and dimensions for each image. The bounding boxes poses are then optimized along with the camera poses in a single BA, similarly to </p></div>
<div><head>III. BACKGROUND ON TWISTS AND HOMOGENEOUS</head></div>
<div><head>TRANSFORMATIONS</head><p>In this section we present the mathematical concepts and notations that we use in this paper. A rigid object o, which can move in the 3D scene, can be associated with its coordinate frame F o . Its pose relative to the world coordinate frame F w can be represented by the homogeneous matrix in the i th frame</p><formula xml:id="formula_0">w T oi = w R oi w t oi 0 1 ∈ SE(3)<label>(1)</label></formula><p>where SE(3) denotes the special euclidean group. This matrix maps points expressed in the object coordinate frame and denoted o X to points expressed in the world coordinate frame and denoted w X, according to the following equation: w X = w T o o X. The velocity of a moving object can be represented using a twist ξ defined as</p><formula xml:id="formula_1">ξ = v x v y v z ω x ω y ω z ⊤ = v ω ⊤ ∈ R 6 (2)</formula><p>where the first</p><formula xml:id="formula_2">3 components v = (v x , v y , v z ) ⊤ ∈ R 3</formula><p>denote the translational velocity and the other components ω = (ω x , ω y , ω z ) ⊤ ∈ R 3 represent the rotational velocity. There exists a matrix representation for twists that can be obtained using the operator [.] ∧ defined by:</p><formula xml:id="formula_3">[ξ] ∧ = [ω] × v 0 0 ∈ se(3)<label>(3)</label></formula><p>where [.] × is the skew-symmetric operator defined such as for a = (a x , a y , a z ) ⊤ ∈ R 3 :</p><formula xml:id="formula_4">[a] × =   0 -a z a y a z 0 -a x -a y a x 0  <label>(4)</label></formula><p>and se( <ref type="formula" target="#formula_3">3</ref>) is the Lie algebra associated to SE(3). We denote w ξ oi the twist corresponding to the velocity of the object o at frame i expressed in the world coordinate frame. Similarly, oi ξ oi is the velocity of the object o at frame i expressed in its own coordinate frame. As velocities can be integrated over time to obtain new positions, there exists a mapping from se(3) to SE(3) called the exponential map and denoted:</p><formula xml:id="formula_5">exp : [ξδt] ∧ ∈ se(3) -→ T ∈ SE(3)<label>(5)</label></formula><p>where δt is the time interval duration <ref type="bibr" target="#b19">[20]</ref>.</p><p>Using the exponential map we can recover the pose of the object o moving according to the twist w ξ oi from its initial pose at frame i, w T oi to its next pose at frame i + 1, w T oi+1 using the following formula:</p><formula xml:id="formula_6">w T oi+1 = exp( w ξ oi δt i ) w T oi = w T oi exp( oi ξ oi δt i ) (6)</formula><p>Note that the choice of coordinate frame matters, it can be useful to define an operator to change the coordinate frame of a twist. Such operator is called the adjoint map w V oi ∈ R 6×6 and links twists in different coordinate frames according to:</p><formula xml:id="formula_7">w ξ oi = w V oi oi ξ oi<label>(7)</label></formula><p>The adjoint map can be computed using the relative pose w T oi between F oi and F w :</p><formula xml:id="formula_8">w V oi = w R oi [ w t oi ] × w R oi 0 w R oi<label>(8)</label></formula><p>For simplicity we will consider in the remainder of this paper that δt = 1 without loss of generality.</p></div>
<div><head>IV. TWISTSLAM: CONSTRAINED SLAM IN DYNAMIC ENVIRONMENT</head><p>In this section we present our approach. Our general idea is to represent the world as a graph of semantic clusters, which is similar to a scene graph and can be seen in figure <ref type="figure" target="#fig_0">2</ref>. The vertices of the graph correspond to objects in the scene and the edges to physical links that exist between objects. Our goal is to estimate the pose of the camera and the pose of every moving object while using mechanical joints between objects to improve those estimations. For example both clusters car in our graph are linked to the road with a planar constraint that allows only 3 degrees of freedom: a rotation around the normal of the plane and 2 translations within the plane. Such simple representations allow us to be highly generic as, for a given semantic class, we only need to define its static parent and the type of mechanical joint. The pipeline of our approach is presented in figure <ref type="figure" target="#fig_1">3</ref>. Using semantic information we create clusters of points corresponding to objects in the scene. Then, we use static semantic clusters (e.g. road, floor, house) to robustly track the camera, even in dynamic scenes. Next, we match keypoints corresponding to dynamic objects (e.g., car, bike,...) to either track them or triangulate new 3D points using stereo information. All poses estimations from the camera and the objects are then refined with static and dynamic 3D points in a bundle adjustment process.</p><p>The main novelty of our approach comes from the fact that we optimize the velocities of dynamic objects rather than their pose and constrain the velocities according to mechanical joints between objects. This approach is highly generic as we only need to define a handful of joints (that correspond to normalized joints in mechanics) and a list of semantic classes pairs for each joint (e.g. the wall-door joint corresponds to a revolute joint, the car-road joint corresponds to a planar joint). As we will latter show, it allows us to remove displacements along directions that are not physically possible (e.g. a car translating vertically). This allows us to obtain a more precise estimation of the dynamic object poses.</p></div>
<div><head>A. Creating Clusters from panoptic segmentation</head><p>Most recent semantic dynamic SLAM systems use either an object detection or an instance segmentation algorithm. Working in the continuity of S 3 LAM <ref type="bibr" target="#b10">[11]</ref> we chose to estimate the panoptic segmentation of images, obtained using <ref type="bibr" target="#b20">[21]</ref>. This allows us to know the semantic class of each pixel in the image and to give a unique id to each object. Similarly to <ref type="bibr" target="#b10">[11]</ref> we fuse multiple 2D observations of a single 3D point to obtain its class and id. Doing so we obtain a semantic map, which allows us to create a set of</p><formula xml:id="formula_9">K clusters O = {O k , k ∈ [1, K]}.</formula><p>A cluster is a set of 3D points corresponding to a single object in the scene. Points are grouped according to their class and instance id. The set of clusters can be expressed as the set of a priori static clusters S (such as road, building, ...) and the set of a priori dynamic clusters D (such as car, bike, human, bus, ...). As static clusters are fixed, we represent their 3D points { w X} in the world frame. In contrast, each dynamic cluster contains a set of 3D points { o X} expressed in the object coordinate frame, a set of poses { o T w } and a set of twists { w ξ o } representing the cluster trajectory and velocity through time. For simplicity in the remainder of this paper we will omit the object index k as its use is straightforward. </p></div>
<div><head>B. Clusters geometry</head><p>Our goal is to constrain the velocity of moving clusters according to mechanical joints. To do so we need to estimate the pose of those joints. We propose to do this using the estimated geometry of some clusters. We chose to consider only planar clusters, which allows our approach to be highly generic as planes are common in man-made environment. For clusters corresponding to a priori chosen classes (such as the road or the facade of a building) we estimate a 3D plane, represented by π = (a, b, c, d) ⊤ with ||π|| 2 = 1, using only its 3D points { w X}. The plane follows the following equation: π ⊤w X = 0 and can be estimated using an SVD. To make it robust to outliers we use a RANSAC scheme.</p></div>
<div><head>C. Dynamic SLAM</head><p>As we do not know which dynamic objects in the scene are really moving we chose to estimate the camera pose using only static objects. Using points from static clusters we minimize the following cost function</p><formula xml:id="formula_10">E( ci T w ) = j∈S ρ(|| i x j -p( ci T w , w X j )|| Σ -1 i,j )<label>(9)</label></formula><p>where i x j is the 2D keypoint corresponding to the observation of w X j in the i th frame, p is the pinhole camera projection function, ρ is a robust cost function (in our case Huber) <ref type="bibr" target="#b21">[22]</ref> and Σ i,j is the covariance matrix of the reprojection error. Doing so the estimated camera pose does not take into account potentially moving objects, hence it is robust in dynamic scenes. However the estimation can be deteriorated in scenes that contain many potentially moving objects that are in reality static, like for example parked cars. To solve this problem, we chose to estimate the pose of all moving objects and integrate them in the bundle adjustment, so that the velocity of static objects is close to 0 and their points act as static points.</p></div>
<div><head>D. Dynamic data association and keypoints</head><p>Dynamic data association is a challenging problem for two reasons: first the combination of the camera and the object movements can produce large displacements in the image space thus needing a large radius search for keypoints matching. Second a large movement can cause an important visual variation of the object in the image (e.g. due to luminosity changes on the object or to viewpoint changes) which makes the matching process more difficult. To overcome those challenges we propose to use the optical flow estimation produced by a CNN <ref type="bibr" target="#b22">[23]</ref> to have a good estimate of the keypoints location and reduce the search radius, thus reducing both search time and the probability of false matches.</p><p>One problem of object tracking compared to classical SLAM is that dynamic objects usually occupy a small portion of the image, leading to too few object points to obtain a precise estimation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. To solve this problem we force the keypoint extraction process to keep more keypoints from areas defined by objects bounding boxes. The keypoints are then used either to create new 3D points with stereo triangulation, which are added to existing clusters or used to create new clusters, or used to track the existing cluster.</p></div>
<div><head>E. Mechanical joints as inter-cluster constraints</head><p>Using matches found by the data association process we seek to estimate the pose of dynamic objects in the scene. Our assumption in this work is that many moving clusters can be represented as being linked to a static parent cluster with a specific mechanical joint. There exist 12 normalized joints (ISO 3952) that can be associated with the degrees of freedom they have. For example the planar joint has 3 degrees of freedom: 2 translations in the plane and 1 rotation around its normal, this joint can represent the displacement of a car relative to its static parent, the road. Another example is the revolute joint which has a single degree of freedom corresponding to the rotation around a single axis. In this case the static parent cluster is the wall, the moving cluster is the door and its only possible movements are rotations around the axis of the joint (corresponding to the hinge).</p><p>To easily model all types of joints, similarly to <ref type="bibr" target="#b23">[24]</ref>, we propose to decompose the space of twist as the sum of two orthogonal spaces:</p><formula xml:id="formula_11">R 6 = F l + F ⊤ l (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where F l (which stands for freedom) is the space of twists allowed by the mechanical joint l with coordinate frame F l . In the case of a planar joint with axis z, F l is defined as:</p><formula xml:id="formula_13">F l = Span( 1 1 0 0 0 1 ⊤ ) (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>where Span is the linear span <ref type="bibr" target="#b24">[25]</ref>. In general we note:</p><formula xml:id="formula_15">F l = Span(A l )<label>(12)</label></formula><p>where A l is a basis of F l . To make the displacement of an object physically accurate, its twists have to lie within the F l space. To do so we project the twist from its original space to F l . This is straightforward as R 6 is Euclidean, the operation projector is a 6 × 6 matrix defined as:</p><formula xml:id="formula_16">Π l = A l (A ⊤ l A l ) -1 A ⊤ l (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>In the example of a planar joint, it is easy to compute that:</p><formula xml:id="formula_18">Π l =        </formula><p>1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1</p><formula xml:id="formula_19">       <label>(14)</label></formula><p>in that case, a general twist can be projected such that:</p><formula xml:id="formula_20">Π l ξ = v x v y 0 0 0 ω z ⊤ (<label>15</label></formula><formula xml:id="formula_21">)</formula><p>As we can see the only remaining degrees of freedom of the twist are coherent with the joint. Using this new constraint, we can modify the reprojection equation:</p><formula xml:id="formula_22">i x j = p( ci T w exp (Π w l ξ oi ) w T oi-1 , o X j )<label>(16)</label></formula><p>However this equation is only true if the twist is expressed in the joint coordinate frame, yet according to <ref type="bibr" target="#b5">(6)</ref>, it is naturally expressed either in the world or in the object coordinate frame.</p><p>To change the coordinate frame of a twist we can use the adjoint map defined in <ref type="bibr" target="#b7">(8)</ref>. Hence the reprojection equation of the j th point in frame i becomes:</p><formula xml:id="formula_23">i x j = p( ci T w exp ( w V l Π l l V w w ξ oi ) w T oi-1 , o X j ) (17)</formula><p>In the remainder of this paper we will note Π = w V l Π l l V w for simplicity. This equation takes a 3D point in the object frame, transforms it in the world frame using the previous object pose and multiplies it by the exponential of the current twist to get its current position. The twist is expressed in the joint coordinate frame with the adjoint map, projected using Π l to keep only the relevant components and expressed again in the world frame with the inverse adjoint map. Doing this we obtain a 3D point in the world frame for frame i with a transformation that perfectly respects the mechanical joint. We then apply the camera pose to obtain the point in the camera coordinate frame, which allows us to project it in the image.</p><p>Using the reprojection function we can estimate the twist corresponding to the transformation of a set of object points between frame i -1 and i by minimizing the following error:</p><formula xml:id="formula_24">E( oi ξ w )= j ρ(|| i x j -p( ci T w exp(Π w ξ oi ) w T oi-1 , o X j )|| Σ -1 i,j ) (18)</formula><p>where ρ is the Huber robust estimator <ref type="bibr" target="#b21">[22]</ref> and Σ i,j is the covariance matrix of the reprojection error. In <ref type="bibr" target="#b0">[1]</ref> the convariance matrix depends on the scale at which the keypoints are observed. In our case we choose to estimate it using the median absolute deviation (MAD) <ref type="bibr" target="#b21">[22]</ref> that is a robust estimator of the standard deviation of the reprojection error. We perform the optimization using the Levenberg-Marquardt algorithm on matches found between the current and the previous frame. Then we refine this twist with an approach similar to <ref type="bibr" target="#b0">[1]</ref> by projecting map points, transformed with the estimated twist, in the current frame to search for additional matches and obtain a more accurate estimation. The object pose in frame i is then updated as w T oi = exp(Π w ξ oi ) w T oi-1 . This tracking procedure is repeated for all objects.</p></div>
<div><head>F. Dynamic Bundle Adjustment</head><p>The goal of classical bundle adjustment is to refine the camera trajectory and 3D points position estimation. The dynamic bundle adjustment has multiple goals. First, the refinement of the dynamic objects trajectory and their 3D points position, jointly with the camera trajectory and 3D static points position. Second, it allows to link the object and the camera trajectory, indeed if the bundle adjustment did not take into account dynamic objects, only the camera pose would have an impact on the object pose, which would not improve it. By taking into account dynamic points whose position is estimated over time we can use them to refine the camera pose, similarly to static points but with less accuracy since object pose estimation is more noisy. Finally, it allows us to apply a soft constrain on twists within a temporal window. Doing so we obtain smoother trajectories and velocities that are more physically plausible.</p><p>Our bundle adjustment cost function can be written as follows:</p><formula xml:id="formula_25">E({ w ξ o , c T w , w X, o X}) = i,j e i,j stat + i,j e i,j dyna + i e i const (<label>19</label></formula><p>) where e i,j stat is the classical static reprojection error:</p><formula xml:id="formula_26">e i,j stat = ρ(|| i x j -p( ci T w , w X j )|| Σ -1 i,j</formula><p>) e i,j dyna is a dynamic reprojection error:</p><formula xml:id="formula_27">e i,j dyna = ρ(|| i x j -p( c T w exp(Π w ξ oi ) w T oi , o X)|| Σ -1 i,j )</formula><p>where Σ -1 i,j is estimated using the MAD as in equation <ref type="bibr" target="#b17">(18)</ref>. And e i const is a constant velocity model that penalizes twists variations by linking 3 consecutive poses:</p><formula xml:id="formula_28">e i const = ρ(||Π w ξoi+1 -Π w ξoi || W )</formula><p>where W is a diagonal weight matrix used to balance the errors, w ξoi+1 is the twist linking the poses exp(Π w ξ oi ) w T oi and exp(Π w ξ oi+1 ) w T oi+1 and w ξoi is the twist linking the poses exp(Π w ξ oi-1 ) w T oi-1 and exp(Π w ξ oi ) w T oi . Those twists are computed using the logmap from SE(3) to se(3) defined in <ref type="bibr" target="#b19">[20]</ref> and can be written for w ξoi+1 as:</p><formula xml:id="formula_29">w ξoi+1 = log(exp((Π w ξ oi+1 ) w T oi+1 )(exp(Π w ξ oi ) w T oi ) -1 )</formula><p>This equation moves each pose while respecting the mechanical joints constraints. Optimizing it can be cumbersome however the Schur trick can be applied as its Hessian is sparse <ref type="bibr" target="#b4">[5]</ref>. These equations are classically optimized on a set of local keyframes that share visual information, but in our case, inspired by <ref type="bibr" target="#b7">[8]</ref> we chose to have 2 sets of keyframes: temporal and spatial. All frames are converted to temporal keyframes to improve the tracking of fast moving objects and be able to track an object as soon as it enters the field of view of the camera. Keyframes stay in the temporal set for a fixed duration (in our case 5 seconds) they are then culled more severely than in ORB-SLAM2. This allows us to keep a reasonable number of keyframes. We chose to optimize camera poses on the set of temporal and local keyframes, while object poses are only optimized on the set of temporal keyframes and fixed in all other keyframes. Doing so, we apply our constant motion model only on the temporal window, allowing clusters to accelerate or decelerate.</p></div>
<div><head>G. Computing the cost functions jacobians</head><p>To optimize the cost functions ( <ref type="formula">18</ref>) and ( <ref type="formula" target="#formula_25">19</ref>) with a Levenberg-Marquardt optimizer we need to compute their jacobian. First we compute the Jacobian of the cost function used for object tracking E( oi ξ w ). Using the chain rule and getting inspiration from <ref type="bibr" target="#b19">[20]</ref> it can be shown that:</p><formula xml:id="formula_30">J E = ∂E( oi ξ w ) ∂ oi ξ w = ∂p( c X) ∂ c X ( c X ⊤ ⊗ I 3 )(I 4 ⊗ c R w )(I 3 ⊗ w T o )∂ exp Π (<label>20</label></formula><formula xml:id="formula_31">)</formula><p>where ⊗ is the Kronecker product, I N is an identity matrix of size N , c R w is the rotation matrix of c T w and:</p><formula xml:id="formula_32">∂ exp = ∂ exp(ξ) ∂ξ =     0 3×3 -[e 1 ] × 0 3×3 -[e 2 ] × 0 3×3 -[e 3 ] × I 3 0 3×3    <label>(21)</label></formula><p>where {e 1 , e 2 , e 3 } is the canonical base of R 3 . Then, we compute the jacobian of e dyna in equation <ref type="bibr" target="#b18">(19)</ref>, which is very similar to the previous jacobian. The derivatives of the function with respect to camera poses and points are the same as for classical bundle adjustment <ref type="bibr" target="#b25">[26]</ref>. For the object poses we can compute:</p><formula xml:id="formula_33">J e dyna = ∂e dyna ∂ oi ξ w = ∂p( c X) ∂ c X ( o X ⊤ ⊗ I 3 )(I 4 ⊗ c R w )(I 3 ⊗ w T o )∂ exp Π<label>(22)</label></formula><p>Finally we compute the jacobian of the constant velocity constrain with respect to each of the 3 twists involved in the constrain:</p><formula xml:id="formula_34">J econst = ∂econst ∂ w ξo i-1 ∂econst ∂ w ξo i ∂econst ∂ w ξo i+1 (23)</formula><p>we only show here the left part of the jacobian as the other parts are similar.</p><formula xml:id="formula_35">∂e const ∂ w ξ oi-1 = ∂ log(T) ∂T (I 4 ⊗ R)∂ exp Π<label>(24)</label></formula><p>with T = exp((Π w ξ oi ) w T oi )(exp(Π w ξ oi-1 ) w T oi-1 ) -1 , R is the rotation matrix of exp((Π w ξ oi ) w T oi )( w T oi-1 ) -1 and the derivative of the logmap is given by <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div><head>V. EXPERIMENTS</head><p>In this section we present the experiments we conducted to test our approach. We evaluate both the accuracy of the camera pose estimation and of the object pose estimation.</p></div>
<div><head>A. Experiments details</head><p>Datasets. We evaluate our approach on the KITTI <ref type="bibr" target="#b26">[27]</ref> tracking dataset which consists in multiple sequences recorded from a camera mounted on a car. This dataset is particularly interesting for our approach as it contains the ground truth for both the camera pose and for some objects poses such as vehicles. It should be noted that the segmentation network can yield an "unknown" class which we consider to be static, as the dynamic classes in the KITTI dataset (cars, bikes and pedestrians) are correctly segmented by the network. Metrics. The metrics for the evaluation of SLAM systems are usually the absolute translation error (ATE) <ref type="bibr" target="#b27">[28]</ref> and the relative pose error (RPE) <ref type="bibr" target="#b28">[29]</ref>. For each sequence we report the translation and rotation parts of the RPE, as it is done by both VDO-SLAM and <software ContextAttributes="used">DynaSLAM</software>2. The object pose estimation accuracy can be evaluated using 2 different types of metrics: on the one hand the ATE and RPE that measure the quality of the objects trajectories and on the other hand the MOTP that evaluates the per-frame accuracy of objects 3D bounding boxes estimations and that we compute similarly to <ref type="bibr" target="#b4">[5]</ref> using KITTI evaluation tools. As we do not estimate object boxes we use the ground truth box at the first pose of each object and propagate it using our camera and object pose estimations. We evaluate the true positive rate (TP) and the MOTP using the projected 3D bounding box (2D), in bird view (BV) and in 3D.</p></div>
<div><head>B. Camera pose estimation</head><p>In this subsection we evaluate the accuracy of our camera pose estimation. Similarly to <ref type="bibr" target="#b4">[5]</ref> we only show here sequences in which the camera is moving. As we can see in table I our approach improves camera pose estimation on several sequences. To evaluate the stability of our approach we also computed the standard deviation over 10 runs for the sequence 3 and obtained a value of 1 ×10 -4 and 4 × 10 -4 for the translation and the rotation respectively. As objects are often either small, only visible for a short time or static, <ref type="bibr" target="#b0">[1]</ref> performs well, but as we track clusters using many points, with a good precision, especially for clusters that do not move, we are able to reduce the drift. <ref type="bibr" target="#b5">[6]</ref> also gives good results but requires depth information while our approach gives similar or better results than RGB based approaches. The most important improvement of our approach is in terms of object tracking accuracy as we can see in table <ref type="table" target="#tab_0">II</ref>. In this subsection we evaluate the accuracy of our object pose estimation. As we can see we improve object tracking accuracy, particularly for static objects such as the car 35 from sequence 11. The most important improvements usually come from the rotational part of the RPE, which is understandable as we only have 1 degree of freedom for the rotation of cars. We also observe that the most challenging cases happen when an object starts and stays far from the camera (e.g. seq. 05 and 10) because object tracking uses 3D points triangulated from stereo matches that are imprecise when points are far from the camera. We argue that the bruteforce keypoint matching of <ref type="bibr" target="#b4">[5]</ref> help them when few frame to frame matches can be found, which can happen when the object is far from the camera. Furthermore we have not implemented a way to relocalize an object that has been lost for multiple frames. Thus on some sequences (e.g. car 0 of seq. 11 and car 12 of seq. 20) in which the objects are alternatively far and close from the camera, we are only able to track them on a small portion of their trajectory. However, we can see that we are generally able to accurately track objects for most of their trajectory. During our experiments we saw that we were able to track pedestrians, despite the fact that they are not rigid. We believe that our approach works because pedestrians undergo small deformations around arms and legs. As for camera pose estimation, we computed the standard deviation of object pose estimation for the sequence 3 and obtained values of 3.2 × 10 -2 , 2.8 × 10 -2 and 4.7 × 10 -2 for the APE, the translational RPE and the rotational RPE. We also evaluated our results with an ANOVA which shows a significant difference (p-values ≤ 0.1) for most of our experiments.</p><p>We also show some qualitative results for the mapping, the camera and object pose estimation. The results are visible in figure <ref type="figure">4</ref>. We are able to track multiple objects on all sequences. The estimated speed is very close from the ground truth with a maximum difference of about 3 km/h which occurs when the object is far from the camera or when it is created. Looking at the bounding boxes we can see that they coincide and thus that the poses are well estimated for near and far objects. As we can see in the middle figure we can accurately track non rigid objects such as the cyclist, as long as most of their surface is rigid. We also show on the right an example of both a tracked static car and a car that slows down and speeds up. As we can see the estimated speed is close to the 0 for the static car, making the dynamic keypoints act like static ones.</p></div>
<div><head>VI. CONCLUSION</head><p>In this paper we proposed a new stereo semantic dynamic SLAM system called TwistSLAM, able to estimate both the pose of the camera as well as to track all dynamic objects in the scene. Using mechanical joints between clusters we can constrain objects movements to physically possible movements, which allows us to improve both camera and objects pose estimation compared to the state of the art.</p></div><figure xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Example of semantic graph: dynamic clusters are linked to static parent clusters with mechanical joints such as planar or revolute.</figDesc></figure>
<figure xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. The pipeline of our approach: static keypoints are extracted from a stereo image (a) and used for camera tracking (d), dynamic keypoints are extracted from bounding boxes within the stereo images (b) and matched using optical flow (c) with the previous frame to track dynamic objects (e). The keyframe is then segmented (f) to create new semantic map points and clusters. Finally the object and camera poses are jointly refined with the dynamic and static map points in a BA (g).</figDesc><graphic coords="5,459.48,128.68,72.72,64.30" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head>TABLE I CAMERA</head><label>I</label><figDesc>POSE ESTIMATION COMPARISON ON THE KITTI TRACKING DATASET.</figDesc><table><row><cell>seq</cell><cell cols="2">ORB-SLAM2 [1]</cell><cell cols="2">DynaSLAM [3]</cell><cell cols="2">VDO-SLAM (RGB-D) [6]</cell><cell cols="2">DynaSLAM2 [5]</cell><cell>Ours</cell><cell /></row><row><cell /><cell cols="5">RPE t (m/f) RPE R (°/f) RPE t (m/f) RPE R (°/f) RPE t (m/f)</cell><cell>RPE R (°/f)</cell><cell>RPE t (m/f)</cell><cell cols="3">RPE R (°/f) RPE t (m/f) RPE R (°/f)</cell></row><row><cell>00</cell><cell>0.04</cell><cell>0.06</cell><cell>0.04</cell><cell>0.06</cell><cell>0.07</cell><cell>0.07</cell><cell>0.04</cell><cell>0.06</cell><cell>0.04</cell><cell>0.05</cell></row><row><cell>01</cell><cell>0.05</cell><cell>0.04</cell><cell>0.05</cell><cell>0.04</cell><cell>0.04</cell><cell>0.12</cell><cell>0.05</cell><cell>0.04</cell><cell>0.04</cell><cell>0.03</cell></row><row><cell>02</cell><cell>0.04</cell><cell>0.03</cell><cell>0.04</cell><cell>0.03</cell><cell>0.02</cell><cell>0.04</cell><cell>0.04</cell><cell>0.02</cell><cell>0.03</cell><cell>0.03</cell></row><row><cell>03</cell><cell>0.07</cell><cell>0.04</cell><cell>0.07</cell><cell>0.04</cell><cell>0.03</cell><cell>0.08</cell><cell>0.06</cell><cell>0.04</cell><cell>0.06</cell><cell>0.02</cell></row><row><cell>04</cell><cell>0.07</cell><cell>0.06</cell><cell>0.07</cell><cell>0.06</cell><cell>0.05</cell><cell>0.11</cell><cell>0.07</cell><cell>0.06</cell><cell>0.06</cell><cell>0.04</cell></row><row><cell>05</cell><cell>0.06</cell><cell>0.03</cell><cell>0.06</cell><cell>0.03</cell><cell>0.02</cell><cell>0.09</cell><cell>0.06</cell><cell>0.03</cell><cell>0.06</cell><cell>0.02</cell></row><row><cell>06</cell><cell>0.02</cell><cell>0.04</cell><cell>0.02</cell><cell>0.04</cell><cell>0.05</cell><cell>0.02</cell><cell>0.02</cell><cell>0.01</cell><cell>0.02</cell><cell>0.04</cell></row><row><cell>07</cell><cell>0.05</cell><cell>0.07</cell><cell>0.05</cell><cell>0.07</cell><cell>-</cell><cell>-</cell><cell>0.05</cell><cell>0.07</cell><cell>0.04</cell><cell>0.04</cell></row><row><cell>08</cell><cell>0.08</cell><cell>0.04</cell><cell>0.08</cell><cell>0.04</cell><cell>-</cell><cell>-</cell><cell>0.10</cell><cell>0.04</cell><cell>0.07</cell><cell>0.03</cell></row><row><cell>09</cell><cell>0.06</cell><cell>0.05</cell><cell>0.06</cell><cell>0.05</cell><cell>-</cell><cell>-</cell><cell>0.06</cell><cell>0.06</cell><cell>0.05</cell><cell>0.04</cell></row><row><cell>10</cell><cell>0.07</cell><cell>0.04</cell><cell>0.07</cell><cell>0.04</cell><cell>-</cell><cell>-</cell><cell>0.07</cell><cell>0.03</cell><cell>0.07</cell><cell>0.03</cell></row><row><cell>11</cell><cell>0.04</cell><cell>0.03</cell><cell>0.04</cell><cell>0.03</cell><cell>-</cell><cell>-</cell><cell>0.04</cell><cell>0.03</cell><cell>0.03</cell><cell>0.02</cell></row><row><cell>13</cell><cell>0.04</cell><cell>0.05</cell><cell>0.04</cell><cell>0.05</cell><cell>-</cell><cell>-</cell><cell>0.04</cell><cell>0.04</cell><cell>0.03</cell><cell>0.04</cell></row><row><cell>14</cell><cell>0.03</cell><cell>0.08</cell><cell>0.03</cell><cell>0.08</cell><cell>-</cell><cell>-</cell><cell>0.03</cell><cell>0.08</cell><cell>0.03</cell><cell>0.06</cell></row><row><cell>18</cell><cell>0.05</cell><cell>0.03</cell><cell>0.05</cell><cell>0.03</cell><cell>0.02</cell><cell>0.07</cell><cell>0.05</cell><cell>0.02</cell><cell>0.04</cell><cell>0.02</cell></row><row><cell>19</cell><cell>0.05</cell><cell>0.03</cell><cell>0.05</cell><cell>0.03</cell><cell>-</cell><cell>-</cell><cell>0.05</cell><cell>0.02</cell><cell>0.03</cell><cell>0.03</cell></row><row><cell>20</cell><cell>0.11</cell><cell>0.07</cell><cell>0.05</cell><cell>0.04</cell><cell>0.03</cell><cell>0.17</cell><cell>0.07</cell><cell>0.04</cell><cell>0.04</cell><cell>0.03</cell></row><row><cell>mean</cell><cell>0.055</cell><cell>0.046</cell><cell>0.051</cell><cell>0.045</cell><cell>-</cell><cell>-</cell><cell>0.053</cell><cell>0.041</cell><cell>0.044</cell><cell>0.034</cell></row><row><cell>std</cell><cell>0.020</cell><cell>0.016</cell><cell>0.016</cell><cell>0.015</cell><cell>-</cell><cell>-</cell><cell>0.019</cell><cell>0.020</cell><cell>0.015</cell><cell>0.011</cell></row><row><cell cols="3">C. Object pose estimation.</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ORB-SLAM2: An open-source SLAM system for monocular, stereo, and RGB-D cameras</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Robotics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1255" to="1262" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LSD-SLAM: Large-scale direct monocular SLAM</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schöps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on computer vision</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DynaSLAM: Tracking, mapping, and inpainting in dynamic scenes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bescos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Fácil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4076" to="4083" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DS-SLAM: A semantic visual SLAM towards dynamic environments</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ IROS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1168" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DynaSLAM II: Tightlycoupled multi-object tracking and SLAM</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bescos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="5191" to="5198" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">VDO-SLAM: a visual dynamic object-aware SLAM system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11052</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maskfusion: Real-time recognition, tracking and reconstruction of multiple moving objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Runz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buffier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int Symp. on Mixed and Augmented Reality (ISMAR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ClusterVO: Clustering moving instances and estimating visual odometry for self and surroundings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2168" to="2177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carlone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on robotics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1309" to="1332" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Kimera: an open-source library for real-time metric-semantic localization and mapping</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rosinol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carlone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1689" to="1696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">S 3 LAM: Structured scene SLAM</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kacete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Royan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07339</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">RGB-D SLAM in dynamic environments using static point weighting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2263" to="2270" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CubeSLAM: Monocular 3-D object SLAM</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Robotics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="925" to="938" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual SLAM algorithms: A survey from 2010 to 2016</title>
		<author>
			<persName><forename type="first">T</forename><surname>Taketomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Uchiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ikeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPSJ Transactions on Computer Vision and Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Real time localization and 3D reconstruction</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mouragnon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lhuillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dhome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dekeyser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sayd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parallel tracking and mapping for small AR workspaces</title>
		<author>
			<persName><forename type="first">G</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 6th IEEE and ACM int. symposium on mixed and augmented reality</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bundle adjustment-a modern synthesis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. workshop on vision algorithms</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="298" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Co-fusion: Real-time segmentation, tracking and fusion of multiple objects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rünz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Int Conf. on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4471" to="4478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A tutorial on se (3) transformation parameterizations and on-manifold optimization</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Blanco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">University of Malaga, Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<title level="m">Detectron2</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Experiments with robust estimation techniques in real-time robot vision</title>
		<author>
			<persName><forename type="first">E</forename><surname>Malis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marchand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE/RSJ Int. Conf. on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="223" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. on Computer Vision</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kinematic sets for realtime robust articulated object tracking</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Comport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chaumette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="374" to="391" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Linear algebra done right</title>
		<author>
			<persName><forename type="first">S</forename><surname>Axler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visual slam tutorial: Bundle adjustment</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR tutorial</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conf. on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A benchmark for the evaluation of RGB-D SLAM systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Engelhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Conf. on Intelligent Robot Systems (IROS)</title>
		<meeting>of the Int. Conf. on Intelligent Robot Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2012-10">Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A tutorial on quantitative trajectory evaluation for visual (-inertial) odometry</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ Int Conf. on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7244" to="7251" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>