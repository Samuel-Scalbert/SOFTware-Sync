<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" />
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">314327DE732C63AEDC18524018BB8838</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-21T10:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract />
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>Stuttering, a neurodevelopmental speech disorder, caused by the failure of speech sensorimotor, is defined by the disturbance of uncontrolled utterances: interjections (Insertion of sounds such as uh, uhm ), and core behaviors: blocks (involuntary pauses), repetitions (involuntary recurring sounds, words or phrases), and prolongations (abnormal extension of a speech sound segments) <ref type="bibr">(Smith and Weber 2017;</ref><ref type="bibr">Guitar 2019;</ref><ref type="bibr">Duffy 2019;</ref><ref type="bibr">Ward 2018)</ref>. Studies show that persons who stutter (PWS) encounter several hardships in social and professional interactions <ref type="bibr">(Kehoe and Contributors 2006)</ref>. In addition, more people are progressively interacting with voice assistants, but they ignore and fail to recognize stuttered speech <ref type="bibr">(Sheikh et al. 2021a)</ref>, and the stuttering detection (SD) can be exploited to improve automatic speech recognition (ASR) for PWS to access voice assistants such as Alexa, Siri, etc.</p><p>Usually, SD is addressed by various listening and brain scan tests <ref type="bibr">(Ingham et al. 1996;</ref><ref type="bibr">Smith and Weber 2017;</ref><ref type="bibr">Sheikh et al. 2021a</ref>). However, this method of SD is high-priced and requires a demanding effort from speech therapists. The presence of uncontrolled utterances is reflected in the acoustic domain, which helps to discriminate them in various stuttering types. Based on the acoustic cues present in stuttered speech, several people employed a machine learning paradigm for SD. Some of the current state-of-the-art stuttering detection deep learning modelling techniques include: ResNet+BiLSTM <ref type="bibr">(Kourkounakis et al. 2020;</ref><ref type="bibr">Jouaiti and Dautenhahn)</ref>, <ref type="bibr">FluentNet (Kourkounakis et al. 2021)</ref>, <ref type="bibr">(Kourkounakis et al. 2021</ref><ref type="bibr">StutterNet (Sheikh et al. 2021b</ref><ref type="bibr">, 2023)</ref>. <ref type="bibr">(Kourkounakis et al. , 2020) )</ref> approached the SD as a multiple binary classification problem and trained separate ResNet+BiLSTM classifiers and FluentNet classifiers for each stuttering type. The models were trained using spectrogram input features on a small set of 24 <software ContextAttributes="used">UCLASS</software> speakers. <ref type="bibr">(Sheikh et al. 2021b)</ref> approached SD via single branch StutterNet and proposed the time delay neural network based (TDNN) first multi-class classifier for SD and its types. The classifier is trained with 20 Mel-frequency cepstral coefficient (MFCCs) <ref type="bibr">(Huang et al. 2001</ref>) input features on a large set of more than 100 <software ContextAttributes="used">UCLASS</software> <ref type="bibr">(Howell et al. 2009)</ref> speakers. <ref type="bibr">(Lea et al. 2021</ref>) recently introduced a new SEP-28k stuttering dataset. (Jouaiti and Dautenhahn) exploited phoneme features and proposed BiLSTM method for SD. The method is trained on mel-spectral and phoneme-based input features by mixing SEP-28k, <software ContextAttributes="used">UCLASS</software>, and FluencyBank datasets. In order to show the efficacy of speech representations in SD, <ref type="bibr">(Bayerl et al. 2022</ref>) employed SVM as a downstream binary classifier on the FluencyBank (English) and KSoF (German) datasets.</p><p>They reported an average F1 score of 53.8% and 46.67% on the KSoF and FluencyBank datasets, respectively. Although the method shows promising results, however, they formulated SD as a binary class classification problem (one vs rest) which requires separate training for each disfluency pair. Additionally, the fluent speech of PWS was not considered in their experimental study which could have a substantial impact on SD, as also demonstrated by <ref type="bibr">(Sheikh et al. 2022b)</ref>. Comprehensive detail on SD methods can be found in (Sheikh 2023) and in the review papers by <ref type="bibr">(Sheikh et al. 2021a;</ref><ref type="bibr">Barrett et al. 2022)</ref>.</p><p>Over the years, several stuttering datasets including: SEP-28k (Lea et al. 2021), <software ContextAttributes="used">UCLASS</software> <ref type="bibr">(Sheikh et al. 2021a</ref><ref type="bibr">), LibriStutter (Kourkounakis et al. 2021</ref><ref type="bibr">), and FluencyBank (Lea et al. 2021</ref>) have been developed for investigating different SD models. Even though the SD methods discussed show promising results on these datasets, however, these datasets are relatively very small and are limited to only a certain number of speakers. Due to the varying nature of stuttering from person to person, these small datasets incline to be biased towards these small pool of speakers (Guitar 2019). In addition, the DL has shown tremendous improvement in ASR <ref type="bibr">(Nassif et al. 2019)</ref>, emotion detection <ref type="bibr">(Pepino et al. 2021;</ref><ref type="bibr">Akçay and</ref><ref type="bibr">Oguz 2020), speaker verification (Li et al. 2020)</ref>, etc, however, the improvement in SD is bounded, most likely due to the limited size of stuttering datasets, which are unable to capture different speaking styles, accents, linguistic content, etc. In addition, collecting medical data requires big-budget and is very taxing, and stuttering data collection is no exception.</p><p>To tackle this, we use the pre-trained speech embeddings that have been successfully used in ASR <ref type="bibr">(Baevski et al. 2020;</ref><ref type="bibr">Mohamed et al. 2022</ref>) and emotion recognition <ref type="bibr">(Pepino et al. 2021)</ref>, for instance. In this paper, we mainly focus on the speaker and Wave2Vec2.0 contextual embeddings. Pre-training a model on such massive datasets can successfully capture the variable speaking styles and emotional behaviors which are extremely important in the SD <ref type="bibr">(Sheikh et al. 2021a</ref>). The general framework of the proposed system is shown in Fig. <ref type="figure" target="#fig_0">3</ref>.</p><p>Our primary contributions to this paper are:</p><p>-We explore the use of speaker embeddings extracted from emphasized channel attention, propagation and aggregation (ECAPA)-TDNN (Desplanques et al. 2020), and <software ContextAttributes="used">Wav2</software>Vec2.0 speech representations for SD. -We provide a novel way for SD, which exploits the information from the fully connected (FC) layer of ECAPA-TDNN <ref type="bibr">(Desplanques et al. 2020</ref>) and draws on speech information from several hidden layers of the <software ContextAttributes="used">Wav2</software>vec2.0 model <ref type="bibr">(Baevski et al. 2020</ref>). -We also provide an analysis of the impact of using different layers from <software ContextAttributes="used">Wav2</software>Vec2.0 and their concatenation in SD and also investigate the impact of combining information from ECAPA-TDNN and <software ContextAttributes="used">Wav2</software>Vec2.0 embeddings via score fusion. The WaveVec2.0 model <ref type="bibr">(Baevski et al. 2020</ref>) is a self-supervised representation learning framework of raw audio, and is comprised of three modules including feature encoder f : X → Z, contextual block transformer g : Z → C and quantization block Z → Q as depicted in Fig. <ref type="figure">2</ref>. The feature encoder is comprised of multi-layer 1D convolution blocks followed by BN and GELU activation functions, takes the normalized raw input X and encodes it into local feature representations Z = f (X). These encoded feature representations of size Z T ×768 are then fed to contextual transformer block to learn contextual speech representations C = g(Z). The paper uses two different transformer networks with the base model consisting of 12 blocks having eight attention heads at each block, and the large model is comprised of 24 blocks with 16 attention heads at each block. The feature representations Z are also fed to the quantization module which is comprised of two codebooks having 320 possible entries in each. For each vector representation</p><formula xml:id="formula_0">z i ∈ Z, a logit of R 2×320 is Fig. 2: Block diagram of Wav2Vec2.0 architecture.</formula><p>chosen using (1) by concatenating the corresponding entries from each codebook, which is then followed by linear transformation to produce the quantized vector q i of the local feature encoder representation z i ∈ Z.</p><formula xml:id="formula_1">p g,v = exp(l g,v + η v )/τ V k=1 exp(l g,v + η v )/τ (1)</formula><p>where l is logit, v is v-th codebook entry, g is codebook group, η = -log(-log(u)) with u are uniform samples from U(0, 1), and τ is the temperature which controls the randomness. Similar to the masked language modelling, the model is pre-trained in a self-supervised fashion using eq. ( <ref type="formula">2</ref>) by randomly masking certain time stamp representation vectors of the feature encoder and the training objective is to reproduce the quantized qt latent speech representation from a set of K+1 distractors including candidate vector q t and K distractors ∈ Q for masked time stamp vectors at the end of contextual transformer block. The distractors are uniformly sampled from masked frames of the same speech utterance.</p><formula xml:id="formula_2">L cont = -log exp(sim(c t , q t )/τ ) q∈Q exp(sim(c t , q)/τ ) (2)</formula><p>where sim(c t , q t ) computes the cosine similarity between the quantized vector q t and contextualized transformer vector c t .</p><p>The authors have released several pre-trained feature embeddings with dimensions of 768 (base) and 1024 (large) and we are using the base one (768-dimensional) pre-trained on 960 hours of <software>LibriSpeech</software> dataset and then fine-tuned for ASR using CTC loss function by adding a linear layer on top of the contextual block.</p><p>The <software ContextAttributes="used">Wav2</software>Vec2.0 showed remarkable improvement in ASR (Baevski et al. 2020), emotion detection <ref type="figure">2</ref>.<ref type="bibr">(Pepino et al. 2021</ref>). In the stuttering speech, most parts of the speech utterance are perturbed, and it seems a plausible way to employ and explore the role of the contextual and encoder representations in SD. In this work, without fine-tuning, we employ a total of 13 contextual embeddings extracted from a local encoder and 12 layers of the contextual transformer block as depicted in Fig. </p></div>
<div><head n="3">Classifier Description</head></div>
<div><head n="3.1">K-Nearest Neighbourhood</head><p>A non-parametric supervised algorithm based on distance metric is used mostly for classification tasks. The prediction of the query sample depends on the voting majority of its K nearest neighbors (Murphy 2012). In this work, use the Minkowski metric distance from eq. ( <ref type="formula">3</ref>) with p = 2 (Euclidean) to fit the K-NN on the SEP-28k dataset using embeddings computed from pre-trained ECAPA-TDNN and <software ContextAttributes="used">Wav2</software>Vec2.0.</p><formula xml:id="formula_3">D = k i=1 ∥x i -y i ∥ p ) 1/p (3)</formula></div>
<div><head n="3.2">Gaussian Back-End</head><p>A naive Bayes classifier (NBC) is simply a Bayesian network to handle continuous features by representing the likelihood of features using Gaussian distribution (Murphy 2012). Given a data set D = (X i , d i ) of N samples with R 1×K 1 pre-trained features , NBC assumes that the likelihood of class conditional densities is normally distributed by</p><formula xml:id="formula_4">p(e|C = c, µ c , Σ c ) = N(e|µ c , Σ c ) (4)</formula><p>where e ∈ X, is extracted pre-trained representation, µ and Σ are class-specific mean vector and covariance matrix respectively. The posterior probability for each target class is computed then by Bayes' formula: vector of R 1×768×2 . Moreover, we also concatenate the contextual embeddings of the local encoder (L1), L7, and layer L11 of C after applying the statistical pooling and LDA (with a component size of four) which results in R N ×12 feature vector. LDA is useful when the number of features is large compared to the number of samples, and there is a need to reduce the dimensionality of the data while preserving the most relevant information for classification. We extract embeddings from the <software>PyTorch</software> version of <software ContextAttributes="used">Wav2</software>Vec2.0 (Paszke et al. 2019).</p><formula xml:id="formula_5">p(C = c|e, µ c , Σ c ) class posterior = class conditional likelihood p(e|C = c, µ c , Σ c ) p(C = c) K i=1 p(e|C = i, µ i , Σ c )p(C = i)<label>(5)</label></formula></div>
<div><head n="4.2">Dataset Description</head><p>In  Speaker embeddings: We see from Table <ref type="table">2</ref>, that the downstream classifiers trained on ECAPA-TDNN embeddings perform poorly in all the stuttering classes as compared to the baseline results from Table <ref type="table">2</ref>. This is evident from Fig. <ref type="figure" target="#fig_2">5</ref> as well, where the different stuttering type utterances are mixed and no clear cluster is visible among the disfluency classes. Furthermore, applying magnitude normalization on ECAPA-TDNN embeddings before passing them to the downstream classifiers, improves the SD performance marginally in the majority of classes. The ECAPA-TDNN is trained and adapted for the speaker identification task and it is likely possible that the information (such as linguistic content, prosody, and emotion state) which isn't essential for that task but could be crucial for SD gets removed from the latent embeddings.</p><p><software ContextAttributes="used">Wav2</software>Vec2.0 contextual embeddings: Table <ref type="table">2</ref>. shows the results with the last but two-layer of contextual transformer block C of <software ContextAttributes="used">Wav2</software>Vec2.0 with and without prior application of LDA. From the results, we can observe that for SD, the contextual embeddings from <software ContextAttributes="used">Wav2</software>Vec2.0 outperforms in all the classes with an overall relative improvement (TA) of 5.82% using <ref type="bibr">KNN, 11.54% using NBC, 10.38% using NN over (Sheikh et al. 2021b</ref>) and over <ref type="bibr">(Sheikh et al. 2022b</ref>) by 11.92% using KNN, 17.97% using NBC and 16.74% using NN. <ref type="bibr">Figure 4.</ref> shows the impact of different contextual embeddings in the detection of various stuttering classes. The plot shows almost a close trend in all the stuttering class accuracies. The detection accuracy of stuttering classes increases with the layer<ref type="foot" target="#foot_1">3</ref> number. We hypothesize that the lower layers including local encoder (L1) representations contain speech information only from the local window of size 25 ms, and, in addition, passing the representations to the downstream classifiers after applying the statistical pooling layer further restricts it in capturing more stutter specific patterns. In addition, the results show that the contextual layers from L6 to L12 of the <software ContextAttributes="used">Wav2</software>Vec2.0 model trained in a self-supervised fashion are able to capture rich stuttering patterns as also depicted in Fig. <ref type="figure" target="#fig_2">5</ref>. As for the last layer (L12), it slightly degrades performance in SD in comparison to its previous layer due to the fact that the transformer block C was fine-tuned and adapted towards the ASR task. By fine-tuning towards ASR, it is possible that the <software ContextAttributes="used">Wav2</software>Vec2.0 model has not focused on the information which is relevant to stuttering, resulting in the loss of rich stuttering information. Consider such an example of prosodic information, which is very essential in SD, but not that important for ASR. Using <software ContextAttributes="used">Wav2</software>Vec2.0 embeddings with NN in SD, there is an overall relative improvement of 61.36%, 47.91%, 14.65%, and 9.02% in repetitions, blocks, interjections, and fluents respectively over the MB StutterNet (Sheikh et al. 2022b), thus outperforms over the state-of-the-art results. Moreover, the prior application of LDA on <software ContextAttributes="used">Wav2</software>Vec2.0 representations further boosts the detection performance in repetitions by 8.29%, prolongation by 10.41%, blocks by 27.24%, and interjections by 8.72%.</p><p>Fusion: In addition, we fuse the ECAPA-TDNN and <software ContextAttributes="used">Wav2</software>Vec2.0 embeddings via score and embedding fusion schemes, the results of which can be seen in Table <ref type="table">2</ref>. While computing the final score p from ECAPA-TDNN and <software ContextAttributes="used">Wav2</software>Vec2.0 prediction probabilities, we empirically optimize the weighting parameter α on test set in p = α * p w2v2 + (1 -α) * p ecapa and we found α = 0.9 gives the best results. The ECAPA-TDNN representations which contain rich information about speakers' identity further enhances the overall detection performance of 2.1% using KNN, 0.65% using NBC, and 1% using NN. However, it doesn't contain enough rich information about suprasegmental, emotional content, etc, which are incredibly important for disfluent classes, thus acting as a negative transfer for some of them. Moreover, concatenating the ECAPA-TDNN and <software ContextAttributes="used">Wav2</software>Vec2.0 speech embeddings results in a further relative improvement of 8.66% in UAR using NN.</p><p>Each layer of the <software>Wav2</software>Vec2.0 model contains different speech representations, exploiting this fact, we integrate the representations after applying LDA (of component size four) from the local encoder (L1), L7 and L11 from contextual block C, resulting in a R N ×12 dimensional data. Concatenating information from multiple layers further improves the minority class recognition including prolongations and blocks by a relative margin of 7.29% and 29.74% respectively using NN. Moreover, the embedding fusion also enhances the UAR by a relative margin of 19. </p></div>
<div><head n="6">Conclusion</head><p>The automated stuttering detection task suffers from a lack of unlabeled data and thus limits the application of large deep models. To address this issue, we introduced a self-supervised learning framework that first learns a feature extractor for a pre-text task on a large amount of audio data and then employs the learned feature extractor for the downstream stuttering detection task with limited audio data. We investigated ECAPA-TDNN-based speaker recognition and <software>Wav2</software>Vec2.0-based speech recognition as two separate pre-text tasks trained on VoxCeleb and <software ContextAttributes="used">LibriSpeech</software>, respectively. Our study reveals that contextual embeddings associated with speech recognition tasks are more appropriate for SD than speaker embeddings. We found that <software ContextAttributes="used">Wav2</software>Vec2.0-based contextual embeddings yielded at least 19.17% relative improvement in UAR over the competitive state-of-the-art systems trained only on limited labeled data. We further improved SD performance by combining embedding from different layers of the <software ContextAttributes="used">Wav2</software>Vec2.0 model. We also found that post-processing the extracted embeddings with LDA improves classification performance. Our benchmarking experiments with three different classifiers for downstream tasks reveal that a simple MLP-based neural network performs best and it opens up opportunities for further advancements.</p><p>Even if the proposed self-supervised framework substantially improved the SD performance over baselines, the performance is still relatively low, possibly due to the presence of a mismatch effect in podcast recordings and data imbalance during training. The future work includes further investigations of the proposed approach by compensating for those effects. In our present work, we did not utilize the stuttering labeled dataset for training the pre-text task. This work can also be extended by including this dataset in this stage which may mitigate the audio domain mismatch between pre-text and downstream tasks. It is possible that the proposed system might fail in the cross lingual setup, and it would also be interesting to analyse the performance of the proposed models in cross lingual setup, where the model is trained on one stuttering language and tested on other. Due to inter-person variances, language/accent/dialect variability, and other speaking variations, stuttering identification is inherently a difficult process. In order to further enhance the stuttering detection systems, speaker-adaptive training and domain adaptation techniques can be exploited to learn these meta-data invariant features. </p></div><figure xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: Block diagram with raw speech as an input followed by embedding extractors, LDA for dimensionality reduction followed by downstream classifiers.</figDesc></figure>
<figure xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Impact of various Wav2Vec2.0 contextual layers in SD with KNN (Top), NBC (Middle), and NN (Bottom).</figDesc></figure>
<figure xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5: t-SNE embeddings from the FC layer of ECAPA-TDNN (T) and L11 of the contextual block C of Wav2Vec2.0 (B). The visualisation is only for exploration purposes to understand which embeddings are better for stuttering detection downstream classifiers (Van der Maaten and Hinton 2008).</figDesc><graphic coords="12,122.51,326.16,235.72,235.72" type="bitmap" /></figure>
<figure xml:id="fig_3"><head /><label /><figDesc>research 9(11) (2008) D. Ward, Stuttering and Cluttering: Frameworks for Understanding and Treatment, 2nd edn. (Psychology Press, ???, 2018)</figDesc></figure>
<figure type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>SEP-28k Dataset Split into Train, Val and Test Sets ( B: Block , F: Fluent , R: Repetition , P: Prolongation , I: Interjection, ID: SetID) For thorough evaluation, we train each of the proposed models 10 times. Table 2. shows the average UAR and accuracy results of different stuttering and fluent classes on exploiting different features extracted from ECAPA-TDNN and Wav2Vec2.0. We compare the proposed speaker and contextual embeddings based SD models to the work ResNet+BiLSTM (Kourkounakis et al. 2020), StutterNet (Sheikh et al. 2021b) (For a fair comparison, we trained these on the SEP-28k dataset with MFCC input features) and to the multibranched StutterNet (Sheikh et al. 2022b) having two different branches with one branch differentiating between fluent and stutter utterances and the other branch distinguishing among different disfluency types. We use LDA for dimensionality reduction of features in each case before passing them to the classifiers for prediction 2 .</figDesc><table><row><cell>this case study, we used SEP-28k stuttering dataset (Lea et al. 2021)</cell></row><row><cell>which consists of 28,177 speech samples from 385 podcasts. After removing</cell></row><row><cell>non-stuttered samples, we are left with 23573 annotated speech segments.</cell></row><row><cell>We randomly selected 80% podcasts (without mixing podcasts). The speaker</cell></row><row><cell>information is missing from the SEP-28k dataset, so we divided the dataset</cell></row><row><cell>based on podcast ids (assuming each podcast is having a unique speaker). This</cell></row><row><cell>dataset contains two different types of annotations including stuttering and</cell></row><row><cell>non-stuttering. We considered only stuttering annotations (repetitions, blocks,</cell></row></table><note><p><p><p><p><p><p />For implementing the proposed pipeline for NN, we use <software ContextAttributes="used" /> library</p>(Paszke  et al. 2019)<p />(Paszke  et al. 2019)</p>, and for LDA, KNN, and NBC, we have used the Scikit-learn<p />, and for LDA, KNN, and NBC, we have used the Scikit-learn</p>(Pedregosa et al. 2011)  <p />(Pedregosa et al. 2011)  </p>toolkit. We have chosen a value of K = 5 using the elbow method in the KNN classifier. The downstream NN is trained with a batch size of 128 using a normal sum of cross entropy loss (L tot = L f + L d , L f :FluentBranch loss, L d :DisfluentBranch loss) function optimized<p />toolkit. We have chosen a value of K = 5 using the elbow method in the KNN classifier. The downstream NN is trained with a batch size of 128 using a normal sum of cross entropy loss (L tot = L f + L d , L f :FluentBranch loss, L d :DisfluentBranch loss) function optimized</p></note></figure>
<figure type="table" xml:id="tab_2"><head /><label /><figDesc>17%, 36.84%, 46.18% over the MFCC-based baselines StutterNet (Sheikh et al. 2021b), MB StutterNet (Sheikh et al. 2022b), and MB ResNet+BiLSTM (Kourkounakis et al. 2020) respectively.</figDesc><table /></figure>
			<note place="foot" n="2" xml:id="foot_0"><p>We have not compared from an execution time perspective. However, since the LDA transformed data has only four dimensions compared to the original 192 (for speaker embeddings) and 768 (for contextual embeddings), we expect that the "with LDA" configuration drastically reduces the computational time.</p></note>
			<note place="foot" n="3" xml:id="foot_1"><p>L1 is a local encoder, L13 is the last layer of C</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was made with the support of the French National Research Agency, in the framework of the project ANR BENEPHIDIRE (18-CE36-0008-03). Experiments presented in this paper were carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER and several universities as well as other organizations(see https://www.grid5000.fr) and using the EXPLOR centre, hosted by the University of Lorraine.</p></div>
			</div>
			<div type="availability">
<div><head>Data Availibity</head><p>The dataset analysed during the current study is publicly available from (Lea  et al. 2021).</p></div>
			</div>

			<div type="annex">
<div><head>Conflict of Interest</head><p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper</p></div>			</div>
			<div type="references">

				<listBibl />
			</div>
		</back>
	</text>
</TEI>