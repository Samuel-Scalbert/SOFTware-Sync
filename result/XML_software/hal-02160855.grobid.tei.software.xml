<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" />
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">5CDAD1B1496DC6B26A7E156C9563D1A6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-21T10:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract />
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1.">INTRODUCTION</head><p>Sound conveys important information in our everyday lives and we depend on sounds to better understand changes in our physical environment and to perceive events occurring around us. We perceive the sound scene (the overall soundscape of e.g. an airport or inside a house) as well as individual sound events (e.g. car honks, footsteps, speech, etc.). Sound event detection within an audio recording refers to the task of detecting and classifying sound events, that is, temporally locating the occurrences of sound events in the recording and recognising which object or category each sound belongs to. Sound event detection has potential applications in noise monitoring in smart cities <ref type="bibr">[1,</ref><ref type="bibr">2]</ref>, surveillance [3], urban planning [1], multimedia information retrieval <ref type="bibr">[4,</ref><ref type="bibr">5]</ref>; and domestic applications such as smart homes, health monitoring systems and home security solutions <ref type="bibr">[6,</ref><ref type="bibr">7,</ref><ref type="bibr">8]</ref> to name a few. In recent years the field has gained increasing interest from the broader machine learning and audio processing research communities.</p><p>Sound event detection (SED) systems trained using weak labels have have seen significant interest <ref type="bibr">[6,</ref><ref type="bibr">9,</ref><ref type="bibr">10,</ref><ref type="bibr">11,</ref><ref type="bibr">12]</ref> in the research community, as they address some of the challenges involved in developing models that require strongly labeled data for training. In This work was made with the support of the French National Research Agency, in the framework of the project LEAUDS Learning to understand audio scenes (ANR-18-CE23-0020) and the French region Grand-Est. Experiments presented in this paper were carried out using the Grid5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER and several Universities as well as other organizations (see https://www.grid5000). particular, strongly labeled data is time-consuming and difficult to annotate as it requires annotating the temporal extent of event occurrences in addition to their presence or absence. Strong label annotations are also more likely to contain human errors/disagreement given the ambiguity in the perception of some sound event onsets and offsets. In the case of weakly labeled data, we only have information about whether an event is present in a recording or not. We have no information about how many times the event occurs nor the temporal locations of the occurrences within the audio clip. For real-world applications it is critical to build systems that generalize over a large number of sound classes and a variety of sound event distributions. In such cases, it may be more feasible to collect large quantities of weakly labeled data as opposed to strongly labeled data which is significantly more costly in time and effort.</p><p>We propose to follow up on <ref type="bibr">DCASE 2018 Task 4 [6]</ref> and investigate the scenario where large-scale SED systems can exploit the availability of a small set of weakly annotated data, a larger set of unlabeled data and an additional training set of synthetic soundscapes with strong labels. Given these data, the goal of this task is to train SED models that output event detections with time boundaries (i.e. strong predictions) in domestic environments. That is, a system has to detect the presence of a sound event as well as predict the onset and offset times of each occurrence of the event. We generate strongly annotated synthetic soundscapes using the <software ContextAttributes="used">Scaper</software> library <ref type="bibr">[13]</ref>. Given a set of user-specified background and foreground sound event recordings, <software ContextAttributes="used">Scaper</software> automatically generates soundscapes containing random mixtures of the provided events sampled from user-defined distributions. These distributions are defined via a sound event specification including properties such as event duration, onset time, signal-to-noise ratio (SNR) with respect to the background and data augmentation (pitch shifting and time stretching). This allows us to generate multiple different soundscape instantiations from the same specification which is set based on our general requirements for the soundscapes. Since generating such strongly labeled synthetic data is feasible on large scale, we provide a strongly labeled synthetic dataset in order to explore if it can help improving SED models. We believe insights learned from this task will be beneficial to the community as such an exploration is novel and will provide a pathway to developing scalable SED systems.</p><p>The remainder of this manuscript is organized as follows. Section 2 provides a brief overview of the task definition and how the development and evaluation datasets were created. Section 3 describes the baseline system and the evaluation procedure for Task 4. Section 4 gives an overview of the systems submitted to the challenge for this task. Finally, conclusions from the challenge are provided in section 5. </p></div>
<div><head n="2.">TASK DESCRIPTION AND DESED DATASET</head></div>
<div><head n="2.1.">Task description</head><p>This task is the follow-up to <ref type="bibr">DCASE 2018 Task 4 [6]</ref>. Systems are expected to produce strongly labeled output (i.e. detect sound events with a start time, end time, and sound class label), but are provided with weakly labeled data (i.e. sound recordings with only the presence/absence of a sound included in the labels without any timing information) for training. Multiple events can be present in each audio recording, including overlapping events. As in the previous iteration of this task, the challenge entails exploiting a large amount of unbalanced and unlabeled training data together with a small weakly annotated training set to improve system performance. However, unlike last year, in this iteration of the challenge we also provide an additional training set with strongly annotated synthetic soundscapes. This opens the door to exploring scientific questions around the informativeness of real (but weakly labeled) data versus strongly-labeled synthetic data, whether the two data sources are complementary or not, and how to best leverage these datasets to optimize system performance.</p></div>
<div><head n="2.2.">DESED development dataset</head><p>The Freesound Dataset (FSD) <ref type="bibr">[15,</ref><ref type="bibr">16]</ref>. Each sound event clip was verified by a human to ensure that the sound quality and the eventto-background ratio were sufficient to be used as an isolated sound event. We also controlled if the sound event onset and offset were present in the clip. Each selected clip was then segmented when needed to remove silences before and after the sound event and between sound events when the file contained multiple occurrences of the sound event class. The number of unique isolated sound events per class used to generate the subset of synthetic soundscapes is presented in Table <ref type="table" target="#tab_0">1</ref>. It also presents the number of clips containing a class and the number of events per class.</p><p>The background textures are obtained from the SINS dataset (activity class "other") <ref type="bibr">[17]</ref>. This particular activity class was selected because it contains a low amount of sound events from the 10 target foreground sound event classes. However, there is no guarantee that these sound event classes are completely absent from the background clips. A total of 2060 unique background clips are used to generate the synthetic subset.</p><p><software>Scaper</software> <software ContextAttributes="used">scripts</software> are designed such that the distribution of sound events per class, the number of sound events per clip (depending on the class) and the sound event class co-occurrence are similar to that of the validation set which is composed of real recordings. The synthetic soundscapes are annotated with strong labels automatically generated by <software ContextAttributes="used">Scaper</software> [13].</p></div>
<div><head n="2.3.">DESED evaluation dataset</head><p>The evaluation dataset is composed of two subsets: a subset with real recording and a subset with synthetic soundscapes.</p></div>
<div><head n="2.3.1.">Real recordings</head><p>The first subset is comprised of audio clips extracted from YouTube and Vimeo videos under creative common licenses. This subset contains 1,013 audio clips and is used for ranking purposes.</p></div>
<div><head n="2.3.2.">Synthetic soundscapes</head><p>The second subset is comprised of synthetic soundscapes generated with <software ContextAttributes="used">Scaper</software><ref type="foot" target="#foot_0">1</ref> . This subset is used for analysis purposes and its design is motivated by the analysis of last year's results <ref type="bibr">[10]</ref>. In particular, most submission from last year were perform badly in terms  <ref type="table" target="#tab_1">2</ref>.</p><p>Varying foreground-to-background SNR: A subset (denoted Synthetic set 1) of 754 soundscapes is generated with a sound event distribution similar to that of the training set. Four versions of this subset are generated varying the value of the foreground events' SNR with respect to the background: 0 dB, 6 dB, 15 dB and 30 dB.</p><p>Audio degradation: Six alternative versions of the previous subset (with SNR=0 dB) are generated introducing artificial degradation with the <software ContextAttributes="used">Audio Degradation Toolbox</software> <ref type="bibr">[20]</ref>. The following degradations are used (with default parameters) : "smartPhonePlayback", "smartPhoneRecording", "unit applyClippingAlternative", "unit applyDynamicRangeCompression", "unit applyHighpassFilter" and "unit applyLowpassFilter".</p><p>Varying onset time: A subset of 750 soundscapes is generated with uniform sound event onset distribution and only one event per soundscape. The sound event SNR parameter is set to 0 dB. Three variants of this subset are generated with the same isolated events, only shifted in time. In the first version, all sound events have an onset located between 250 ms and 750 ms, in the second version the sound event onsets are located between 4.75 s and 5.25 s and in the last version the sound event onsets are located between 9.25 s and 9.75 s.</p><p>Long sound events vs. short sound events: A subset with 522 soundscapes is generated where the background is selected from one of the five long sound event classes (<software>Blender</software>, Electric shaver/toothbrush, Frying, Running water and Vacuum cleaner). The foreground sound events are selected from the five short sound event classes (Alarm/bell/ringing, Cat, Dishes, Dog and Speech). Three variants of this subset are generated with similar sound event <software ContextAttributes="used">scripts</software> and varying values of the sound event SNR parameter (0 dB, 15 dB and 30 dB).</p></div>
<div><head n="3.">BASELINE</head><p>The baseline system is inspired by the winning system from DCASE 2018 Task 4 by Lu [21]<ref type="foot" target="#foot_1">2</ref> . It uses a mean-teacher model which is a combination of two models: a student model and a teacher model (both have the same architecture). Our implementation of the mean-teacher model is based on the work of Tarvainen and Valpola <ref type="bibr">[22]</ref>. The student model is the final model used at inference time, while the teacher model is aimed at helping the student model during training and its weights are an exponential moving average of the student model's weights. A depiction of the baseline model is provided in Figure <ref type="figure">1</ref>.</p><p>The models are a combination of a convolutional neural network (CNN) and a recurrent neural network (RNN) followed by an aggregation layer (in our case an attention layer). The output of the RNN gives strong predictions (the weights of this model are denoted θs) while the output of the aggregation layer gives the weak predictions (the weights of this model are denoted θ).</p><p>The student model is trained on the synthetic and weakly labeled data. The loss (binary cross entropy) is computed at the frame level for the strongly labeled synthetic data and at the clip level for the weakly labeled data. The teacher model is not trained, rather, its weights are a moving average of the student model (at each epoch).</p><p>During training, the teacher model receives the same input as the student model but with added Gaussian noise, and helps train the student model via a consistency loss (mean-squared error) for both strong (frame-level) and weak predictions. Every batch contains a combination of unlabeled, weakly and strongly labeled samples.</p><p>This results in four loss components: two for classification (weak and strong) and two for consistency (weak and strong), which are combined as follows: Submissions were evaluated according to an event-based F1-score with a 200 ms collar on the onsets and a collar on the offsets that is the greater of 200 ms and 20% of the sound event's length. The overall F1-score is the unweighted average of the class-wise F1scores (macro-average). In addition, we provide the segment-based F1-score on 1 s segments as a secondary measure. The metrics are computed using the <software ContextAttributes="used">sed eval</software> library <ref type="bibr">[23]</ref>.</p><formula xml:id="formula_0">L(θ) =L classw (θ) + σ(λ)Lcons w (θ) + L classs (θs) + σ(λ)Lcons s (θs)<label>(1</label></formula></div>
<div><head n="4.2.">System performance</head><p>The official team ranking (best system from each team) along with some characteristics of the submitted systems is presented in Table <ref type="table">3</ref>. Submissions are ranked according to the event-based F1score computed over the real recordings in the evaluation set. For a more detailed comparison, we also provide the event-based F1score on the YouTube and Vimeo subsets and the segment-based F1-score over all real recordings. The event-based F1-score on the validation set is reported for the sake of comparison with last year's results (75% of the 2019 validation is comprised of the 2018 evaluation set). The performance on synthetic recordings is not taken into account in the ranking, but the event-based F1-score on Synthetic set 1 (0 dB) is presented here as well.</p><p>Twelve teams outperform the baseline with the best systems <ref type="bibr">[24,</ref><ref type="bibr">25,</ref><ref type="bibr">26]</ref> outperforming the baseline by 16% points and the best system from 2018 by over 10 % points. While the ranking on the YouTube subset is similar to the official ranking, there rankings based on the Vimeo and synthetic subsets are notably different. Performance on the Vimeo set is in general considerably lower than on the YouTube set and Synthethic set 1. The fact that no data from Vimeo was used during training (unlike data from YouTube and synthetic data) suggests that the submitted systems struggle to generalize to an entirely unseen set of recording conditions.</p><p>All three top performing teams used a semi-supervised meanteacher model <ref type="bibr">[22]</ref>. Lin and Wang <ref type="bibr">[24]</ref> focused on the importance of semi-supervised learning with a guided learning setup [27] and on how synthetic data can help within this setup when used together with a sufficient amount of real data. Delphin-Poulat and Plapous <ref type="bibr">[25]</ref> focused on data augmentation and Shi <ref type="bibr">[26]</ref> focused on a specific type of data augmentation where both audio files and their labels are mixed. <ref type="bibr">Cances et al. [28]</ref> proposed a multi-task learning setup where audio tagging (producing weak predictions) and the sound event localization in time (strong predictions) are treated as two separate subtasks <ref type="bibr">[29]</ref>. The latter was also the least complex of the-performing systems.</p><p>Most of the top-performing systems also demonstrate the importance of employing class dependent post-processing <ref type="bibr">[24,</ref><ref type="bibr">25,</ref><ref type="bibr">28]</ref>, which improves performance significantly compared to e.g. using a fixed median filtering approach. This highlights the benefits of applying dedicated segmentation post-processing <ref type="bibr">[28,</ref><ref type="bibr">30]</ref>.</p></div>
<div><head n="5.">CONCLUSION</head><p>This paper presents DCASE 2019 Task 4 and the DESED dataset, which focus on SED in domestic environments. The goal of the task is to exploit a small dataset of weakly labeled sound clips together with a larger unlabeled dataset to perform SED. An additional training dataset composed of synthetic soundscapes with strong labels is provided in order to explore the gains achievable with simulated data. The best submissions from this year outperform last year's winning submission by over 10 % points, representing a notable advancement. Evaluation on different subsets, and in particular the Vimeo subset, suggests there is still a significant challenge in generalizing to unseen recording conditions.</p></div>
<div><head n="6.">ACKNOWLEDGMENT</head><p>The authors would like to thank the Hamid Eghbal-Zadeh from Johannes Kepler University (Austria) who participated to the initial discussions about this task as well as all participants to the task.</p></div><figure type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Class-wise statistics for the synthetic development subset.</figDesc><table><row><cell>Class</cell><cell>Unique events</cell><cell cols="2">Dev set Clips Events</cell></row><row><cell>Alarm/bell/ringing</cell><cell>190</cell><cell>392</cell><cell>755</cell></row><row><cell>Blender</cell><cell>98</cell><cell>436</cell><cell>540</cell></row><row><cell>Cat</cell><cell>88</cell><cell>274</cell><cell>547</cell></row><row><cell>Dishes</cell><cell>109</cell><cell>444</cell><cell>814</cell></row><row><cell>Dog</cell><cell>136</cell><cell>319</cell><cell>516</cell></row><row><cell>Electric shaver/toothbrush</cell><cell>56</cell><cell>221</cell><cell>230</cell></row><row><cell>Frying</cell><cell>64</cell><cell>130</cell><cell>137</cell></row><row><cell>Running water</cell><cell>68</cell><cell>143</cell><cell>157</cell></row><row><cell>Speech</cell><cell>128</cell><cell>1272</cell><cell>2132</cell></row><row><cell>Vacuum cleaner</cell><cell>74</cell><cell>196</cell><cell>204</cell></row><row><cell>Total</cell><cell>1011</cell><cell>2045</cell><cell>6032</cell></row></table></figure>
<figure type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Class-wise statistics for the synthetic evaluation subsets</figDesc><table><row><cell>as in</cell></row></table><note><p>development (training) part of DESED dataset is composed of 10-sec audio clips recorded in domestic environment or synthesized to simulate a domestic environment. The task focuses on the same 10 classes of sound events used in Task 4 ofDCASE 2018 [6]. The DESED dataset is comprised of a subset of real recordings taken from AudioSet [14] and a subset of synthetic soundscapes generated using Scaper. The subset of real recordings is the same set of foreground sounds and a set of background sounds automatically sequencing them into random soundscapes sampled from a user-specified distribution controlling the number and type of sound events, their duration, signal-to-noise ratio, and several other key characteristics. The foreground events are obtained from the</p></note></figure>
<figure type="table" xml:id="tab_2"><head /><label /><figDesc>Mean-teacher model. η and η represent noise applied to the different models (in this case dropout).</figDesc><table><row><cell /><cell /><cell>Student</cell><cell /><cell>Teacher</cell></row><row><cell>Weakly labeled data</cell><cell /><cell>Weak classification cost</cell><cell>Weak consistency cost</cell></row><row><cell /><cell /><cell>Aggregation</cell><cell /><cell>Aggregation</cell></row><row><cell /><cell /><cell>(max, attention...)</cell><cell /><cell>(max, attention...)</cell></row><row><cell>Strong labeled data</cell><cell>Cat Speech ...</cell><cell>Strong classification cost</cell><cell>Strong consistency cost</cell></row><row><cell /><cell /><cell /><cell>′</cell><cell>′</cell></row><row><cell>Unlabeled data</cell><cell /><cell /><cell>Exponential moving average</cell></row><row><cell /><cell /><cell /><cell>+</cell><cell>with</cell><cell>∼  (0, 0.5)</cell></row><row><cell /><cell>Figure 1:</cell><cell /><cell /></row></table><note><p><p><p><p><p>of segmentation. One of the goal of this subset is to analyze to which extent strongly labeled data in the training set helped refining the segmentation. The foreground events are obtained from the FSD</p>[15, 16]</p>. The selection process was the same as described for the development dataset. Background sounds are extracted from YouTube videos under a Creative Common license and from the Freesound subset of the MUSAN dataset</p>[19]</p>. The synthetic subset is further divided into several subsets (described below) for a total of 12,139 audio clips synthesized from 314 isolated events. The isolated sound event distribution per class is presented in Table</p></note></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>The JAMS[18]  annotation files corresponding to these soundscapes will be released on:</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>The <software>code</software> for the baseline model is open source and available on: https://github.com/turpaultn/DCASE2019_task4/tree/ public/baseline</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl />
			</div>
		</back>
	</text>
</TEI>