<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compressive Learning with Privacy Guarantees</title>
				<funder ref="#_gXNWUW5">
					<orgName type="full">FNRS</orgName>
				</funder>
				<funder>
					<orgName type="full">"Fonds de la Recherche Scientifique" (F.R.S. -FNRS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
				<date type="published" when="2021-01-20">January 20, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">A</forename><surname>Chatalic</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IRISA (</orgName>
								<orgName type="institution" key="instit1">Univ. Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">V</forename><surname>Schellekens</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ICTEAM/ELEN</orgName>
								<orgName type="institution">UCLouvain (</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IRISA (</orgName>
								<orgName type="institution" key="instit1">Univ. Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ICTEAM/ELEN</orgName>
								<orgName type="institution">UCLouvain (</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Univ Lyon</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">ENS de Lyon</orgName>
								<orgName type="institution" key="instit5">UCB Lyon 1</orgName>
								<orgName type="institution" key="instit6">LIP</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Compressive Learning with Privacy Guarantees</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-20">January 20, 2021</date>
						</imprint>
					</monogr>
					<idno type="MD5">DAFAD8D8A3DC0F417CF57056C4878EDA</idno>
					<idno type="DOI">10.1093/imaiai/iaab005</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-07T09:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div><p>This work addresses the problem of learning from large collections of data with privacy guarantees. The compressive learning framework proposes to deal with the large scale of datasets by compressing them into a single vector of generalized random moments, called a sketch vector, from which the learning task is then performed. We provide sharp bounds on the so-called sensitivity of this sketching mechanism. This allows to leverage standard techniques to ensure differential privacy -a well established formalism for defining and quantifying the privacy of a random mechanism -by adding Laplace of Gaussian noise to the sketch. We combine these standard mechanisms with a new feature subsampling mechanism, which reduces the computational cost without damaging privacy. The overall framework is applied to the tasks of Gaussian modeling, k-means clustering and principal component analysis (PCA), for which sharp privacy bounds are derived. Empirically, the quality (for subsequent learning) of the compressed representation produced by our mechanism is strongly related with the induced noise level, for which we give analytical expressions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1">Introduction</head><p>The size and availability of datasets has increased dramatically in the last few decades, leading to tremendous breakthroughs in machine learning and artificial intelligence. However, the large volume and level of detail of these data present two key challenges. Firstly, the sheer size of datasets calls for new machine learning methods able to process them efficiently, both in time and memory. Secondly, the data collected is often of a sensitive nature, and using it to learn publicly released models raises serious privacy concerns, creating a need for algorithms that guarantee the privacy of the dataset contributors.</p><p>Compressive learning <ref type="bibr" target="#b27">[27]</ref> has been proposed as an answer to the first challenge. In the compressive learning framework, the dataset is compressed into a sketch, a vector of generalized random moments <ref type="bibr" target="#b15">[15]</ref> -obtained by averaging over the dataset certain random (nonlinear) features of the records <ref type="bibr" target="#b44">[44]</ref> -whose size is independent from the number of records. The learning step can then be performed from this sketch only, using greatly reduced computational resources (see Figure <ref type="figure">1</ref>). Once the sketch is computed, the dataset can be discarded. As opposed to many machine learning algorithms, compressive learning does not need the data to be stored in one place nor to be accessed multiple times; computing the sketch can be done in one pass over the data or from a data stream, has a low memory footprint, and is embarrassingly parallelizable. As the size of the sketch does not depend on the size of the dataset, but rather on the amount of information we want to extract from its underlying distribution, learning from this vector has a computational cost which is independent of the initial dataset size.</p><p>As compressive learning requires only aggregate information from many individual records, it is intuitively a good candidate to answer the second challenge of privacy preservation. Differential privacy (DP) <ref type="bibr" target="#b19">[19]</ref> was proposed by Dwork et al. as a formal privacy definition, that intuitively requires the output of an algorithm to not depend too much on the presence of any record in the dataset. It has many powerful properties, and has been shown to be robust to many attacks, which has made it widely accepted by the scientific community as a standard definition of privacy. It has further received a lot of attention in the industry <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b23">23]</ref>.</p><p>A standard approach to ensure differential privacy is to add noise (typically Laplace or Gaussian) to the output of the mechanism one wishes to make private. The privacy level of the resulting randomized mechanism is then known to be determined by the so-called sensitivity of the initial mechanism, which we assume to be deterministic in the following. This is the approach conducted in this paper <ref type="foot" target="#foot_0">1</ref> , leading to a generic mechanism relying on noise addition that produces differentially private versions of the sketch, which is applied to the tasks of k-means clustering, Gaussian mixture modeling and principal component analysis (PCA). This differentially private sketch is a private representation of the dataset, which can be used -possibly multiple times and for different purposes -without infringing the privacy of any user in the dataset. By sharply characterizing the sensitivity of the sketching mechanism, we obtain sharp privacy guarantees for the resulting mechanism.</p><p>Subsampling the dataset is another common practice to enhance privacy <ref type="bibr" target="#b7">[7]</ref>. Although it does not allow to grant privacy alone, it is known to amplify privacy of any existing differentially-private mechanism <ref type="bibr" target="#b4">[4]</ref>. We introduce a simple feature subsampling mechanism, which differs from more standard data subsampling mechanisms, so that each data sample only contributes to some of the entries of the sketch, in order to reduce the computational cost of sketching. Privacy guarantees are also established for this mechanism through appropriately modified measures of sentitivity. Subsampling allows to reduce drastically the computational complexity, and can be performed in some settings without degrading the quality of the sketch for subsequent learning.</p><p>Finally, as privacy naturally has to be traded off for utility, we show empirically for the k-means clustering task that the utility of a noisy sketch is driven -provided the sketch dimension exceeds some task-dependent threshold -by a signal-to-noise ratio, which provides guidelines to parameter tuning of the algorithms. The obtained framework has thus a good balance between computational efficiency, privacy preservation and quality of the learned model.</p></div>
<div><head>Summary of contributions</head><p>The contributions of this paper are as follows:</p><p>â€¢ We build on existing compressive learning and differential privacy techniques to define a noisy sketching mechanism which exploits nonlinear random features. â€¢ We derive sharp sensitivity estimates for this mechanism, leading via standard tools to sharp differential privacy guarantees for sketches designed to handle three unsupervised learning tasks: k-means clustering, Gaussian mixture modeling and principal components analysis. â€¢ We extend our framework to subsampled sketches, giving the same privacy guarantees for a lower computational cost. â€¢ We show that the utility of a noisy sketch, i.e. its quality for subsequent learning, can be measured by a signal-to-noise ratio, and use this quantity for tuning some parameters.</p></div>
<div><head>Related Work</head><p>We focus on the three learning tasks considered in this paper: Gaussian modeling (GMM), PCA and k-means clustering. The two latter have already received a lot of attention in the differential privacy literature, while the former has been less studied. Addition of noise is the most common way to achieve differential privacy, whether it is on the intermediate steps of an iterative algorithm or directly on the output. Private variants of standard iterative methods include DPLloyd for k-means <ref type="bibr" target="#b8">[8]</ref>, and variants with improved convergence guarantees <ref type="bibr" target="#b38">[38]</ref>. The popular k-means++ seeding method has also been generalized to a private framework <ref type="bibr" target="#b41">[41]</ref>. For Gaussian modeling, DP-GMM <ref type="bibr" target="#b59">[59]</ref> and DP-EM <ref type="bibr" target="#b42">[42]</ref> have been proposed. Note that for iterative algorithms, the privacy budget needs to be split between iterations, de facto limiting the total number of iterates, which becomes a hyper-parameter. Our approach does not suffer from this drawback since the sketch is released at once. Moreover, the same sketch can be used to run the learning algorithm multiple times with e.g. different initializations.</p><p>Releasing a private synopsis of the data (similarly to our sketch) rather than directly a noisy solution has already been studied as well. EUGkM <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b48">48]</ref> suggests for instance to use noisy histograms for clustering (but this method is by nature limited to small dimensions), and private coresets have been investigated by Feldman et al. <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25]</ref>. For PCA, noise can be added directly on the covariance matrix <ref type="bibr" target="#b22">[22]</ref>.</p><p>The exponential mechanism is another standard noise-additive approach for privacy. A random perturbation is drawn according to a distribution calibrated using a user-defined quality measure, and added to the output. It has been used with success for PCA, perturbing either the covariance <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b29">29]</ref> or directly the eigenvectors of the covariance <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b0">1]</ref>, and with genetic algorithms for k-means <ref type="bibr" target="#b60">[60]</ref>. Such algorithms depend strongly on the quality measure of the output, which must be chosen carefully. Our sketch-based approach is in contrast more generic: the same sketch allows to solve different tasks such as clustering and GMM fitting, and it can easily be extended to new sketches in the future. Alternatively, our mechanism can be seen as a straightforward instantiation of the exponential mechanism, where the output (the sketch) is carefully designed so that is makes sense to simply use the ğ¿ 1 or ğ¿ 2 norms as quality measures.</p><p>Our sketching mechanism makes use of random projections, which have proven to be very useful to solve efficiently large-scale problems, and induce as well a controlled loss of information which can be leveraged to derive privacy guarantees <ref type="bibr" target="#b9">[9]</ref>. Balcan et al. investigated the large-scale high-dimensional clustering setting with an approach based on Johnson-Lindenstrauss dimensionality reduction <ref type="bibr" target="#b3">[3]</ref>. Many other embeddings based on random projections have been proposed, see e.g. <ref type="bibr" target="#b32">[32]</ref>. Linear compression of the number of samples (rather than reducing the dimension) has been considered <ref type="bibr" target="#b61">[61]</ref> but is less scalable. Random algorithms have also been used for PCA and, more generally, for low-rank factorization <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b1">2]</ref>. Note however that as explained in the next section, the features resulting from the random projection undergo in our setting a nonlinear transformation, in the spirit of random features <ref type="bibr" target="#b44">[44]</ref>, and are averaged; they thus differ a lot from what is done in these works, although they share this common idea.</p><p>Private k-means clustering algorithms based on the minimum enclosing ball problem have also been proposed <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b47">47]</ref>. Yet, it is not clear how such methods compare in practice to the numerous other candidates.</p><p>Private empirical risk minimization <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b56">56]</ref> has emerged as a generic way to design private learning algorithms, but it relies on specific assumptions (e.g. convexity, which does not hold for PCA, GMM modeling and kmeans) on the loss function which defines the learning task, and still relies on multiple passes over the whole dataset.</p><p>Closer to our work, Balog et al. <ref type="bibr" target="#b6">[6]</ref> recently proposed to release kernel mean embeddings, either as sets of synthetic data points in the input space or using feature maps, similarly to our method. However, to the best of our knowledge, the impact of privacy on the quality of learning in such methods has not been studied in the literature.</p></div>
<div><head>Paper outline and reading guide</head><p>The main existing tools and concepts from compressive learning and differential privacy are respectively recalled in sections 2 and 3. The reader knowledgeable with tools from either of these fields can probably safely skip the corresponding sections except to get familiar with the chosen notations. These tools are combined in Section 4 where we generically characterize the sensitivity of the sketching mechanism and provide new explicit expressions of this sensitivity for particular feature maps. Section 5 is devoted to describing the proposed feature subsampling mechanism and characterizing its privacy level, and Section 6 gives evidence of the relevance of a noise-to-signal ratio as a proxy for utility, before exploring its use to provide guidelines to tune mechanisms.</p></div>
<div><head n="2">Statistical Learning using Compressive Methods</head><p>Throughout the paper, ğ‘‘ always refers to the dimension of the data samples. We denote D ğ‘› â‰œ ğ¸ ğ‘› the set of (ordered) collections of ğ‘› learning examples in a domain ğ¸, and D â‰œ âˆª ğ‘›âˆŠN D ğ‘› . Unless otherwise specified, we will typically consider ğ¸ = R ğ‘‘ . The number of elements in a collection X is denoted |X |. Note that we work with ordered datasets for technical reasons, but this order does not matter from a learning perspective.</p><p>Essentially, machine learning aims at inferring the parameters ğ›‰ âˆŠ H of a mathematical model from a collection X = (ğ± 1 , â€¦, ğ± ğ‘› ) of training samples in R ğ‘‘ drawn from a probability distribution ğœ‹ 0 , i.e. ğ± ğ‘– iid âˆ¼ ğœ‹ 0 . In statistical learning, a task is defined by a loss function ğ‘™ âˆ¶ (ğ±, ğ›‰) â†¦ ğ‘™(ğ±, ğ›‰) âˆŠ R which measures the relevance of the parameter ğ›‰ with respect to ğ± for the task, and the associated risk function R(ğœ‹, ğ›‰) = E ğ±âˆ¼ğœ‹ ğ‘™(ğ±, ğ›‰) which extends this loss to distributions (and thus depends on the chosen loss, although we do not reflect this in the notation for conciseness). Intuitively, R(ğœ‹, ğ›‰) characterizes how ğ›‰ is suited to solve the learning task for distribution ğœ‹. Learning thus amounts to finding ğ›‰ * âˆŠ arg min ğ›‰âˆŠH R(ğœ‹ 0 , ğ›‰), but since the true distribution ğœ‹ 0 is unknown in practical applications, one typically uses the empirical distribution ğœ‹ X = 1 ğ‘› âˆ‘ ğ‘› ğ‘–=1 ğ›¿ ğ± ğ‘– associated to a dataset X and looks for Î¸ âˆŠ arg min ğ›‰âˆŠH R(ğœ‹ X , ğ›‰); this is known as empirical risk minimization.</p><p>Standard approaches for empirical risk minimization access each data sample multiple times, and hence require them to be stored for the whole runtime of the algorithm. Compressive learning was introduced as a machine learning method that bypasses these needs by learning from a heavily compressed summary of the dataset, called the sketch, instead of the full dataset. In our context, this sketch is defined as the sample average of a feature map ğš½, as depicted in Figure <ref type="figure">1</ref>.</p><formula xml:id="formula_0">Definition 1 (Sketch). The sketch ğ³ X of a dataset X = (ğ± 1 , â€¦ , ğ± ğ‘› ) âˆŠ D associated with the feature map ğš½ is defined as ğ³ X â‰œ 1 ğ‘› ğ‘› âˆ‘ ğ‘–=1 ğš½(ğ± ğ‘– ), where ğš½ âˆ¶ R ğ‘‘ â†’ C ğ‘š or R ğ‘š .<label>(1)</label></formula><p>The choice of the feature map depends on the learning task to solve, however ğš½ will typically be a nonlinear function, in order to capture more information from X than the first order moments. In this paper, we consider non-linear functions of the form ğš½(ğ±) = ğ‘“(Î© ğ‘‡ ğ±), where Î© âˆˆ R ğ‘‘Ã—ğ‘š is a randomly generated (but fixed) matrix and the nonlinear ğ‘“ is applied pointwise. Hence, the sketch is a collection of generalized random moments of the empirical distribution, i.e. ğ³ X = E ğ±âˆ¼ğœ‹ X ğš½(ğ±). Note that although the feature map is a nonlinear function, sketching is a linear operation w.r.t. distributions. Estimating the desired model parameters from the sketch is done by solving a problem of the form Î¸ âˆˆ arg min ğ›‰âˆˆH S(ğ³ X , ğ›‰), where S(ğ³ X , â€¢) is a surrogate for R(ğœ‹ X , â€¢). This framework bears similarities with compressive sensing <ref type="bibr" target="#b26">[26]</ref>, which investigates the possibility of recovering signals from a small number of linear measurements, provided that these signals belong to (or can be well approximated by) a low-dimensional model. In this sense, learning from the sketch can be seen as an inverse problem on probability distributions: one will try to recover, in a model adapted to the learning task, a distribution whose sketch (i.e. linear observations) matches the moments computed on the dataset.</p><p>In sections 2.1 and 2.2, we formally define the three learning tasks we are interested in -namely clustering, density fitting and PCA -, and explain how they can be solved using a compressive approach.</p></div>
<div><head n="2.1">Sketching with Fourier Features for Clustering and Density Fitting</head><p>We first define the tasks of unsupervised ğ‘˜-means clustering and Gaussian mixture modeling, as these can both be answered with the same feature map.</p></div>
<div><head>Definition 2 (k-means clustering task). Given an integer ğ‘˜ &gt; 0, k-means clustering consists in finding centroids</head><formula xml:id="formula_1">ğ¶ = {ğœ 1 , â€¦ , ğœ ğ‘˜ } âŠ† R ğ‘‘ minimizing the empirical risk R KM associated to the loss function ğ‘™ KM (ğ±, ğœ) â‰œ min 1â‰¤ğ‘—â‰¤ğ‘˜ â€–ğ± -ğœ ğ‘— â€– 2 2 .</formula><p>In this specific case, the empirical risk (computed on the empirical distribution ğœ‹ X of the dataset X = (ğ± 1 , â€¦, ğ± ğ‘› )) is also called the sum of squared errors (SSE):</p><formula xml:id="formula_2">R KM (ğœ‹ X , ğ¶) = SSE(X , ğ¶) â‰œ 1 ğ‘› ğ‘› âˆ‘ ğ‘–=1 min 1â‰¤ğ‘—â‰¤ğ‘˜ â€–ğ± ğ‘– -ğœ ğ‘— â€– 2 2 .<label>(2)</label></formula><p>For Gaussian modeling, the empirical risk is the log-likelihood of the mixture of Gaussians that the model fits to the data. Definition 3 (Gaussian mixture modeling task). Given an integer ğ‘˜ &gt; 0, Gaussian mixture modeling consists in finding the parameters (weights</p><formula xml:id="formula_3">ğ›¼ 1 , â€¦, ğ›¼ ğ‘˜ âˆŠ R + s.t. âˆ‘ 1â‰¤ğ‘–â‰¤ğ‘˜ ğ›¼ ğ‘– = 1, locations ğ› 1 , â€¦ , ğ› ğ‘˜ âˆˆ R ğ‘‘ and covariances Î£ 1 , â€¦ , Î£ ğ‘˜ âˆŠ R ğ‘‘Ã—ğ‘‘ ) of a Gaussian mixture ğ‘€ whose p.d.f. ğ‘ ğ‘€ (ğ±) â‰œ âˆ‘ ğ‘˜</formula><p>ğ‘–=1 ğ›¼ ğ‘– N (ğ±; ğ› ğ‘– , Î£ ğ‘– ) maximizes the log-likelihood, i.e. minimizes the empirical risk R GMM associated to the loss function ğ‘™ GMM (ğ±, ğ‘€ ) â‰œ -ln ğ‘ ğ‘€ (ğ±).</p><p>To solve clustering and density modeling tasks in a compressive manner, previous works focused on random Fourier features (RFF), which consist in using the complex exponential as the nonlinear function <ref type="bibr" target="#b44">[44]</ref> in the feature map. This has been applied to clustering and fitting parametric mixture models, such as Gaussian mixture models <ref type="bibr" target="#b33">[33]</ref> or alpha-stable distributions <ref type="bibr" target="#b35">[35]</ref>.</p><p>Formally, for a given matrix of frequencies Î© = [ğ›š 1 , â€¦ , ğ›š ğ‘š ], the random Fourier feature map is defined by ğš½ RFF (ğ±) â‰œ exp (iÎ© ğ‘‡ ğ±) âˆˆ C ğ‘š (i.e., we chose the complex exponential nonlinearity ğ‘“(â‹…) = exp(iâ‹…)). The frequency vectors are typically i.i.d. Gaussians, i.e. ğ›š ğ‘– âˆ¼ N (0, ğœ 2 ğ¼ ğ‘‘ ), where the variance parameter ğœ 2 must be adapted to the data, e.g. using prior knowledge on the distribution. The choice of the frequency distribution indeed defines implicitly in the sample space a kernel function <ref type="bibr" target="#b27">[27]</ref>, which can be interpreted as an inner product whose scale must be coherent with the scale of the clusters to identify.</p><p>Once the empirical sketch ğ³ X of a dataset has been computed, k-means clustering (or GMM fitting) can be performed by solving a linear inverse problem: one wants to recover an "ideal" distribution, i.e. belonging to a meaningful mathematical model P, whose moments are as close as possible to the moments ğ³ X measured on the dataset. An intuitive and sound choice for the model P is the set of mixtures of ğ‘˜ diracs <ref type="bibr" target="#b36">[36]</ref> for clustering, or the set of mixtures of ğ‘˜ normal distributions <ref type="bibr" target="#b34">[34]</ref> for GMM fitting. Writing the elements of P as mixtures of ğ‘˜ simple parametric probability distributions, we then solve</p><formula xml:id="formula_4">ğœˆ * âˆˆ arg min ğœˆâˆˆP â€–E ğ±âˆ¼ğœˆ ğš½(ğ±) -ğ³ X â€– 2 ,<label>(3)</label></formula><p>which is a parametric optimization problem acting as a surrogate for risk minimization as explained at the beginning of the section. For clustering, the locations of the ğ‘˜ diracs forming ğœˆ * define the estimated centroids, while for GMMs ğœˆ * itself is the estimated mixture density.</p><p>Quantized Sketches Note that the expression of random Fourier features can be rewritten ğš½ RFF (ğ±) = (cos(Î© ğ‘‡ ğ±) + i cos(Î© ğ‘‡ ğ± -ğœ‹ 2 )). It was shown that the cosine in this expression can be replaced by any other nonlinear, periodic function ğœŒ, while preserving the properties of the sketch. Specifically, if a uniform, random dithering ğ® âˆˆ [0, 2ğœ‹[ ğ‘š is added before the nonlinearity, i.e. ğ‘“(â‹…) = ğœŒ(â‹… + ğ®) + iğœŒ(â‹… + ğ® -ğœ‹ 2 ) with ğ‘¢ ğ‘— iid âˆ¼ U([0, 2ğœ‹[), the moment-fitting cost function defined in (3) can easily be adapted so that recovery can still be performed. In particular, a good approximation to the complex exponential moment fitting can be obtained using a quantized sketching variant <ref type="bibr" target="#b46">[46]</ref>, i.e. quantizing the cosine of both real and imaginary parts with Â±1 (one-bit quantization). Although the average sketch will in the end belong to C ğ‘š , the mechanism produces individual sketches ğš½ RFF (ğ± ğ‘– ) in ({-1, 1} + i{-1, 1}) ğ‘š , and thus has a reduced memory footprint (2ğ‘š bits); this is especially convenient if these individual sketches have to be computed on low-power and low-memory devices, or to be sent over a network before being averaged.</p><p>To account for all possibilities, we provide a unified definition of the random Fourier feature (RFF) map, covering both quantized and unquantized cases.</p></div>
<div><head>Definition 4 (Random Fourier features).</head><p>For Î© = [ğ›š 1 , â€¦ , ğ›š ğ‘š ], the random Fourier feature map is defined by</p><formula xml:id="formula_5">ğš½ RFF (ğ±) â‰œ [ğœŒ(Î© ğ‘‡ ğ± + ğ®) + iğœŒ(Î© ğ‘‡ ğ± + ğ® - ğœ‹ 2 )] âˆˆ C ğ‘š ,<label>(4)</label></formula><p>with the particular cases { ğ® = ğŸ and ğœŒ = cos for random Fourier features ğ® âˆŠ [0, 2ğœ‹[ ğ‘š and ğœŒ = 2 -1/2 sign âˆ˜ cos for quantized features.</p><p>Note that with the normalization used on ğœŒ, we have for any ğ‘— that |ğš½ RFF (ğ±) ğ‘— | = 1 for both quantized and nonquantized features.</p></div>
<div><head n="2.2">Sketching with Quadratic Features for Compressive PCA</head><p>Principal component analysis consists, for a given ğ‘˜ &lt; ğ‘‘, in finding a ğ‘˜-dimensional linear subspace that best fits the data. Such a subspace can be parametrized by a matrix ğ‘Š âˆŠ R ğ‘‘Ã—ğ‘˜ . Definition 5 (PCA task). Principal component analysis aims at finding ğ‘Š âˆŠ R ğ‘‘Ã—ğ‘˜ minimizing the empirical risk R PCA associated to the loss function ğ‘™ PCA (ğ±, ğ‘Š ) â‰œ â€–ğ± -ğ‘Š ğ‘Š ğ‘‡ ğ±â€– 2  2 . The matrix of second moments ğ¶ = 1 ğ‘› âˆ‘ 1â‰¤ğ‘–â‰¤ğ‘› ğ± ğ‘– ğ± ğ‘‡ ğ‘– , which can be seen as a sketch computed using the feature map ğš½(ğ±) = vec(ğ±ğ± ğ‘‡ ), is known to capture all the information needed to solve the PCA problem. In practical applications, only the first ğ‘˜ eigenvectors of ğ¶ are needed, and so the sketch can be further reduced using low-rank matrix recovery techniques. The following feature map is then used <ref type="bibr" target="#b27">[27]</ref>.</p><formula xml:id="formula_6">Definition 6 (Random quadratic features). Let Î© = [ğ›š 1 , â€¦ , ğ›š ğ‘š ] âˆŠ R ğ‘‘Ã—ğ‘š . Choosing the nonlinearity ğ‘“(â‹…) = (â‹…) 2 , the feature function used for PCA is ğš½ RQF (ğ±) â‰œ [(ğ›š ğ‘‡ 1 ğ±) 2 , â€¦ , (ğ›š ğ‘‡ ğ‘š ğ±) 2 ] ğ‘‡ .</formula><p>We will typically consider two different sampling schemes for Î©:</p><p>â€¢ Gaussian: the (ğ›š ğ‘– ) 1â‰¤ğ‘–â‰¤ğ‘š are drawn as ğ›š ğ‘– âˆ¼ N (0, ğ‘‘ -1 ğ¼ ğ‘‘ ). Note that with this variance, we have E ğ›šâˆ¼N (0,ğ‘‘ -1 ğ¼ ğ‘‘ ) â€–ğ›šâ€– 2 2 = 1 for coherence with the next sampling scheme. â€¢ Union of orthonormal bases: when ğ‘š/ğ‘‘ is an integer, we consider Î© = [ğµ 1 , â€¦, ğµ ğ‘š/ğ‘‘ ] where the (ğµ ğ‘– ) 1â‰¤ğ‘–â‰¤ğ‘š/ğ‘‘ are ğ‘‘Ã—ğ‘‘ blocs whose columns form orthonormal bases of R ğ‘‘ . This setup is useful for two reasons. First it makes it possible to use structured blocs ğµ ğ‘– for which the matrix-vector product can be computed efficiently using fast transforms, but it also yields sharp privacy guarantees, as will be discussed in Section 4.1.2. Note that after averaging this feature map over all data samples, we are left with rank-one measurements of the data covariance matrix, i.e.</p><formula xml:id="formula_7">ğ³ X = [ğ›š ğ‘‡ ğ‘– ğ¶ğ›š ğ‘– ] ğ‘‡ 1â‰¤ğ‘–â‰¤ğ‘š = [âŸ¨ğ›š ğ‘– ğ›š ğ‘‡ ğ‘– , ğ¶âŸ©] ğ‘‡ 1â‰¤ğ‘–â‰¤ğ‘š â‰œ M(ğ¶)</formula><p>, where M is a linear operator acting on matrices. Solving the PCA task is here again casted into a linear inverse problem. Indeed, one aims at recovering the first eigenvectors of ğ¶, which amounts to finding a low-rank approximation of ğ¶. It is well established in the literature that the feature function proposed above is suitable for this task <ref type="bibr" target="#b26">[26,</ref><ref type="bibr">Section 4.6]</ref>. The problem thus boils down to finding a low-rank approximation from the sketch <ref type="bibr" target="#b27">[27]</ref>:</p><formula xml:id="formula_8">Äˆ âˆŠ arg min Î£â‰¥0,rank(Î£)â‰¤ğ‘˜ â€–M(Î£) -ğ³ X â€–.<label>(5)</label></formula><p>This is a well studied problem which can be solved using e.g. nuclear norm relaxation <ref type="bibr" target="#b26">[26]</ref>. As discussed later in the manuscript, a Burer-Monteiro factorization <ref type="bibr" target="#b11">[11]</ref> can also be used, yielding an optimization problem which, despite being non convex, usually displays nice properties <ref type="bibr" target="#b55">[55]</ref> and incurs a smaller memory cost than the convex nuclear norm formulation.</p></div>
<div><head n="3">Differential Privacy</head><p>Publishing quantities computed from a collection of people's records -e.g. a machine learning model or aggregate statistics -can compromise the privacy of these users, even when these quantities result from aggregation over millions of data providers <ref type="bibr" target="#b17">[17]</ref>. Differential Privacy (DP) was proposed as a strong privacy definition by Dwork et al. <ref type="bibr" target="#b19">[19]</ref>, and has since been studied and used extensively in research and industry <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b50">50]</ref>. We here give a brief introduction to DP, and also detail the assumptions made on the attacker (the attack model), which have a direct impact on the kind of guarantees that can be achieved. </p></div>
<div><head>Attack Model</head><p>We consider a curator DP model, where a trusted curator has access to the data, and publishes a noisy sketch of this data. The adversary is non-interactive, in that they have full access to the sketch of the dataset, or to sketches of disjoint subsets of the dataset if the latter is distributed across multiple devices (Figure <ref type="figure" target="#fig_1">2</ref>), but cannot query the curator(s) for more data. Whereas there exist some approaches that use random projection matrices as encryption keys <ref type="bibr" target="#b51">[51]</ref>, we here assume that the feature map ğš½ and the matrix of frequencies Î© are publicly known (similarly to, e.g., <ref type="bibr" target="#b32">[32]</ref>). This is essential for analysts, who need to know the feature map in order to learn from the sketch. The model also covers the case where analysts may be adversaries. We assume that each user contributes exactly one record to the total dataset, albeit our results can be extended to allow for multiple records per user. We do not make any assumptions on the background knowledge available to the adversary, nor on the operations that they are able to make. Hence, our privacy guarantees are robust to extreme cases where the adversary knows the entire database save for one user, and has infinite compute power.</p></div>
<div><head n="3.1">Definition and Properties</head><p>Randomness is an old tool for introducing uncertainty ("privacy by plausible deniability") when using sensitive information, e.g. implemented as randomized response surveys <ref type="bibr" target="#b58">[58]</ref>. Differential privacy <ref type="bibr" target="#b19">[19]</ref> provides a formal definition of the privacy guarantees offered by a randomized data release mechanism ğ‘… âˆ¶ D â†’ Z. Intuitively, a mechanism ğ‘… provides differential privacy if its output does not depend significantly on the presence of any one user in the database, hence hiding this presence from an adversary. Definition 7 (Differential privacy <ref type="bibr" target="#b19">[19]</ref>). The randomized mechanism ğ‘… achieves ğœ€-differential privacy (noted ğœ€-DP) iff for any measurable set ğ‘† of the co-domain of ğ‘…, and any X , Y âˆˆ D s.t. X âˆ¼ Y for some neighboring relation âˆ¼ (see below):</p><formula xml:id="formula_9">P [ğ‘…(X ) âˆˆ ğ‘†] â‰¤ exp(ğœ€) P [ğ‘…(Y) âˆˆ ğ‘†]<label>(6)</label></formula><p>The parameter ğœ€ &gt; 0 is called the privacy budget.</p><p>The smaller ğœ€, the closer the output distributions for two neighboring datasets are, and the stronger the privacy guarantee. Equivalently, differential privacy can be defined through the notion of privacy loss of a randomized mechanism. This is particularly useful when proving that a mechanism is differentially private.</p><p>Definition 8 (Privacy loss <ref type="bibr" target="#b21">[21]</ref>). Let ğ‘… be a randomized algorithm taking values in Z. If ğ‘… admits a density ğ‘ ğ‘…(X ) over Z for each input X , the privacy loss function is defined by</p><formula xml:id="formula_10">ğ¿ ğ‘… (ğ¬, X , Y) â‰œ log( ğ‘ ğ‘…(X ) (ğ¬) ğ‘ ğ‘…(Y) (ğ¬) ).</formula><p>The random mechanism ğ‘… achieves ğœ€-differential privacy iff sup ğ¬âˆˆZ X ,YâˆˆDâˆ¶ X âˆ¼Y ğ¿ ğ‘… (ğ¬, X , Y) â‰¤ ğœ€.</p><p>Intuitively, small values of the privacy loss of ğ‘… for some pair X , Y characterize regions of the codomain where output random variables ğ‘…(X ) and ğ‘…(Y) have "close" distributions.</p></div>
<div><head>Neighboring relation</head><p>The neighboring relation âˆ¼ in definition 7 defines the practical guarantees that DP offers. A common definition, called "unbounded" differential privacy (UDP), states that two datasets are neighbors if they differ by the addition or deletion of exactly one sample. From definition 7, this implies that the output of an algorithm that satisfies unbounded DP does not significantly depend on the presence of any one user in the dataset. An alternative is bounded DP (BDP), which defines two datasets as neighbors if and only if they differ by exactly one record by replacement.</p><p>We denote âŸ¦1, ğ‘›âŸ§ = {1, â€¦, ğ‘›}, S ğ‘› the permutation group of {1, â€¦ , ğ‘›} and ğœ(X ) a permuted collection: ğœ((ğ± </p><formula xml:id="formula_11">X B âˆ¼ Y â‡” |X | = |Y| and âˆƒ ğœ 1 , ğœ 2 âˆŠ S |X | s.t. ğœ 1 (X ) B â‰ˆ ğœ 2 (Y), (ğ± 1 , â€¦, ğ± ğ‘› ) B â‰ˆ (ğ² 1 , â€¦, ğ² ğ‘› ) â‡” âˆ€ğ‘– âˆŠ âŸ¦1</formula><p>, ğ‘› -1âŸ§, ğ± ğ‘– = ğ² ğ‘– , and ğ± ğ‘› , ğ² ğ‘› are arbitrary.</p><p>We assume |X | = |Y| + 1 in the definition for succinctness only, but the relation U âˆ¼ is symmetric. The key practical difference between the two definitions is that BDP assumes that the size of the dataset is not a sensitive value and can be published freely. Unbounded differential privacy is a stronger definition, as an ğœ€-UDP algorithm is necessarily 2ğœ€-BDPï¿½(using the composition lemmas presented below, and because if X B âˆ¼ Y, X can be obtained from Y by removing an element and adding a new one), while the reverse is not necessarily true. This bound might however not be tight. In the following, we mainly focus on the UDP setting, which is sometimes more tricky. However, most of the results are also adapted for BDP.</p><p>Composition An important property of differential privacy is composition: using several differentially private algorithms on the same dataset results in similar guarantees, but with a total privacy budget equal to the sum of the budgets of the individual algorithms. Hence, one can design a complex DP algorithm by splitting its privacy budget ğœ€ between different simpler routines.</p><p>Lemma 1 (Sequential composition <ref type="bibr" target="#b39">[39,</ref><ref type="bibr">Theorem 3]</ref>). Let (ğ‘… ğ‘– ) 1â‰¤ğ‘–â‰¤ğ‘Ÿ be a collection of DP mechanisms on the same domain with respective privacy budgets (ğœ€ ğ‘– ) 1â‰¤ğ‘–â‰¤ğ‘Ÿ . Then ğ‘… âˆ¶ X â†¦ (ğ‘… 1 (X ), â€¦ , ğ‘… ğ‘Ÿ (X )) provides (âˆ‘ ğ‘Ÿ ğ‘–=1 ğœ€ ğ‘– )-DP. This holds for both bounded and unbounded DP. Parallel composition can also be performed; the following lemma however holds only in the unbounded case.</p><p>Lemma 2 (Parallel composition <ref type="bibr" target="#b39">[39,</ref><ref type="bibr">Theorem 4]</ref>). Let (ğ‘… ğ‘– ) 1â‰¤ğ‘–â‰¤ğ‘Ÿ be a collection of independent ğœ€-UDP algorithms on the same domain D, and D ğ‘– be disjoint subsets of D. Then ğ‘… âˆ¶ X â†¦ (ğ‘… 1 (X âˆ©D 1 ), â€¦ , ğ‘… ğ‘Ÿ (X âˆ© D ğ‘Ÿ )) provides ğœ€-UDP, where (ğ± 1 , â€¦, ğ± ğ‘› ) âˆ© D ğ‘— denotes the subtuple with original ordering of the samples (ğ± ğ‘– ) 1â‰¤ğ‘–â‰¤ğ‘› that are in D ğ‘— .</p><p>These lemmas hold only when the ğ‘… ğ‘– are differentially private according to the same neighboring relation between datasets. Note also that privacy is robust to post-processing: if a mechanism ğ‘… is ğœ€-DP, then ğ‘“(ğ‘…(â€¢)) is also ğœ€-DP for any function ğ‘“. Thus Lemma 2 implies in particular that in a distributed setting, each data holder can compute and release an ğœ€-DP synopsis of its local data (e.g. a noisy sketch), and merging these quantities will lead to a global synopsis which is also ğœ€-DP with respect to the whole dataset.</p><p>Alternative privacy definitions Many alternative definitions of privacy have been proposed in the literature <ref type="bibr" target="#b54">[54]</ref>. Traditional statistical disclosure control metrics, such as ğ‘˜-anonymity <ref type="bibr" target="#b49">[49]</ref>, define anonymity as a property of the data, e.g. requiring that each user is indistinguishable from ğ‘˜ -1 others. However, anonymizing large-scale high-dimensional data (such as, e.g., mobility datasets) was shown to be hard, due to the high uniqueness of users in such datasets <ref type="bibr" target="#b16">[16]</ref>. Researchers have proposed to make privacy a property of the algorithm, enforcing for instance that the mutual information leakage is bounded <ref type="bibr" target="#b18">[18]</ref>. Differential privacy is the most popular of such definitions, as it considers a worst-case adversary, and is hence "future-proof": no future release of auxiliary information can break the privacy guarantees. Connections between differential privacy and other information-theoretic definitions have also been investigated <ref type="bibr" target="#b57">[57]</ref>.</p></div>
<div><head n="3.2">The Laplace Mechanism</head><p>In this section, we describe the Laplace mechanism <ref type="bibr" target="#b19">[19]</ref>, a very common and simple mechanism to release privately a function ğ‘“ computed over sensitive values. This mechanism adds Laplace noise to the function's output, whose scale ensures differential privacy. In the following, L(ğ‘) denotes the centered Laplace distribution of parameter ğ‘. </p><formula xml:id="formula_12">âˆ¼ L(ğ‘) (resp. L C (ğ‘)).</formula><p>The Laplace mechanism provides differential privacy if the scale ğ‘ of the noise is chosen carefully. This scale depends on the notion of sensitivity, which measures the maximum variation of a function between two neighboring datasets.</p><formula xml:id="formula_13">Definition 13 (ğ¿ 1 -sensitivity). The ğ¿ 1 -sensitivity of a function ğ‘“ âˆ¶ D â†’ R ğ‘š for a neighborhood relation âˆ¼ is defined as Î” 1 (ğ‘“) â‰œ sup X ,YâˆˆDâˆ¶ X âˆ¼Y â€–ğ‘“(X ) -ğ‘“(Y)â€– 1 . (<label>7</label></formula><formula xml:id="formula_14">)</formula><p>This definition extends to complex-valued functions by the canonical isomorphism between C ğ‘š and R 2ğ‘š .</p><p>Throughout the paper, we will use superscripts Î” U 1 and Î” B 1 to denote sensitivities computed respectively w.r.t. the UDP and BDP neighboring relations. Dwork et. al <ref type="bibr" target="#b20">[20]</ref> proved that the Laplace mechanism provides ğœ€-differential privacy for the noise level ğ‘ = Î” 1 (ğ‘“)/ğœ€. We propose below a straightforward extension of this result for the complex setting. Although only an upper bound on the sensitivity is required in order to prove that a mechanism is differentially private, we will also provide sharp bounds when possible, hence the notion of "sharp privacy level".</p><formula xml:id="formula_15">Theorem 1. Let ğ‘“ âˆ¶ D â†’ R ğ‘š or C ğ‘š .</formula><p>The Laplace mechanism applied on ğ‘“ is differentially private with sharp privacy budget ğœ€ * = Î” 1 (ğ‘“)/ğ‘. For ğœ€ &gt; 0, the lowest noise level yielding ğœ€-differential privacy is given by ğ‘ * = Î” 1 (ğ‘“)/ğœ€. This holds for both bounded and unbounded DP, provided that the sensitivities are computed according to the relevant neighborhood relation.</p><formula xml:id="formula_16">Proof. Let X , Y âˆŠ D be such that X âˆ¼ Y.</formula><p>Let ğ‘ X and ğ‘ Y denote the probability densities of the Laplace mechanism applied on ğ‘“ for datasets X and Y. In the real case, the privacy loss function takes the form</p><formula xml:id="formula_17">ğ¿ ğ‘“ (ğ¬, X , Y) = log( ğ‘ X (ğ¬) ğ‘ Y (ğ¬) ) = 1 ğ‘ (â€–ğ‘“(Y) -ğ¬â€– 1 -â€–ğ‘“(X ) -ğ¬â€– 1 )</formula><p>Hence: sup</p><formula xml:id="formula_18">ğ¬âˆŠR ğ‘š X ,YâˆŠDâˆ¶ X âˆ¼Y ğ¿ ğ‘“ (ğ¬, X , Y) = 1 ğ‘ sup X ,YâˆŠDâˆ¶ X âˆ¼Y ğ‘š âˆ‘ ğ‘—=1 sup ğ‘  ğ‘— âˆŠR |ğ‘“(Y) ğ‘— -ğ¬ ğ‘— | -|ğ‘“(X ) ğ‘— -ğ¬ ğ‘— | ( * ) = sup X ,YâˆŠDâˆ¶ X âˆ¼Y â€–ğ‘“(X ) -ğ‘“(Y)â€– 1 ğ‘ = Î” 1 (ğ‘“) ğ‘ .</formula><p>The inequality â‰¤ in (*) follows from the triangle inequality; ğ‘  ğ‘— = ğ‘“(Y) ğ‘— shows the equality. In the complex case, the proof is similar but using the density of a complex Laplace variable (Definition 11), and the definition of ğ¿ 1 -sensitivity in the complex case.</p><p>Note that the function ğ‘“ âˆ¶ X â†¦ |X | has UDP/BDP sensitivities Î” U 1 (ğ‘“) = 1 and Î” B 1 (ğ‘“) = 0, as all neighboring datasets have the same size for BDP. Releasing ğ‘› publicly is therefore ğœ€-BDP for any value of ğœ€, but this is not the case with UDP. This confirms the intuition that UDP treats the dataset size as sensitive, while BDP does not.</p></div>
<div><head n="3.3">Approximate Differential Privacy and the Gaussian Mechanism</head><p>Differential privacy is a very strong guarantee, and for many real-world tasks it can lead to severe degradations of the algorithms performance (utility) for small privacy budgets. For this reason, many relaxations of DP have been introduced, the most prominent of which is approximate differential privacy, also commonly called (ğœ€, ğ›¿)-DP <ref type="bibr" target="#b20">[20]</ref>.</p><p>Definition 14 (Approximate differential privacy <ref type="bibr" target="#b20">[20]</ref>). The randomized mechanism ğ‘… achieves (ğœ€, ğ›¿)-approximate differential privacy (noted (ğœ€, ğ›¿)-DP) for ğœ€ &gt; 0, ğ›¿ â‰¥ 0 iff for any measurable set ğ‘† of the co-domain of ğ‘…, and any X , Y âˆˆ D s.t. X âˆ¼ Y for some neighboring relation:</p><formula xml:id="formula_19">P [ğ‘…(X ) âˆˆ ğ‘†] â‰¤ exp(ğœ€) â‹… P [ğ‘…(Y) âˆˆ ğ‘†] + ğ›¿.<label>(8)</label></formula><p>The most common mechanism to achieve (ğœ€, ğ›¿)-DP is the Gaussian mechanism, adding Gaussian noise to the output of a function. As for the Laplace mechanism, we here consider potentially complexvalued outputs, and denote ğ‘§ âˆ¼ N C (0, ğœ<ref type="foot" target="#foot_1">2</ref> ) a random variable whose real and imaginary component are independently identically distributed as â„œğ‘§, â„‘ğ‘§ âˆ¼ N (0, ğœ 2 ) (note that the variance of ğ‘§ then reads</p><formula xml:id="formula_20">ğœ 2 ğ‘§ = 2ğœ 2 ). Definition 15 (Gaussian Mechanism). For any ğ‘“ âˆ¶ D â†’ R ğ‘š (resp. C ğ‘š ), the Gaussian mechanism with parameter ğœ is the random mechanism X â†¦ ğ‘“(X ) + ğ› where (ğœ‰ ğ‘— ) 1â‰¤ğ‘—â‰¤ğ‘š iid âˆ¼ N (0, ğœ 2 ) (resp. N C (0, ğœ 2 )).</formula><p>The advantage of this DP relaxation is that the noise standard deviation needed for (ğœ€, ğ›¿)-DP scales not with the ğ¿ 1 but with the ğ¿ 2 sensitivity of ğ‘“, defined just below, which can be significantly smaller for many functions, including our sketching operator.</p><formula xml:id="formula_21">Definition 16 (ğ¿ 2 -sensitivity). The ğ¿ 2 -sensitivity of a function ğ‘“ âˆ¶ D â†’ R ğ‘š for a neighborhood relation âˆ¼ is defined as Î” 2 (ğ‘“) â‰œ sup X ,YâˆˆDâˆ¶ X âˆ¼Y â€–ğ‘“(X ) -ğ‘“(Y)â€– 2 .</formula><p>This definition extends to complexvalued functions using the canonical isomorphism between C ğ‘š and R 2ğ‘š .</p><p>The "classical" noise calibration for the (real) Gaussian mechanism comes from [21, Appendix A], which shows that, assuming ğœ€ &lt; 1, a standard deviation ğœ &gt; (2 ln(1.25/ğ›¿)) 0.5 Î” 2 (ğ‘“)/ğœ€ is sufficient to guarantee (ğœ€, ğ›¿)-DP. This bound is commonly used but not sharp, especially in the high privacy regime (i.e. small ğœ€), and restricted to ğœ€ &lt; 1. The calibration of the required noise parameter ğœ has recently been carefully tightened by Balle et al. <ref type="bibr" target="#b5">[5]</ref>, which is the mechanism we will use in this work 2 .</p><p>Theorem 2 (Analytical Gaussian mechanism <ref type="bibr" target="#b5">[5,</ref><ref type="bibr">Theorem 9]</ref>). For each ğœ€, ğ›¿ &gt; 0, the lowest noise level ğœ * such that the (real) Gaussian mechanism provides (ğœ€, ğ›¿)-DP is given by ğœ * = ğœ‚(ğœ€, ğ›¿)</p><formula xml:id="formula_22">Î” 2 (ğ‘“) âˆš 2ğœ€</formula><p>, where ğœ‚(ğœ€, ğ›¿) is described in <ref type="bibr" target="#b5">[5]</ref> and can be computed with a numerical algorithmic procedure.</p><p>Note that the term ğœ‚(ğœ€, ğ›¿) depends on ğœ€, hence it is incorrect to say that ğœ * scales in ğœ€ -1/2 . In particular, when ğœ€ â†’ 0 the noise level converges to a finite constant <ref type="bibr" target="#b5">[5,</ref><ref type="bibr">Section 2.1]</ref>. Compared to the standard Gaussian mechanism of Dwork and Roth, Theorem 2 has also the advantage to hold in the low-privacy regime (i.e. when ğœ€ &gt; 1).</p><p>The result holds for complex-valued feature maps as well using the canonical isomorphism between C ğ‘š and R 2ğ‘š , as applying the complex Gaussian mechanism on a complex-valued ğš½(â€¢) is equivalent to applying the real Gaussian mechanism to [â„œğš½(â€¢); â„‘ğš½(â€¢)], and</p><formula xml:id="formula_23">â€–[â„œğš½(â€¢); â„‘ğš½(â€¢)]â€– 2 = â€–ğš½(â€¢)â€– 2 .</formula><p>We expect from the literature and from the definitions of the sensitivities that using the Gaussian mechanism should help to reduce the noise level required to achieve privacy, and thus increase the overall learning performance. We will see in particular in the next section that the ğ¿ 1 and ğ¿ 2 sensitivities scale with the sketch size ğ‘š respectively in ğ‘š and ğ‘š 1/2 .</p><p>Note that simple composition theorems also exist for approximate differential privacy similarly to Lemma 1. We provide here only the result on sequential composition for succinctness, but results on parallel composition can be found in the literature as well.</p><p>Lemma 3 (Sequential composition <ref type="bibr" target="#b21">[21,</ref><ref type="bibr">Theorem 3.16]</ref>). Let (ğ‘… ğ‘– ) 1â‰¤ğ‘–â‰¤ğ‘Ÿ be a collection of (ğœ€ ğ‘– , ğ›¿ ğ‘– )-DP mechanisms on the same domain. Then ğ‘… âˆ¶ X â†¦ (ğ‘… 1 (X ), â€¦ , ğ‘… ğ‘Ÿ (X )) provides (âˆ‘ ğ‘Ÿ ğ‘–=1 ğœ€ ğ‘– , âˆ‘ ğ‘Ÿ ğ‘–=1 ğ›¿ ğ‘– )-DP. We now explain how the privacy definitions introduced in this section can be satisfied with the sketching framework.</p></div>
<div><head n="4">Differentially Private Sketching</head><p>Sketching, as proposed in Definition 1, is not sufficient per se to ensure the differential privacy of user contributions, despite the fact that the sketch itself (which is just at most ğ‘š â‰ª ğ‘›ğ‘‘ real or complex numbers) cannot contain much information about each of the ğ‘› samples ğ± ğ‘– âˆˆ R ğ‘‘ . In particular, although the vectors (ğ›š ğ‘— ) ğ‘š ğ‘—=1 are randomly drawn, the sketching mechanism induced by a given set of such vectors is deterministic. We construct a noisy sketch, based on the Laplacian (resp. Gaussian) mechanism, that guarantees ğœ€-differential privacy (resp. (ğœ€, ğ›¿)-differential privacy).</p><p>The clean sketch ğ³ X from (1) can be written ğ³ X = ğšº(X )/|X |, where ğšº(X ) â‰œ âˆ‘ ğ‘› ğ‘–=1 ğš½(ğ± ğ‘– ) denotes the sum of features and |X | the number of records. Our mechanism adds noise to the numerator and denominator separately, i.e. releases (ğšº(X ) + ğ›, |X | + ğœ) where both ğ› and ğœ are random. Both quantities are thus made private provided that the noise levels are properly chosen, as discussed in the following subsections. This also means that we can further average sketches after computation in a distributed setting. The sketch ğ³ X can then be estimated from these two quantities, e.g. using ğ¬(X ) â‰œ (ğšº(X ) + ğ›)/(|X | + ğœ), which is private by composition properties of differential privacy. Note that DP is also robust to postprocessing, so one could for instance replace |X | + ğœ by max(|X | + ğœ, 1) to avoid dividing by a null or negative quantity. The noise ğ› added to ğšº can be either Laplacian or Gaussian, and we provide guarantees for both cases respectively in Section 4.1 and Section 4.2, each time for both random Fourier features and PCA. In the following, we use the notations ğšº RFF and ğšº RQF when ğšº is computed using respectively ğš½ = ğš½ RFF and ğš½ = ğš½ RQF .</p></div>
<div><head n="4.1">Private Sketching with the Laplace Mechanism</head><p>We introduce formally the noisy sum of features.</p></div>
<div><head>Definition 17. The noisy sum of features ğšº</head><formula xml:id="formula_24">L of a dataset X = (ğ± ğ‘– ) ğ‘› ğ‘–=1 âˆŠ D ğ‘› with noise parameters ğ‘ is the random variable ğšº L (X ) = ğšº(X ) + ğ›, where ğšº(X ) â‰œ âˆ‘ ğ‘› ğ‘–=1 ğš½(ğ± ğ‘– ) and âˆ€ğ‘— âˆŠ 1, ğ‘š , ğœ‰ ğ‘— iid âˆ¼ { L C (ğ‘) if ğš½ is complex-valued L(ğ‘) if ğš½ is real-valued .</formula><p>The scale of the noise will depend on the feature map used. Remember that we need an estimate of the sketch, and not just the sum of features. We introduce a generic lemma for this purpose. 2 ) is ğœ€ 2 -UDP. The result comes from the sequential composition Lemma 1.</p></div>
<div><head>Lemma 4. For any privacy parameter ğœ€ &gt; 0 and any choice of</head><formula xml:id="formula_25">ğœ€ 1 , ğœ€ 2 &gt; 0 such that ğœ€ 1 + ğœ€ 2 = ğœ€, if ğšº has a finite sensitivity Î” U 1 (ğšº), then any mechanism to estimate ğ³ X using ğšº L (X ) instantiated with a noise level ğ‘ = Î” U 1 (ğšº)/ğœ€ 1 and |X | + ğœ, where ğœ âˆ¼ L(ğœ€ -1 2 ), is ğœ€-UDP. Proof. The Laplace mechanism applied on ğšº with ğ‘ = Î” U 1 (ğšº)/ğœ€ 1 is ğœ€ 1 -UDP</formula><p>To prove differential privacy of the sketching mechanism, we thus only need to compute the sensitivity Î” U 1 (ğšº) of the sum-of-features function. We will see in Section 4.2 that a similar result can be stated for the Gaussian mechanism using the ğ¿ 2 sensitivity. We introduce in the following lemma a common expression to deal with the different cases -Laplacian and Gaussian mechanisms, real-and complexvalued feature maps.</p><formula xml:id="formula_26">Lemma 5. Let ğšº âˆ¶ (ğ± 1 , â€¦, ğ± ğ‘› ) â†¦ âˆ‘ 1â‰¤ğ‘–â‰¤|X | ğš½(ğ± ğ‘– )</formula><p>where ğš½ is any feature map taking values in R ğ‘š or C ğ‘š . For ğ‘ = 1, 2, the ğ¿ ğ‘ sensitivity of ğš½ for datasets on a domain ğ¸ is</p><formula xml:id="formula_27">Î” U ğ‘ (ğšº) = sup ğ±âˆŠğ¸ ğ‘„ U ğ‘ (ğ±)</formula><p>where ğ‘„ U ğ‘ (ğ±) = â€–ğš½(ğ±)â€– ğ‘ for real-valued features maps, and extends to complex-valued feature maps using the canonical isomorphism between C ğ‘š and R 2ğ‘š .</p><p>Note that in particular, ğ‘„ U 1 (ğ±) = â€–R(ğš½(ğ±))â€– 1 + â€–I(ğš½(ğ±))â€– 1 for a complex-valued ğš½. Proof. For a real-valued feature map ğš½, we have by definitions 9, 13 and 16:</p><formula xml:id="formula_28">Î” U ğ‘ (ğšº) = sup X ,YâˆŠDâˆ¶X U âˆ¼Y â€–ğšº(X ) -ğšº(Y)â€– ğ‘ = sup X =(ğ± 1 ,â€¦,ğ± ğ‘› )âˆŠD, Y=(ğ² 1 ,â€¦,ğ² ğ‘›-1 )âˆŠD, such that X U âˆ¼Y âˆ¥ ğ‘› âˆ‘ ğ‘–=1 ğš½(ğ± ğ‘– ) - ğ‘›-1 âˆ‘ ğ‘–=1 ğš½(ğ² ğ‘– )âˆ¥ ğ‘ = sup ğ±âˆŠğ¸ â€–ğš½(ğ±)â€– ğ‘ . (<label>9</label></formula><formula xml:id="formula_29">)</formula><p>The result extends to the complex case using the canonical isomorphism between C ğ‘š and R 2ğ‘š , with</p><formula xml:id="formula_30">â€–[R(ğš½(ğ±)); I(ğš½(ğ±))]â€– 2 = â€–ğš½(ğ±)â€– 2 and â€–[R(ğš½(ğ±)); I(ğš½(ğ±))]â€– 1 = â€–R(ğš½(ğ±))â€– 1 + â€–I(ğš½(ğ±))â€– 1 .</formula><p>In the following, we use the notations ğ‘„ RFF ğ‘ , ğ‘„ RQF ğ‘ when the feature maps ğš½ RFF , ğš½ RQF are used. Note however that this Lemma is generic, and could be applied to any new feature map in the future. We compute Î” U 1 (ğšº RFF ) and Î” U 1 (ğšº RQF ) using Lemma 5 respectively in Section 4.1.1 and Section 4.1.2. Note that we compute the sensitivities using the expression of the feature maps given in Definitions 4 and 6, but any constant factor ğ‘ ğš½ could be used in these expressions provided that the inverse problem is solved using the same scaling (one could for instance use ğ‘ ğš½ = 1/ âˆš ğ‘š to get normalized features); this would yield similar privacy guarantees, but the sensitivity and thus the noise level ğ‘ would be multiplied by the same factor. We discuss how to optimally split the privacy budget between ğœ€ 1 and ğœ€ 2 in Section 6.5.</p></div>
<div><head n="4.1.1">Random Fourier Features</head><p>We compute the ğ¿ 1 -sensitivity of ğšº RFF in Lemma 7. We first introduce a lemma on diophantine approximation, that will be needed to prove the sharpness of our bound. Definition 18. The scalars (ğœ” ğ‘— ) 1â‰¤ğ‘—â‰¤ğ‘š âˆŠ R are called nonresonant frequencies <ref type="bibr" target="#b52">[52]</ref>  </p><formula xml:id="formula_31">(ğšº RFF ) = ğ‘š âˆš 2.</formula><p>Proof. We recall that ğš½ RFF (ğ±) = (ğœŒ(Î© ğ‘‡ ğ± + ğ®) + iğœŒ(Î© ğ‘‡ ğ± + ğ® -ğœ‹ 2 )) in order to deal with both unquantized (ğœŒ = cos, ğ® = 0) and quantized (ğœŒ = 2 -1/2 sign âˆ˜ cos, ğ® âˆŠ [0, 2ğœ‹[ ğ‘š ) mechanisms with the same formalism. Using the definition of ğ‘„ ğ‘ from Lemma 5 in the Laplace real case, we have As a result, denoting ğ‘“ ğ›— (ğ±) â‰œ âˆ‘ ğ‘š ğ‘—=1 ğ‘“(ğ›š ğ‘‡ ğ‘— ğ± -ğœ‘ ğ‘— ) for each ğ›— âˆŠ R ğ‘š , we obtain</p><formula xml:id="formula_32">ğ‘„ RFF 1 (ğ±) â‰œ â€–â„œ(ğš½ RFF (ğ±))â€– 1 + â€–â„‘(ğš½ RFF (ğ±))â€– 1 = ğ‘š âˆ‘ ğ‘—=1 |ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— )| + |ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— - ğœ‹ 2 )| Denoting ğ‘“(â€¢) â‰œ ğœŒ(â€¢) + ğœŒ(â€¢ -ğœ‹/2) we show that |ğœŒ(â€¢)| + |ğœŒ(â€¢ -ğœ‹/2)| = sup ğœ‘âˆˆ{0,ğœ‹/2,</formula><formula xml:id="formula_33">ğ‘„ RFF 1 (ğ±) = ğ‘š âˆ‘ ğ‘—=1 sup ğœ‘ ğ‘— âˆŠ{0,ğœ‹/2,ğœ‹,3ğœ‹/2} ğ‘“(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— -ğœ‘ ğ‘— ) = sup ğ›—âˆŠ{0,ğœ‹/2,ğœ‹,3ğœ‹/2} ğ‘š ğ‘“ ğ›—-ğ® (ğ±).<label>(10)</label></formula><p>In the complex exponential case ğœŒ = cos and ğ‘“ âˆ¶ ğ‘¥ â†¦ âˆš 2 cos(ğ‘¥ -ğœ‹/4). In the quantized case as </p><formula xml:id="formula_34">ğœŒ = 2 -1/2 sign</formula><formula xml:id="formula_35">Î” U 1 (ğšº RFF ) = sup ğ±âˆŠR ğ‘‘ ğ‘„ RFF 1 (ğ±) = sup ğ›—âˆŠ{0,ğœ‹/2,ğœ‹,3ğœ‹/2} ğ‘š sup ğ±âˆŠR ğ‘‘ ğ‘“ ğ›—-ğ® (ğ±) = ğ‘š âˆš 2,<label>(11)</label></formula><p>where the supremum is independent of the choice of ğ›—.</p><p>Note that this holds only for ğ¸ = R ğ‘‘ . If the domain is restricted to e.g. ğ¸ = B 2 = {ğ± âˆ¶ â€–ğ±â€– 2 â‰¤ 1} the upper bound may not be reached, even with nonresonant frequencies, so an improved privacy may be possible.</p><p>For this result to be applicable, we still need to prove that the frequencies are nonresonant in practice. </p><formula xml:id="formula_36">Î” U 1 (ğšº RQF ) = sup ğ±âˆŠB 2 ğ‘„ RQF 1 (ğ±) = sup ğ±âˆŠB 2 â€–ğš½ RQF (ğ±)â€– 1 = sup ğ±âˆŠB 2 ğ‘š âˆ‘ ğ‘—=1 (ğœ” ğ‘‡ ğ‘— ğ±) 2 = sup ğ±âˆ¶â€–ğ±â€–â‰¤1 ğ± ğ‘‡ ( ğ‘š âˆ‘ ğ‘—=1 ğ›š ğ‘— ğ›š ğ‘‡ ğ‘— )ğ± = ğœ† max (Î©Î© ğ‘‡ ) = â€–Î©â€– 2 2 .</formula><p>The quantity â€–Î©â€– 2 2 can be computed numerically for a given Î©. When ğ‘š is a multiple of ğ‘‘ and Î© is a concatenation of ğ‘š/ğ‘‘ orthonormal bases as detailed in Section 2.2, we have Î©Î© ğ‘‡ = ğ‘š/ğ‘‘ğˆ ğ‘‘ and thus â€–Î©â€– 2 2 = ğ‘š/ğ‘‘. When Î© has i.i.d. N (0, 1/ğ‘‘) entries, â€–Î©â€– 2 2 is of the same order with high probability.</p></div>
<div><head n="4.2">Approximate Differential Privacy with the Gaussian Mechanism</head><p>In practice, in order to increase the utility of private mechanisms relying on additive perturbations, ğœ€-DP is often relaxed to approximate (ğœ€, ğ›¿)-DP. In this section we provide an (ğœ€, ğ›¿)-DP sketching mechanism based on the Gaussian mechanism.</p></div>
<div><head>Definition 19. The Gaussian noisy sum of features ğšº</head><formula xml:id="formula_37">G (X ) of a dataset X = (ğ± ğ‘– ) ğ‘› ğ‘–=1 with noise param- eters ğœ is the random variable ğšº G (X ) = ğšº(X ) + ğ› where ğšº(X ) â‰œ âˆ‘ ğ‘› ğ‘–=1 ğš½(ğ± ğ‘– ) and âˆ€ğ‘— âˆŠ 1, ğ‘š , ğœ‰ ğ‘— iid âˆ¼ { N C (0, ğœ 2 ) if ğš½ is complex-valued N (0, ğœ 2 ) if ğš½ is real-valued .</formula><p>The only difference with Definition 17 is that the noise added on the sum of features ğšº(X ) is Gaussian. We now introduce an equivalent of the composition lemma 4 for the Gaussian case.</p><p>Lemma 10. For any privacy parameter ğœ€ &gt; 0 and choice of ğœ€ 2 ) is (ğœ€ 2 , 0)-UDP. The result comes from Lemma 3 on sequential composition of approximate differential privacy.</p><p>Note that we add Laplacian noise on the dataset size; if Gaussian noise was added we would have to split not only ğœ€ but also ğ›¿ between the sum of features and the dataset size. As there is no difference between</p><formula xml:id="formula_38">Î” U 1 (| â€¢ |) and Î” U 2 (| â€¢ |)</formula><p>, allocating a part of ğ›¿ to the denominator would not bring any substantial gain compared to putting all the budget on the numerator.</p><p>We now compute the sensitivities Î” U 2 (ğšº RFF ) (Section 4.2.1) and Î” U 2 (ğšº RQF ) (Section 4.2.2). Here again, in case the feature maps are multiplied by a constant factor, the ğ¿ 2 sensitivity and thus the noise level ğœ need to be multiplied by the same factor.</p></div>
<div><head n="4.2.1">Random Fourier Features</head><p>For random Fourier features, computing the ğ¿ 2 sensitivity is much more straightforward than the ğ¿ 1 sensitivity, as each component of the feature map has a constant modulus. We get the following result. Proof. Using the fact that |ğš½(ğ±) ğ‘— | = 1 for any ğ‘— and ğ±, we have by Lemma 5</p><formula xml:id="formula_39">Î” U 2 (ğšº RFF ) = sup ğ±âˆŠR ğ‘‘ ğ‘„ RFF 2 (ğ±) = sup ğ±âˆˆR ğ‘‘ âˆš âˆš âˆš â· ğ‘š âˆ‘ ğ‘—=1 |ğš½ ğ‘— (ğ±)| 2 = âˆš ğ‘š.</formula><p>As expected, the standard deviation of the Gaussian noise is smaller than the standard deviation of the Laplacian noise that one would need to add in order to reach the same privacy level with the Laplace mechanism. Indeed, the ğ¿ 2 sensitivity only scales with âˆš ğ‘š, where the ğ¿ 1 sensitivity was scaling linearly with ğ‘š.</p><p>For bounded differential privacy, we have the following result. </p></div>
<div><head n="4.2.2">Random Quadratic Features</head><p>In this subsection, we consider again datasets whose elements are bounded by 1 in ğ¿ 2 -norm, and reuse the notations</p><formula xml:id="formula_40">ğ¸ = B 2 , D ğ‘› = ğ¸ ğ‘› , D â‰œ âˆª ğ‘›âˆŠN D ğ‘› .</formula><p>Lemma 13. The function ğšº RQF built using a matrix of frequencies</p><formula xml:id="formula_41">Î© = [ğ›š 1 , â€¦, ğ›š ğ‘š ], has ğ¿ 2 sensitivity Î” U 2 (ğšº RQF ) = ğ‘† 4 (Î©), where ğ‘† 4 (Î©) = (sup ğ±âˆ¶â€–ğ±â€– 2 â‰¤1 âˆ‘ ğ‘š ğ‘—=1 (ğœ” ğ‘‡ ğ‘— ğ±) 4 ) 1/2 .</formula><p>Proof. We have by Lemma 5</p><formula xml:id="formula_42">Î” U 2 (ğšº RQF ) = sup ğ±âˆŠB 2 ğ‘„ RQF 2 (ğ±) = sup ğ±âˆŠB 2 â€–ğš½ RQF (ğ±)â€– 2 = ( sup ğ±âˆ¶â€–ğ±â€– 2 â‰¤1 ğ‘š âˆ‘ ğ‘—=1 (ğœ” ğ‘‡ ğ‘— ğ±) 4 ) 1/2 = ğ‘† 4 (Î©).</formula><p>The quantity ğ‘† 4 (Î©) can be estimated numerically.</p></div>
<div><head n="5">A faster mechanism with frequency subsampling</head><p>We now introduce a sketching mechanism that subsamples the features as shown in Figure <ref type="figure">3</ref>, and then build on top of it a noisy sketch that guarantees ğœ€-differential privacy. This mechanism differs from the standard approach which consists in subsampling the data samples <ref type="bibr" target="#b4">[4]</ref> rather than the features. The following result is for instance well known in the literature.</p></div>
<div><head>Lemma 14 ([4, Table 1]).</head><p>Let ğ‘€ be an ğœ€-UDP mechanism, and denote by S the Poisson data-subsampling mechanism with parameter ğ›¼ (i.e. each data sample ğ± ğ‘– is kept independently from the others with probability ğ›¼). Then the mechanism X â†¦ ğ‘€ (S(X )) is ğœ€ â€² -UDP with ğœ€ â€² = log(1 + ğ›¼(exp(ğœ€) -1)) &lt; ğœ€.</p><p>We will see in Lemma 15 that this privacy level ğœ€ â€² is sharp when Lemma 14 is applied to our sketching mechanism, but also that the same bound is obtained -and is still sharp -when sampling the features with probability ğ›¼ rather than the data samples. Although sampling the data is more generic -this can be used for any application and any mechanism, while sampling the features is more specific -, both techniques are relevant in our setting, and we will see in Section 6.3 that subsampling the features rather than the data can in some cases yield better utility-privacy tradeoffs at constant computational complexity. We will focus mostly on pure differential privacy guarantees for simplicity and conciseness, and give a generic upper bound that applies to approximate differential privacy as well.</p><p>The proposed subsampling mechanism, which consists in computing only some of the ğ‘š entries of ğš½(ğ± ğ‘– ) for each data sample ğ± ğ‘– as shown in Figure <ref type="figure">3</ref>, is mainly introduced in order to reduce the computational complexity of the sketching operation. This complexity is dominated by the computation of all the (ğ›š ğ‘‡ ğ‘— ğ± ğ‘– ) 1â‰¤ğ‘–â‰¤ğ‘›,1â‰¤ğ‘—â‰¤ğ‘š , i.e. by the matrix product Î© ğ‘‡ ğ‘‹, which costs Î˜(ğ‘šğ‘‘ğ‘›) when using a dense matrix Î©. As shown below in Lemma 16, subsampling (whether it is on the data samples or on the features) does not bring any advantage in enforcing differential privacy, i.e. the noise level required to get privacy is at least the same as without subsampling. Indeed, the privacy "amplification" induced by the subsampling operation is compensated by the fact that the sketch must be properly rescaled afterwards. We will prove however that in some settings, the guarantees obtained with and without subsampling are exactly the same. Moreover, it will be shown in Section 6.4 that feature subsampling can be performed for large collections without significantly damaging the utility of the sketch, which motivates our approach.</p><p>Subsampling does also reduce the amount of information that is released about each sample, although this has no impact on differential privacy guarantees. Indeed, in the extreme case of feature subsampling we will measure only one floating point (or complex) number per data sample. When using a quantized sketch, this is further reduced to one bit (or two) of information per sample. For instance, if we only have the quantized random Fourier measurement of a sample ğ± associated to the frequency ğ›š ğ‘— , we can can only infer that ğ± belongs to a union of "slices" of the ambient space delimited by affine hyperplanes orthogonal to ğ›š ğ‘— . But in practice these features are further averaged over the samples (such individual sketches are computed by the data holder but not released publicly), the subsampling is performed randomly (so that we don't know which entry of the sketch a given sample contributed to) and, in the differential privacy scenario, noise can still be added to the obtained sketch. Although we only focus on differential privacy in this paper, we expect that this variant of the framework would be beneficial when working with alternative privacy definitions that rely on average information-theoretic quantities, such as mutual information <ref type="bibr" target="#b57">[57]</ref>.</p></div>
<div><head>Subsampling schemes</head><p>We define H â‰œ {0, 1} ğ‘š the set of binary masks ğ¡ and H ğ‘› the set of all possible tuples (ğ¡ 1 , â€¦ , ğ¡ ğ‘› ) of ğ‘› such masks. Pointwise multiplication is denoted âŠ™. In the following, we consider a real number 0 &lt; ğ›¼ â‰¤ </p><formula xml:id="formula_43">H ğ‘Ÿ â‰œ {ğ¡ âˆŠ H | ğ‘š âˆ‘ ğ‘–=1 â„ ğ‘– = ğ‘Ÿ},<label>(12)</label></formula><p>where 1 â‰¤ ğ‘Ÿ â‰¤ ğ‘š is an integer, in which case U(H ğ‘Ÿ ) âˆŠ P ğ›¼ with ğ›¼ â‰œ ğ‘Ÿ/ğ‘š; â€¢ Block-uniform feature sampling: when ğ‘š/ğ‘‘ is an integer and ğ‘Ÿ is a multiple of ğ‘‘, U(H struct.</p></div>
<div><head>ğ‘Ÿ</head><p>) is the uniform distribution over H struct.</p></div>
<div><head>ğ‘Ÿ</head><p>, the subset of H ğ‘Ÿ containing only the vectors which are structured by blocs of size ğ‘‘, i.e. H struct.</p></div>
<div><head>ğ‘Ÿ</head><p>â‰œ {ğ¡ = [â„ 1 , â€¦, â„ ğ‘š ] âˆŠ H ğ‘Ÿ |âˆ€ğ‘– âˆŠ âŸ¦1, ğ‘š/ğ‘‘âŸ§, â„ (ğ‘–-1)ğ‘‘+1 = â„ (ğ‘–-1)ğ‘‘+2 = â€¦ = â„ ğ‘–ğ‘‘ }. In this case we also have U(H struct.</p></div>
<div><head>ğ‘Ÿ</head><p>) âˆŠ P ğ›¼ with ğ›¼ â‰œ ğ‘Ÿ/ğ‘š. This scheme will be useful when Î© is a structured transform, as explained in the next paragraph.</p><p>A note on sketching complexity When computing ğ‘Ÿ = âŒˆğ›¼ğ‘šâŒ‰ features per input sample rather than computing the whole matrix product Î© ğ‘‡ ğ‘‹, the sketching complexity goes down from Î˜(ğ‘šğ‘‘ğ‘›) to Î˜(ğ‘Ÿğ‘‘ğ‘›). In the high-dimensional setting, previous works <ref type="bibr" target="#b12">[12]</ref> suggested to speed such computations by using structured matrices Î© made of âŒˆğ‘š/ğ‘‘âŒ‰ square ğ‘‘ Ã— ğ‘‘ blocks associated to as many fast transforms. In that case, the matrix-vector multiplication for each square block is performed at once using the corresponding fast transform with complexity Î˜(ğ‘‘ log(ğ‘‘)). We can thus rely on block-uniform subsampling mechanism introduced above using ğ‘Ÿ = ğ‘‘, so that for each data sample ğ± ğ‘– we compute the ğ‘‘ measurements associated to a randomly chosen block. The sketching cost is then Î˜(ğ‘‘ log(ğ‘‘)ğ‘›), while computing the same number ğ‘Ÿ = ğ‘‘ of measurements with a dense matrix Î© would have scaled in Î˜(ğ‘‘ 2 ğ‘›).</p></div>
<div><head>Sketching with subsampling</head><p>We first define how features are subsampled using a fixed tuple of masks, and then define the sketching mechanism using random masks. Definition 20. The sum of subsampled features of a dataset X = (ğ± 1 , â€¦, ğ± ğ‘› ), using a fixed set of binary masks ğ» = (ğ¡ 1 , â€¦, ğ¡ ğ‘› ) âˆŠ H ğ‘› that has been drawn according to some distribution in P ğ›¼ is defined as</p><formula xml:id="formula_44">ğšº ğ» (X ) â‰œ 1 ğ›¼ ğ‘› âˆ‘ ğ‘–=1 ğš½(ğ± ğ‘– ) âŠ™ ğ¡ ğ‘– .</formula><p>The constant 1/ğ›¼ in Definition 20 is used to ensure that we always have E ğ» |X | -1 ğšº ğ» (X ) = ğ³ X when ğ» is drawn according to ğ‘ ğ‘› ğ¡ for some ğ‘ ğ¡ âˆŠ P ğ›¼ . We will see below that, although subsampling reduces the noise level allowing to make the (unnormalized) sum of features private (which is sometimes referred to as "privacy amplification by subsampling"), the rescaling factor 1/ğ›¼ is required to obtain sketches of comparable utility for a given noise level, and privacy is not amplified once taking this factor into account. We now introduce the whole mechanism, where the masks themselves are drawn randomly. Definition 21. The Laplacian subsampled sum of features Ì… ğšº L (X ) of a dataset X âˆŠ D ğ‘› using a mask distribution ğ‘ ğ¡ âˆŠ P ğ›¼ and a noise parameter ğ‘ is the random variable</p><formula xml:id="formula_45">Ì… ğšº L (X ) = ğšº ğ» (X ) + ğ›, (<label>13</label></formula><formula xml:id="formula_46">)</formula><p>where</p><formula xml:id="formula_47">âˆ€ğ‘— âˆŠ 1, ğ‘š , ğœ‰ ğ‘— iid âˆ¼ { L C (ğ‘) if ğš½ is complex-valued L(ğ‘) if ğš½ is real-valued , and ğ» = (ğ¡ 1 , â€¦, ğ¡ ğ‘› ) with ğ¡ ğ‘– iid âˆ¼ ğ‘ ğ¡ .</formula><p>For a deterministic set of masks ğ», we denote ğšº L,ğ» (X ) = ğšº ğ» (X ) + ğ› the sum that is randomized only on ğ›. Compared to the deterministic sum of features ğšº, the Laplacian subsampled sum of features Ì… ğšº L thus picks at random some values from each feature vector ğš½(ğ± ğ‘– ) according to a random mask ğ¡ ğ‘– and then adds Laplacian noise ğ› on the sum of those contributions. Note that in this mechanism (and by opposition to ğšº L,ğ» ), both ğ› and ğ» are random quantities.</p><p>In order to formulate our results, we define a quantity ğ‘„ U 1 which is similar to the quantity from Lemma 5 but takes into account a mask ğ¡ âˆˆ H. Although we only consider ğ‘„ U 1 for the moment, we also introduce the quantities ğ‘„ B 1 , ğ‘„ U 2 , ğ‘„ B 2 which will be used in Section 5.3 for generalizing some results to the BDP and/or approximate DP settings. For a real-valued feature map and ğ‘ âˆŠ {1, 2}, we define</p><formula xml:id="formula_48">ğ‘„ U ğ‘ (ğ±, ğ¡) â‰œ 1 ğ›¼ â€–ğš½(ğ±) âŠ™ ğ¡â€– ğ‘<label>(14)</label></formula><formula xml:id="formula_49">ğ‘„ B ğ‘ (ğ±, ğ², ğ¡) â‰œ 1 ğ›¼ â€–(ğš½(ğ±) -ğš½(ğ²)) âŠ™ ğ¡â€– ğ‘ .<label>(15)</label></formula><p>The definition extends to complex-valued feature maps using the canonical isomorphism between C ğ‘š and R 2ğ‘š , but using the same mask ğ¡ for both real and imaginary parts.</p><p>Similarly to Section 4 where the privacy was directly driven by the quantity Î” U 1 (ğšº), itself equal to sup ğ±âˆŠğ¸ ğ‘„ U 1 (ğ±), the following lemma gives a generalization taking the masks into account.</p><p>Lemma 15. The Laplacian subsampled sum Ì… ğšº L (X ) from Definition 21 with noise level ğ‘ is UDP with sharp privacy parameter ğœ€ * , defined as</p><formula xml:id="formula_50">exp(ğœ€ * ) = sup ğ±âˆŠğ¸ E ğ¡ exp( 1 ğ‘ ğ‘„ U 1 (ğ±, ğ¡)).<label>(16)</label></formula><p>The proof can be found in Appendix C.1. For the standard Poisson data-subsampling mechanism, Equation ( <ref type="formula" target="#formula_50">16</ref>) can be rewritten</p><formula xml:id="formula_51">exp(ğœ€ * ) = sup ğ±âˆŠğ¸ {(1 -ğ›¼) â‹… 1 + ğ›¼ â‹… exp( 1 ğ‘ 1 ğ›¼ â€–ğš½(ğ±)â€– 1 )} = 1 + ğ›¼(exp( 1 ğ›¼ Î” U 1 (ğšº) ğ‘ ) -1). (<label>17</label></formula><formula xml:id="formula_52">)</formula><p>We thus recover the known bound of Lemma 14, however with the additional guarantee in our case that the bound is sharp. Indeed, according to this lemma if ğ‘€ is a random mechanism and S denotes the Poisson data-subsampling mechanism with parameter ğ›¼, then the mechanism X â†¦ ğ‘€ (S(X )) is ğœ€ â€² -UDP with ğœ€ â€² = log(1 + ğ›¼(exp(ğœ€) -1)). Applying this result to the mechanism ğ‘€ âˆ¶ X â†¦ ğ›¼ -1 ğšº(X ) + ğ› which, by Theorem 1, is ğœ€-UDP with ğœ€ = Î” U 1 (ğ›¼ -1 ğšº)ğ‘ -1 = ğ›¼ -1 Î” U 1 (ğšº)ğ‘ -1 when ğ› has iid (complex) Laplace components with parameter ğ‘, yields that the mechanism Ì… ğšº L (X ) is ğœ€ â€² -UDP with exp(ğœ€ â€² ) = 1 + ğ›¼(exp(ğœ€) -1) = 1 + ğ›¼(exp(ğ›¼ -1 Î” U 1 (ğšº)ğ‘ -1 ) -1) = exp(ğœ€ * ). Lemma 15 allows us to show that subsampling cannot improve differential privacy guarantees. Lemma 16. If the Laplacian subsampled sum Ì… ğšº L (X ) from Definition 21 is ğœ€-UDP, then the noisy sum ğšº L (X ) computed with the same feature map and the same noise parameter (but without subsampling) is ğœ€-UDP as well.</p><p>Before we prove Lemma 16, let us just mention that for specific feature maps discussed later, the Laplacian subsampled sum Ì… ğšº L (X ) is in fact just as differentially private as the one computed without subsampling (i.e. ğšº L (X )), while offering flexible tradeoffs between computational complexity and utility.</p><p>Furthermore, note that Lemma 16 also applies to standard Poisson data-subsampling. Hence, the idea that privacy might be "amplified" by subsampling should be mitigated. While the required noise level in order to make the sum of features private is indeed smaller when subsampling, the plain subsampled sum of features still needs to be rescaled by ğ›¼ -1 to obtain a sketch whose utility is comparable with that of the sketch computed using all samples. Overall, with both subsampling strategies (on the samples or on the features), one can at best obtain the same guarantees as when no subsampling is used.</p><p>Proof. Recall the definitions of ğ‘„ U 1 (ğ±) and ğ‘„ U 1 (ğ±, ğ¡) given respectively in Lemma 5 and Equation ( <ref type="formula" target="#formula_48">14</ref>). Using Jensen's inequality and the fact that the masks are drawn according to some ğ‘ ğ¡ âˆŠ P ğ›¼ , we have for any ğ± the lower bound</p><formula xml:id="formula_53">E ğ¡ exp( 1 ğ‘ ğ‘„ U 1 (ğ±, ğ¡)) â‰¥ exp( 1 ğ‘ E ğ¡ ğ‘„ U 1 (ğ±, ğ¡)) = exp( 1 ğ‘ ğ‘„ U 1 (ğ±))</formula><p>According to Lemma 15, taking the supremum on ğ±, we get sup</p><formula xml:id="formula_54">X ,YâˆŠDâˆ¶X U âˆ¼Y sup ğ¬âˆŠZ ğ‘ Ì… ğšº L (X ) (ğ¬) ğ‘ Ì… ğšº L (Y) (ğ¬) = sup ğ±âˆŠğ¸ E ğ¡ exp( 1 ğ‘ ğ‘„ U 1 (ğ±, ğ¡)) â‰¥ sup ğ±âˆŠğ¸ exp( 1 ğ‘ ğ‘„ U 1 (ğ±)) = exp( 1 ğ‘ sup ğ±âˆŠğ¸ ğ‘„ U 1 (ğ±)) = exp( 1 ğ‘ Î” U 1 (ğšº))</formula><p>where the last equality comes from Lemma 5. If Ì… ğšº L (X ) is ğœ€-DP, we thus have exp(</p><formula xml:id="formula_55">1 ğ‘ Î” U 1 (ğšº)) â‰¤ sup X ,YâˆŠDâˆ¶X U âˆ¼Y sup ğ¬âˆŠZ ğ‘ Ì… ğšº L (X ) (ğ¬) ğ‘ Ì… ğšº L (Y) (ğ¬) â‰¤ exp(ğœ€)</formula><p>which means ğ‘ â‰¥ Î” U 1 (ğšº)/ğœ€, hence by Theorem 1, ğšº L (X ) is ğœ€-DP.</p><p>In the following, we denote Ì… ğšº RFF ğ» and Ì… ğšº RQF ğ» the sums of subsampled features when using respectively ğš½ = ğš½ RFF or ğš½ = ğš½ RQF as a feature map. We now provide specific results for these two feature maps.</p></div>
<div><head n="5.1">Random Fourier Features</head><p>The following lemma generalizes the notion of sensitivity to the subsampled case. We include the BDP case which will be used in Section 5.3.</p></div>
<div><head>Lemma 17.</head><p>Consider ğš½ RFF built using nonresonant frequencies, and ğ‘Ÿ âˆŠ âŸ¦1, ğ‘šâŸ§. For each ğ¡ âˆŠ H ğ‘Ÿ we have sup</p><formula xml:id="formula_56">ğ±âˆŠR ğ‘‘ ğ‘„ U 1 (ğ±, ğ¡) = sup ğ±âˆŠR ğ‘‘ inf ğ¡ â€² âˆŠH ğ‘Ÿ ğ‘„ U 1 (ğ±, ğ¡ â€² ) = âˆš 2ğ‘š. sup ğ±,ğ²âˆŠR ğ‘‘ ğ‘„ B 1 (ğ±, ğ², ğ¡) = sup ğ±,ğ²âˆŠR ğ‘‘ inf ğ¡ â€² âˆŠH ğ‘Ÿ ğ‘„ B 1 (ğ±, ğ², ğ¡ â€² ) = 2 âˆš 2ğ‘š.</formula><p>Moreover </p><formula xml:id="formula_57">(X ) (ğ¬) ğ‘ Ì… ğšº RFF L (Y) (ğ¬) = sup ğ±âˆŠğ¸ E ğ¡ exp( 1 ğ‘ ğ‘„ U 1 (ğ±, ğ¡)) = E ğ¡ exp( 1 ğ‘ sup ğ±âˆŠğ¸ ğ‘„ U 1 (ğ±, ğ¡)) = exp( 1 ğ‘ ğ‘š âˆš 2) = exp(ğœ€).</formula><p>The second and third equalities are consequences of Lemma 17, and hold because ğ¡ belongs to H ğ‘Ÿ almost surely.</p></div>
<div><head n="5.2">Random Quadratic Features</head><p>We recall that for random quadratic features,  </p><formula xml:id="formula_58">ğ¸ = B 2 = {ğ‘¥ âˆŠ R ğ‘‘ âˆ¶ â€–ğ‘¥â€– 2 â‰¤ 1}, D ğ‘› â‰œ (B 2 ) ğ‘› ,</formula><formula xml:id="formula_59">= B 2 sup X ,YâˆŠDâˆ¶X U âˆ¼Y sup ğ¬âˆŠZ ğ‘ Ì… ğšº RQF L (X ) (ğ¬) ğ‘ Ì… ğšº RQF L (Y) (ğ¬) = sup ğ±âˆŠğ¸ E ğ¡ exp( 1 ğ‘ ğ‘„ U 1 (ğ±, ğ¡)) â‰¤ sup ğ±âˆŠğ¸ sup ğ¡âˆŠsupp(ğ‘ ğ¡ ) exp( 1 ğ‘ ğ‘„ U 1 (ğ±, ğ¡)) = exp( 1 ğ‘ 1 ğ›¼ sup ğ¡âˆŠsupp(ğ‘ ğ¡ ) â€–Î© ğ¡ â€–<label>2</label></formula></div>
<div><head>ğ‘Ÿ</head><p>) be the block-uniform distribution. For any ğœ€ &gt; 0, releasing Ì… ğšº RQF L (X ) with mask distribution ğ‘ ğ¡ and noise parameter ğ‘ = ğ‘š/(ğ‘‘ğœ€) is ğœ€-UDP, and the bound is sharp.</p><p>Proof. By Lemma 15 and Lemma 21, it follows that sup</p><formula xml:id="formula_60">X ,YâˆŠDâˆ¶X U âˆ¼Y sup ğ¬âˆŠZ ğ‘ Ì… ğšº RQF L (X ) (ğ¬) ğ‘ Ì… ğšº RQF L (Y) (ğ¬) = sup ğ±âˆŠğ¸ E ğ¡ exp( 1 ğ‘ ğ‘„ U 1 (ğ±, ğ¡)) = sup ğ±âˆŠğ¸ exp( 1 ğ‘ ğ‘š ğ‘‘ â€–ğ±â€– 2 2 ) = exp( ğ‘š ğ‘‘ğ‘ ) = exp(ğœ€),</formula><p>where the second equality comes from Lemma 21 as any ğ± for which â€–ğ±â€– 2 = 2 reaches the supremum for all ğ¡ âˆŠ H struct.</p></div>
<div><head>ğ‘Ÿ</head><p>simultaneously.</p><p>Note that the noise level required to get differential privacy when Î© is a union of orthonormal bases is independent of ğ‘Ÿ and is the same as when ğ‘Ÿ = ğ‘š, i.e. without subsampling.</p></div>
<div><head n="5.3">An Upper Bound for Approximate and Bounded Differential Privacy</head><p>Similarly to Definition 21, we define the Gaussian subsampled sum of features.</p></div>
<div><head>Definition 22. The Gaussian subsampled sum of features Ì…</head><p>ğšº G (X ) of a dataset X âˆŠ D ğ‘› using a mask distribution ğ‘ ğ¡ âˆŠ P ğ›¼ and a noise parameter ğœ is the random variable</p><formula xml:id="formula_61">Ì… ğšº G (X ) = ğšº ğ» (X ) + ğ›,<label>(18)</label></formula><p>where âˆ€ğ‘— âˆŠ 1, ğ‘š , ğœ‰ </p><formula xml:id="formula_62">ğ‘— iid âˆ¼ { N C (0, ğœ 2 ) if ğš½ is complex-valued N (0, ğœ</formula><formula xml:id="formula_63">, Y âˆˆ D â€² such that X â‰ˆ Y âˆ€ğ¬ âˆŠ Z âˆ¶ ğ‘ ğ‘… ğ» (X ) (ğ¬) â‰¤ exp(ğœ€)ğ‘ ğ‘… ğ» (Y) (ğ¬) + ğ›¿,<label>(19)</label></formula><p>Note that the sensitivities depend on the neighboring relation used (UDP/BDP), but are always computed for an ordered relation, thus for ğ‘ âˆŠ {1, 2}, we have</p><formula xml:id="formula_64">Î” ğ‘,â‰ˆ (ğšº ğ» ) = ğ‘„ ğ‘ (ğ¡ ğ‘› ) if ğ» = (ğ¡ 1 , â€¦, ğ¡ ğ‘› ).</formula><p>The result follows by taking the expectation of these inequalities, which hold simultaneously for all ğ» provided that ğ‘ â‰¥ max ğ»âˆŠsupp(ğ‘ ğ» ) ğ‘ * ğ» = max ğ¡âˆˆsupp(ğ‘ ğ¡ ) ğ‘„ 1 (ğ¡)/ğœ€ (resp. that ğœ â‰¥ max ğ»âˆŠsupp(ğ‘ ğ» ) ğœ * ğ» = ğœ‚(ğœ€, ğ›¿) max ğ¡âˆˆsupp(ğ‘ ğ¡ ) ğ‘„ 2 (ğ¡)/(2ğœ€) 1/2 ).</p><p>The masks are drawn i.i.d. according to ğ‘ ğ¡ , and for any pair (X , ğ») we have ğšº ğ» (ğœ(X )) = ğšº ğœ -1 (ğ») (X ), thus for any dataset X and permutation ğœ âˆŠ S |X | we have</p><formula xml:id="formula_65">ğ‘ ğ‘…(X ) (ğ¬) = E ğ» [ğ‘ ğ‘… ğ» (X ) (ğ¬)] = E ğ» [ğ‘ ğ‘… ğœ -1 (ğ») (X ) (ğ¬)] = E ğ» [ğ‘ ğ‘… ğ» (ğœ(X )) (ğ¬)]</formula><p>If X and Y are two datasets such that X âˆ¼ Y (we assume for now |X | â‰¥ |Y|), then there are two permutations ğœ 1 âˆŠ S |X | , ğœ 2 âˆŠ S |Y| such that ğœ 1 (X ) â‰ˆ ğœ 2 (Y) for the related ordered relation (it follows from the definition for BDP, and one can take one permutation to be the identity for UDP).</p><p>Thus using the appropriate noise level according to Equation <ref type="bibr" target="#b19">(19)</ref> we have </p><formula xml:id="formula_66">âˆ€X âˆ¼ Y, ğ¬ âˆŠ Z âˆ¶ E ğ» [ğ‘ ğ‘… ğ» (ğœ 1 (X )) (ğ¬)] â‰¤ exp(ğœ€)E ğ» [ğ‘ ğ‘… ğ» (ğœ 2 (Y)) (ğ¬)] + ğ›¿ i.e. âˆ€X âˆ¼ Y, ğ¬ âˆŠ Z âˆ¶ ğ‘ ğ‘…(ğœ 1 (X )) (ğ¬) â‰¤ exp(ğœ€)ğ‘ ğ‘…(ğœ 2 (Y)) (ğ¬) + ğ›¿ i.e. âˆ€X âˆ¼ Y, ğ¬ âˆŠ Z âˆ¶ ğ‘ ğ‘…(X ) (ğ¬) â‰¤ exp(ğœ€)ğ‘ ğ‘…(Y) (ğ¬) +</formula><formula xml:id="formula_67">(ğ›š ğ‘‡ ğ‘– ğ±) 4 ) 1/2 = ğ‘š ğ‘Ÿ ğ‘† 4 (Î© ğ¡ )</formula><p>As ğš½ RQF takes positive values and vanishes in ğŸ, which belongs to ğ¸, the same bound holds for BDP.</p><p>Note that in these two cases, subsampling increases the bounds and might have a negative impact on the utility (for subsequent learning) of the mechanism.</p><p>Table <ref type="table">1</ref>: Summary of privacy results without subsampling (Section 4) and for the sum of features only. For each type of privacy guarantee (column) and for each sketch feature function (row), we provide a potentially loose (â‰¤) or sharp (=) bound on the relevant sensitivity Î”, which can be plugged into the associated privacy-preserving sum of features mechanism (top row). We use the notation ğœ‚ = ğœ‚(ğœ€, ğ›¿), which refers to Theorem 2. (1) With â„ = ğŸ, i.e. Î” 1 (ğšº RFF ) = sup ğ± ğ‘„ B 1 (ğ±, ğŸ) where ğ‘„ B 1 is computed with ğ‘Ÿ = ğ‘š. (2) Using a simple triangle inequality. </p><formula xml:id="formula_68">Pure ğœ€-DP Approximate (ğœ€, ğ›¿)-DP Ì… ğšº L (X ) = ğšº ğ» (X ) + ğ› with ğœ‰ ğ‘— âˆ¼ L(ğ‘), ğ» âˆ¼ ğ‘ ğ‘› ğ¡ Ì… ğšº G (X ) = ğšº ğ» (X ) + ğ› with ğœ‰ ğ‘— âˆ¼ N (0, ğœ</formula><formula xml:id="formula_69">ğ‘ * â‰¤ ğ‘š/(ğ‘‘ğœ–)</formula><p>No particular closed form Table <ref type="table">2</ref>: Summary of privacy results with subsampling (Section 5). For each type of privacy guarantee (column) and for each sketch feature function (row), we provide a potentially loose (â‰¤) or sharp (=) bound on the required additive noise levels (ğ‘ * or ğœ * ). We denote ğœ‚ = ğœ‚(ğœ€, ğ›¿), which refers to Theorem 2, and ğ›¼ the subsampling parameter (and ğ‘Ÿ â‰œ ğ›¼ğ‘š when relevant). (1) Using a simple triangle inequality.</p></div>
<div><head n="6">Utility Guarantees under Differential Privacy</head><p>Having established the differential privacy properties of noisy sketching mechanisms, we conclude this paper by investigating the impact of different aspects of those mechanisms on their utility for subsequent learning (i.e. the quality of the models learned from noisy sketches, as measured by the metrics introduced in Definitions 2, 3 and 5). More precisely, we derive a principled approach to tune various parameters of our mechanism (e.g. the subsampling strategy, the split of the privacy budget, the sketch size) a priori. Given a fixed target privacy level, several choices of parameters are indeed possible, that can each yield a different utility value. Our goal is to pick the best choice of parameters (or at least a promising one), without accessing the data, which would require to allocate a significant part of the privacy budget for parameter tuning.</p><p>We first establish, both from theoretical sketched learning guarantees as well as from numerical simulations, that a proxy comprising of a noise-to-signal ratio (NSR) and the sketch size ğ‘š can qualitatively predict the utility (Section 6.1). The NSR is then computed analytically (Section 6.2) and used to tune some of the parameters of our method: the subsampling strategy (Sections 6.3 and 6.4), the splitting of the privacy budget (Section 6.5), and the choice of the sketch size (Section 6.6, where we exploit the combined influence of the NSR and ğ‘š on the utility).</p></div>
<div><head n="6.1">Noise to signal ratio as a proxy for utility</head><p>We recall (see the beginning of Section 2) that a learning task is defined by a risk function R and a domain H (also known as the hypothesis class), and the parameters one would like to learn are ğ›‰ * âˆˆ arg min ğ›‰âˆˆH R(ğœ‹ 0 , ğ›‰), where ğœ‹ 0 is the true (unknown) distribution of the data. The goal is to estimate -in our case, from the noisy sketch only -a set of parameters Î¸ such that the quantity R(ğœ‹ 0 , Î¸ )-R(ğœ‹ 0 , ğ›‰ * ) , called an excess risk, can be controlled. Previous works <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b10">10]</ref> showed that such a control can be achieved using proof techniques that leverage the analogy between sketching and compressive sensing. Indeed, although the feature map ğš½ is non-linear, sketching is a linear operation w.r.t. distributions, and we denote A the associated operator defined as A(ğœ‹) = E ğ‘¥âˆ¼ğœ‹ Î¦(ğ‘¥). With this notation, the clean "true" sketch would be ğ³ = A(ğœ‹ 0 ), and the clean empirical sketch can be denoted ğ³ X = A(ğœ‹ X ). In practice, we observe a noisy version ğ¬(X ) of the empirical sketch, which can for example be computed as the ratio ğ¬(X ) = (ğšº(X ) + ğ›)/(|X | + ğœ) with ğ› being either Laplacian or Gaussian according to Definitions 17 and 19. As shown in <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b10">10]</ref>, for ğ‘˜-means clustering, Gaussian mixture modeling and PCA, learning from the noisy sketch ğ¬(X ) can be expressed as solving a linear inverse problem on a certain parametric set of probability distributions. Under some assumptions on the sketching function ğš½ and the learning task, the excess risk can be bounded by a quantity that involves a measure of noise level â€–ğâ€– 2 , with ğ â‰œ ğ¬(X ) -ğ³.</p><p>As a proxy for the utility of a noisy sketch, we thus propose the noise-to-signal ratio (NSR), defined as NSR â‰œ â€–ğ¬(X ) -ğ¬â€– 2 2 /â€–ğ¬â€– 2 w.r.t. some reference sketch ğ¬, that will typically be the clean empirical sketch of X , ğ³ X , or the "true" sketch ğ³ of the assumed underlying distribution ğœ‹ 0 .</p><p>The NSR was indeed shown empirically <ref type="bibr" target="#b45">[45]</ref> to be a good proxy to estimate the utility of a sketching mechanism for the task of clustering where performance is measured with the SSE (sum of squared errors) defined in <ref type="bibr" target="#b1">(2)</ref>. Figures <ref type="figure">4</ref> and<ref type="figure">5</ref> give an overview of this correlation. On Figure <ref type="figure">4</ref>, we plot the relative SSE (RSSE, i.e. the ratio between the SSE obtained with centroids determined from a sketch and the SSE obtained with centroids computed using Lloyd's algorithm) for data generated according to Gaussian mixtures with parameters ğ‘˜ = ğ‘‘ = 10, ğ‘š = 10ğ‘˜ğ‘‘. The desired NSR is obtained by adding isotropic noise of controlled magnitude on the clean sketch computed without subsampling. In Figure <ref type="figure">5</ref>, we plot the RSSE for ğ‘› = 10 4 using different sketch sizes and NSRs, again obtained with isotropic noise and without subsampling. The red dashed line corresponds to ğ‘š = 2ğ‘˜ğ‘‘, and as expected <ref type="bibr" target="#b36">[36]</ref> the reconstruction fails below this line. From this plot, we derive that when ğ‘š â‰¥ 2ğ‘˜ğ‘‘, one can consider that the reconstruction is successful provided that NSR â‰¤ ğ‘š/(10 3 ğ‘˜ğ‘‘) â‰œ NSR max (ğ‘š) (yellow area). We thus propose to use NSR-minimization as a criterion to tune the parameters of our method, assuming the sketch size ğ‘š is fixed (we discuss how to select this size in Section 6.6). </p></div>
<div><head n="6.2">Analytical estimation of the noise level</head><p>We now compute in this section the expected noise level (and NSR) induced by the mechanisms introduced in the previous sections and possibly combined with a hybrid dataset subsampling mechanism (i.e., we consider subsampling at the same time features and data samples as explained below).</p><p>Let X be a fixed dataset. The noise level can be measured with respect to the "true" sketch ğ³ of the assumed underlying distribution ğœ‹ 0 , or with respect to the clean empirical sketch ğ³ X . In the first case, which is relevant to take into account the statistical significance due to the size ğ‘› of the dataset, we define ğ â‰œ ğ¬(X ) -ğ³, and the noise level as Eâ€–ğâ€– 2  2 , where the expectation is taken on both the randomness of the mechanism and on the draw of X . We define the noise-to-signal ratio (NSR) as the noise level normalized by the signal energy, i.e. NSR = Eâ€–ğâ€– 2  2 /â€–ğ³â€– 2 2 . When ğ³ X is chosen as the reference signal rather than ğ³, we have NSR = Eâ€–ğ¬(X ) -ğ³ X â€– 2 2 /â€–ğ³ X â€– 2 2 , and the expectation is taken w.r.t. the randomness of the mechanism only.</p><p>Subsampling the dataset. Although we were mainly interested in subsampling the individual features ğš½(ğ± ğ‘– ) when introducing our sampling mechanism in Section 5, we have seen that another straightforward way to reduce the computational complexity is to simply subsample the dataset. The sum of features combining both subsampling strategies is given as</p><formula xml:id="formula_70">Î£(X ) â‰œ 1 ğ›½ ( 1 ğ›¼ ğ‘› âˆ‘ ğ‘–=1 ğ‘” ğ‘– â‹… ğš½(ğ± ğ‘– ) âŠ™ ğ¡ ğ‘– + ğ›),<label>(20)</label></formula><p>where the scalars (ğ‘” ğ‘– ) 1â‰¤ğ‘–â‰¤ğ‘› are in {0, 1} and randomly drawn (i.i.d. Bernoulli with parameter ğ›½, or ğ‘› â€² among ğ‘› without replacement, in which case we define ğ›½ â‰œ ğ‘› â€² /ğ‘›), the masks (ğ¡ ğ‘– ) 1â‰¤ğ‘–â‰¤ğ‘› are drawn as previously i.i.d. according to a distribution ğ‘ ğ¡ âˆŠ P ğ›¼ , and the additive noise ğ› is Laplacian or Gaussian. Note that, when the (ğ‘” ğ‘– ) 1â‰¤ğ‘–â‰¤ğ‘› are drawn i.i.d. according to a Bernoulli distribution, then (20) can be seen as a special case of Definition 21 with ğ¡ â€² ğ‘– = ğ‘” ğ‘– ğ¡ ğ‘– and ğ›¼ â€² = ğ›¼ğ›½. We here clearly dissociate the two sampling strategies, which allows us to consider sampling the data without replacement, but will also make it easier to separate the contributions to the NSR coming from the two sampling strategies. From now on, we consider an estimator ğ¬(X ) of ğ³ X as a function of the sum of features Î£(X ) introduced in (20), which by an adequate choice of the parameters encompasses all the mechanisms previously defined.</p><p>Noise-to-signal ratio when ğ‘› is public. When the dataset size ğ‘› is assumed to be public (e.g. in a BDP setting), we can use the estimator ğ¬(X ) â‰œ Î£(X )/ğ‘›. The following result is proved in Appendix D.</p><p>Lemma 26. The noise-to-signal ratio of the mechanism ğ¬(X ) = Î£(X )/ğ‘› with additive noise of variance ğœ 2 ğœ‰ , features subsampling with parameter ğ›¼ â‰œ ğ‘Ÿ/ğ‘š and i.i.d. Poisson subsampling of the dataset samples with parameter 0 â‰¤ ğ›½ â‰¤ 1 is w.r.t. ğ³:</p><formula xml:id="formula_71">NSR ğ³ = 1 ğ‘› ( 1 ğ›¼ğ›½ E ğ± â€–ğš½(ğ±)â€– 2 â€–ğ³â€– 2 -1) + ğ‘š ğ‘› 2 ğ›½ 2 ğœ 2 ğœ‰ â€–ğ³â€– 2 w.r.t. ğ³ X : NSR ğ³ X = 1 ğ‘› ( 1 ğ›¼ğ›½ -1)( 1 ğ‘› ğ‘› âˆ‘ ğ‘–=1 â€–ğš½(ğ± ğ‘– )â€– 2 ) 1 â€–ğ³ X â€– 2 + ğ‘š ğ‘› 2 ğ›½ 2 ğœ 2 ğœ‰ â€–ğ³ X â€– 2 .</formula><p>The expressions for sampling without replacement differ slightly and are given in the proof in Appendix D.</p><p>Noise-to-signal ratio when ğ‘› is sensitive. When the dataset size is considered sensitive, noise ğœ must be added on ğ‘› for privacy as discussed earlier. Our estimator of the sketch can then be written</p><formula xml:id="formula_72">ğ¬(X ) = Î£(X )ğ‘“(|X | + ğœ), where ğ‘“(|X | + ğœ) is an estimator of 1/|X |. The noise-to-signal ratio is now defined as NSR â‰œ Eâ€– Î£(X )ğ‘“(|X | + ğœ) -ğ¬â€– 2 2 /â€–ğ¬â€– 2 2</formula><p>, where ğ¬ stands for the reference signal, which can again be either ğ³ or ğ³ X . An analytic expression of this NSR is given in Appendix D which involves the bias and variance of the estimator of 1/|X | defined by ğ‘“. Considering an unbiased estimator, a Cramer-Rao lower bound leads to the following result which is proved in Appendix D. Lemma 27. When using an estimator ğ‘“ of 1/ğ‘› computed from the quantity ğ‘›+ğœ, where ğœ âˆ¼ L(0, ğœ ğœ / âˆš 2), a Cramer-Rao bound on the noise-to-signal ratio of the sketching mechanism is</p><formula xml:id="formula_73">NSR ğœ â‰¥ (1 + ğœ 2 ğœ 2ğ‘› 2 )(NSR + 1) -1,</formula><p>where NSR refers to the ratio obtained without ğ‘“ (i.e. when ğœ = 0) as computed in Lemma 26, and can be computed with respect to either ğ³ or ğ³ X .</p></div>
<div><head n="6.3">Comparison of the two subsampling strategies</head><p>For a given dataset size ğ‘›, the sketching cost scales in Î˜(ğ‘›ğ›¼ğ›½) when subsampling the sketches with ğ‘Ÿ = ğ›¼ğ‘š observations (with ğ›¼ â‰¤ 1) and subsampling the dataset by using only ğ‘› â€² = ğ›½ğ‘› samples. Hence for a given ğ‘›, a constant product ğ›¼ğ›½ means a constant computational complexity. We now use Lemma 26 to show that, for an equivalent computational complexity and privacy, subsampling the sketches leads to a better NSR, and hence likely a better utility, than subsampling the dataset.</p><p>For Poisson data subsampling, the only term of the NSR (given in Lemma 26) that varies with (ğ›¼, ğ›½) at constant complexity ğ›¼ğ›½ is the term coming from the additive noise:</p><formula xml:id="formula_74">NSR ğœ‰ â‰œ ğ‘š ğ‘› 2 ğ›½ 2 ğœ 2 ğœ‰ â€–ğ³â€– 2 .<label>(21)</label></formula><p>Note that this holds as well when working with the Cramer-Rao bound from Lemma 27, as the term ğœ ğœ does not depend on ğ›¼, ğ›½ at all. To investigate how this varies we need to take into account that for a fixed target privacy ğœ€, the variance ğœ 2 ğœ‰ also depends on ğ›½. Let us consider the ğœ€-DP setting with random Fourier features as an illustration (a similar reasoning holds for random quadratic features). When the (ğ‘” ğ‘– ) 1â‰¤ğ‘–â‰¤ğ‘› are i.i.d. Bernoulli random variables with parameter ğ›½, then the distribution of the (ğ‘” ğ‘– ğ¡ ğ‘– ) 1â‰¤ğ‘–â‰¤ğ‘› is in P ğ›¼ğ›½ and thus according to Lemma 15, releasing Î£ as defined in <ref type="bibr" target="#b20">(20)</ref> with noise ğ› of parameter ğ‘ ğœ‰ (i.e. with total noise level ğ‘ ğœ‰ /ğ›½ given that ğ› is normalized by when the product ğ›¼ğ›½ is constant. Displayed using <ref type="bibr" target="#b23">(23)</ref> with the convention ğ‘š 3 ğ‘› 2 â€–ğ³â€– 2 = 1 to fix a vertical scale. where (i) follows from Lemma 15 (applied on the mechanism which subsamples only the features and adds Laplacian noise at level ğ‘ ğœ‰ ) and Lemma 18 (assuming non-resonant frequencies and distribution ğ‘ ğ¡ such that ğ¡ âˆˆ H ğ‘Ÿ almost surely, where ğ‘Ÿ = ğ›¼ğ‘š). This is equivalent to saying that ( <ref type="formula" target="#formula_70">20</ref>) is ğœ€-DP provided</p><formula xml:id="formula_75">ğ›½ -1 in (20)) is ğœ€-DP with exp(ğœ€) = sup ğ±âˆŠğ¸ E ğ‘” E ğ¡ exp( ğ›½ ğ‘ ğœ‰ 1 ğ›¼ğ›½ â€–ğš½(ğ±) âŠ™ (ğ‘”ğ¡)â€– 1 ) = sup ğ±âˆŠğ¸ ((1 -ğ›½) â€¢ 1 + ğ›½ â€¢ E ğ¡ exp( 1 ğ‘ ğœ‰ 1 ğ›¼ â€–ğš½(ğ±) âŠ™ ğ¡â€– 1 )) (i) = (1 -ğ›½) + ğ›½ â€¢ exp(ğœ€ â€² ), with ğœ€ â€² = âˆš 2ğ‘š ğ‘ ğœ‰ ,</formula><formula xml:id="formula_76">ğ‘ ğœ‰ = âˆš 2ğ‘š ğœ€ â€² , with ğœ€ â€² = log(1 + (exp(ğœ€) -1)/ğ›½)<label>(22)</label></formula><p>and that this noise level is sharp. The same bound could have been obtained by applying Lemma 14 on our mechanism which subsamples only the features, however here again Lemma 15 additionally proves that the bound is sharp. Given that ğœ 2 ğœ‰ âˆ ğ‘ 2 ğœ‰ , the resulting NSR term is then according to ( <ref type="formula" target="#formula_74">21</ref>) and ( <ref type="formula" target="#formula_76">22</ref>) of the order of</p><formula xml:id="formula_77">NSR ğœ‰ âˆ ğ‘š 3 ğ‘› 2 â€–ğ³â€– 2 1 ğ›½ 2 (ğœ€ â€² ) 2 = ğ‘š 3 ğ‘› 2 â€–ğ³â€– 2 1 ğ›½ 2 log 2 (1 + (exp(ğœ€) -1)/ğ›½) .<label>(23)</label></formula><p>In particular we have NSR ğœ‰ âˆ ğ‘š 3 ğ‘› 2 â€–ğ³â€– 2 1 ğœ€ 2 when ğ›½ = 1. The behavior of this quantity as a function of the Poisson data subsampling rate 0 &lt; ğ›½ &lt; 1 (recall that we consider a constant overall subsampling rate ğ›¼ğ›½) depends on the considered privacy regime. As illustrated on Figure <ref type="figure" target="#fig_7">6</ref>, for each of the three curves: a) when ğœ€ â‰ª ğ›½, we have ğœ€ â€² â‰ˆ ğœ€/ğ›½ and 1/(ğ›½ 2 (ğœ€ â€² ) 2 ) â‰ˆ 1/ğœ€ 2 , thus NSR ğœ‰ is of the same order for ğ›½ â‰ª 1 and ğ›½ = 1, hence the difference between the two subsampling schemes is negligible; b) if ğœ€ â‰« 1 then ğœ€ â€² â‰ˆ ğœ€ and NSR ğœ‰ is thus increased by a factor 1/ğ›½ 2 when subsampling on ğ‘› (i.e. when ğ›½ â‰ª 1, and in comparison with the setting ğ›½ = 1), which might be damageable in terms of utility. Put differently, in light of the expression <ref type="bibr" target="#b23">(23)</ref>, for each privacy parameter ğœ€, the minimum value of 1/(ğ›½ğœ€ â€² ) 2 -the quantity which drives NSR ğœ‰ -is achieved at ğ›½ = 1. The effect on the total NSR for the two sampling scenarios is shown in Figure <ref type="figure" target="#fig_8">7</ref> using the analytic expressions of Lemma 26 and Equation <ref type="bibr" target="#b23">(23)</ref>. This confirms that subsampling the features rather than the samples yields substantial NSR gains for moderate ğœ€ (i.e. neither too large nor too small). For large ğœ€, the noise level is very low anyway, and no difference appears between the two scenarios. Additional experiments (not shown here) show that when sampling the dataset without replacement rather than with Poisson sampling, and measuring the NSR with ğ³ X as a reference signal, subsampling the features can become slightly disadvantaging for large values of ğœ€, but the difference is very small.</p></div>
<div><head n="6.4">Regimes combining privacy and utility</head><p>In this section, we try to highlight the regimes in which the sketches produced by our mechanism are still useful from a learning perspective. We do so by comparing the different contributions to the NSR.</p><p>In light of the results of Section 6.3, we focus on subsampling the features only (i.e. ğ›½ = 1). In this setting, and when working with random Fourier features, since â€–ğš½ RFF (ğ±)â€– 2 = ğ‘š for every ğ±, the different contributions to the NSR (computed with ğ³ as reference) are of the order of</p><formula xml:id="formula_78">NSR X â‰ˆ 1 ğ‘› (ğ¶ 0 -1), NSR ğœ‰ â‰ˆ ğ¶ 0 ğ‘š 2 ğ‘› 2 ğœ€ 2 , NSR ğ» â‰ˆ ğ¶ 0 ğ‘› ( 1 ğ›¼ -1),</formula><p>where ğ¶ 0 â‰œ ğ‘š/â€–ğ³â€– 2 . Using the interpretation of â€–ğ³â€– 2 /ğ‘š as an expected value <ref type="bibr" target="#b27">[27]</ref> the quantity ğ¶ 0 is essentially independent of ğ‘š and satisfies ğ¶ 0 &gt; 1. In practice, empirical simulations on very different datasets suggest that one can safely assume ğ¶ 0 &lt; 10.</p><p>Acceptable noise level without subsampling. The total noise is acceptable when the sum of these contributions to the NSR is smaller than some threshold NSR max , which depends on the sketch size ğ‘š as seen on Figure <ref type="figure">4</ref>. Necessary conditions read:</p><formula xml:id="formula_79">NSR X â‰¤ NSR max â‡” ğ‘› â‰³ 1 NSR max NSR ğœ‰ â‰¤ NSR max â‡” ğ‘› â‰³ ğ‘š ğœ€âˆšNSR max<label>(24)</label></formula><p>Thus utility is preserved (and privacy achieved) when</p><formula xml:id="formula_80">ğ‘› â‰³ max( 1 NSR max , ğ‘š ğœ€âˆšNSR max ).<label>(25)</label></formula><p>Acceptable noise level with subsampling. The noise induced by feature subsampling is acceptable when NSR ğ» â‰² NSR max , i.e., for subsampling with ğ›¼ = 1/ğ‘š, when ğ‘› â‰³ ğ‘š/NSR max . Combining this with the two conditions from the previous paragraph, we conclude that</p><formula xml:id="formula_81">ğ‘› â‰³ ğ‘š max( 1 NSR max , 1 ğœ€âˆšNSR max )<label>(26)</label></formula><p>allows drastic feature subsampling while preserving utility.</p><p>Regime where feature subsampling adds insignificant noise. When</p><formula xml:id="formula_82">NSR ğ» â‰ª max(NSR X , NSR ğœ‰ ) â‡” 1 ğ›¼ â‰ª 1 + max(1, ğ‘š 2 ğ‘›ğœ€ 2 ),<label>(27)</label></formula><p>feature subsampling adds insignificant noise compared to the other noises. When subsampling with parameter ğ›¼ = 1/ğ‘š, this is equivalent to ğ‘› â‰ª ğ‘š ğœ€ 2 . In light of <ref type="bibr" target="#b24">(24)</ref>, one can check that the regime where subsampling noise is insignificant while the total noise is acceptable corresponds to</p><formula xml:id="formula_83">ğ‘š ğœ€âˆšNSR max â‰² ğ‘› â‰ª ğ‘š ğœ€ 2 ,<label>(28)</label></formula><p>which is only feasible when the target privacy satisfies ğœ€ â‰ª âˆšNSR max .</p><p>Example (compressive clustering with random Fourier Features) When performing compressive clustering using random Fourier Features, we observed empirically on Figure <ref type="figure">5</ref> that NSR max â‰ˆ 10 -3 ğ‘š/(ğ‘˜ğ‘‘). Thus the condition <ref type="bibr" target="#b26">(26)</ref> for having an acceptable noise level when subsampling can be rewritten</p><formula xml:id="formula_84">ğ‘› â‰³ max(10 3 ğ‘˜ğ‘‘, âˆš 10 3 ğ‘˜ğ‘‘ğ‘š ğœ€ ).<label>(29)</label></formula><p>Similarly, subsampling with ğ›¼ = 1/ğ‘š will induce an insignificant noise level only when ğœ€ â‰ª âˆš10 -3 ğ‘š/(ğ‘˜ğ‘‘) according to Equation <ref type="bibr" target="#b28">(28)</ref>. This confirms that sketching is compatible with drastic features subsampling for private compressive clustering when working with large collections, but also that subsampling can be perfomed without any impact on the NSR for high privacy levels.</p></div>
<div><head n="6.5">A Heuristic for Privacy Budget Splitting (Laplacian Noise)</head><p>When using unbounded differential privacy, one needs to split the total privacy budget ğœ€ between a budget ğœ€ ğœ‰ â‰œ ğ›¾ğœ€ (where ğ›¾ âˆŠ]0, 1[) used for releasing the sum of sketches Î£, and a budget ğœ€ ğœ â‰œ (1 -ğ›¾)ğœ€ used for releasing the dataset size. This is only needed in the UDP setting since for BDP there is no need to split the privacy budget, given that the dataset size is not considered as sensitive.</p><p>We build a heuristic for the ğœ€-DP setting which consists in choosing ğ›¾ * âˆŠ]0, 1[ minimizing the NSR. In light of Section 6.3, we consider for simplicity ğ›½ = 1, i.e. subsampling is only performed on the features but not on the samples. We further focus on random Fourier features, where â€–ğš½ RFF (ğ±)â€– 2 2 = ğ‘š does not depend on ğ±, leading to a simplified expression of the Cramer-Rao bound on the NSR from Lemma 27:</p><formula xml:id="formula_85">NSR RFF * = (1 + ğœ 2 ğœ 2ğ‘› 2 )(1 - 1 ğ‘› + ğ‘š ğ‘›â€–ğ³â€– 2 ( 1 ğ›¼ + 1 ğ‘› ğœ 2 ğœ‰ )) -1,</formula><p>with ğ³ as the reference signal. By injecting for ğœ </p><formula xml:id="formula_86">when ğ‘› â‰« 1/ğœ€, ğ›¾ * (ğ‘›, ğœ€) â‰ˆ 1 -(ğ‘›ğœ€) -2/3 .</formula><p>In practice, it is important to choose ğ›¾ independently of ğ‘› in order for the whole mechanism to stay private. Given that the NSR only decreases with ğ‘›, we have for any ğœ€ &gt; 0 and any ğ‘› 0 that arg min ğ›¾ max ğ‘›â‰¥ğ‘› 0 NSR(ğ›¾, ğ‘›) = ğ›¾ * (ğ‘› 0 , ğœ€), yielding a simple rule to choose ğ›¾. In light of Section 6.4, in the regime of acceptable noise levels we have Note that this is only a heuristic, allowing to choose ğ›¾ independently of ğ‘› but optimized for the worstcase scenario with acceptable utility; even if ğ‘› &lt; ğ‘› 0 the mechanism will be guaranteed to be private (although with limited utility).</p></div>
<div><head n="6.6">Choice of the Sketch Size</head><p>Because the noise level depends on the sketch size ğ‘š, the design of a sketching procedure becomes delicate since overestimating ğ‘š decreases the performance, unlike in the non-private case where increasing ğ‘š usually only helps. As an illustration of this fact, consider the numerical experiment represented Figure <ref type="figure" target="#fig_11">8</ref> (top row), where we estimate the relative SSE (RSSE) achieved by compressive k-means (CKM) from the ğœ€-DP sketch as a function of its size ğ‘š. Relative SSE is defined as the ratio between the method SSE, as defined in Equation ( <ref type="formula" target="#formula_2">2</ref>), and the SSE of Lloyd's standard kmeans algorithm, which is not private nor compressive. As expected, in the non-private setting the SSE decreases monotonically with ğ‘š. However, when ğœ€ &lt; âˆ and ğ‘› is moderate, increasing ğ‘š (and thus the noise, which is proportional to ğ‘š according to Lemma 7) results in a worse SSE at some point. This phenomenon is more pronounced when the privacy constraints are higher, i.e. a smaller ğœ€ induces a smaller range of "optimal" values for the sketch size. There is thus a trade-off to make between revealing enough information for CKM to succeed (ğ‘š large enough) and not revealing too much information, such that the noise needed to ensure the privacy guarantee is not too penalizing, this trade-off being more difficult in the high privacy regime. This behavior can be explained by the observations of Section 6.4 (paragraph "acceptable noise level") relative to the NSR. We consider for conciseness here that no subsampling is used (i.e. NSR ğ» = 0) and 10 -0.5 10 0.0 10 0.5 10 1.0 10 1.  ğœ€-DP (ğ›¿ = 0). Given that utility is measured w.r.t. the RSSE (which is relative to the optimal error for the given dataset, but agnostic to the true data distribution), we take ğ³ X as the reference signal to compute the NSR, i.e. we have NSR X = 0. Utility is then preserved provided that NSR ğœ‰ â‰¤ NSR max , which according to <ref type="bibr" target="#b29">(29)</ref> translates to the condition ğ‘› â‰¥ âˆš 1000ğ‘˜ğ‘‘ğ‘š/ğœ€. Recall that we also need ğ‘š â‰¥ 2ğ‘˜ğ‘‘ as shown in Section 6.1. These conditions can be rewritten 2 â‰¤ ğ‘š/(ğ‘˜ğ‘‘) â‰¤ ğ‘› 2 ğœ€ 2 /(10 3 (ğ‘˜ğ‘‘) 2 ), which is possible only when ğ‘› â‰¥ âˆš 2 Ã— 10 3 ğ‘˜ğ‘‘/ğœ€. In Figure <ref type="figure" target="#fig_11">8</ref> we have ğ‘˜ = 4, ğ‘‘ = 8, thus this requirement translates respectively for ğ‘› = 10 4 , 10 5 , 10 6 to the conditions ğœ€ â‰¥ 0.14, ğœ€ â‰¥ 0.014, ğœ€ â‰¥ 0.0014, which correspond quite well to what is observed (top row).</p><p>As shown on Figure <ref type="figure" target="#fig_11">8</ref> (bottom), relaxing the privacy constraint to allow ğ›¿ &gt; 0 mitigates the impact of ğ‘š on the noise to add (recall from theorem 11 that the noise is then proportional to âˆš ğ‘š instead of ğ‘š), and that even for smaller values of ğ‘›. This relaxation has the clear advantage of improving the utility for similar values of ğ‘› and ğœ€ even for small ğ›¿, and also facilitates the choice of ğ‘š, as good utilities can be reached on a wider range of sketch sizes.</p></div>
<div><head n="7">Discussion and Perspectives</head><p>We proposed a framework to learn from potentially massive datasets using limited computational resources, while ensuring the differential privacy of the data providers. Beside being promising as privacyinducing mechanism in terms of privacy, our framework has several key interesting features compared to other methods from the literature, that are discussed here together with main limitations and perspectives.</p><p>Efficient and Distributed Learning Firstly, the computational advantages of non-private sketching <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b36">36]</ref> remain valid after our addition of a privacy layer. In particular, learning from the sketch can be done with time and space complexities which do not depend on the number of samples ğ‘› in the dataset. Moreover, the sketching process is embarassingly parallel due to the averaging operation. Sketches coming from several separate data holders can thus be aggregated again after sketching, providing distributed differential privacy for free, without any need for a trusted central party.</p><p>Versatility Another advantage is that the sketch, acting as a surrogate for the whole dataset, contains more information than just the output of one specialized algorithm, and can thus be used multiple times. This can be leveraged to solve different learning tasks from a same sketch without breaking privacy, assuming that those tasks can be solved using the same sketching operator <ref type="bibr" target="#b27">[27]</ref>. This is what we already observed for random Fourier features, which can be used for both ğ‘˜-means clustering and fitting a Gaussian mixture model, two different but related estimation problems.</p><p>This potential versatility of the sketch also allows to run the learning algorithm with different initializations and parameters, producing multiple solutions; the distance to the empirical sketch can be used as a metric to pick the best of these solutions. This is in contrast with usual (e.g. iterative) differentially private methods that can be highly sensitive to the choice of such parameters (which have to be selected a priori, as accessing the data for parameter tuning breaks the privacy guarantee). Of course the devil is in the details, and further studies are needed to investigate to what extent it is possible to choose parameters such as the sketch dimension or the "scale parameter" of random Fourier features (see below) so as to combine privacy, utility and versatility. Preliminary investigations indicate that one can find sketch sizes enabling to achieve good utility for both compressive gaussian mixture modeling and compressive k-means with ğ›¿ = 10 -8 and ğœ€ above or equal to 10 -1.5</p><p>Open challenges Although the sketch serves as a general-purpose synopsis of the dataset, at least some a priori knowledge about the data distribution and/or the target task is required when designing the sketch feature map ğš½ âˆ¶ ğ± â†¦ ğ‘“(Î© ğ‘‡ ğ±). We discussed how the nonlinearity ğ‘“ must be selected according to the desired task, and explained in Section 6.6 that the choice of the sketch size ğ‘š could be seen as a trade-off between performance and privacy. Going for approximate DP mitigates this difficulty. Another crucial point is the choice of the frequencies distribution for Fourier features (Î© is drawn i.i.d. Gaussian in the PCA setting, and this concern does not apply in that case). Even when the general shape of the frequency distribution is selected and only a single scale parameter ğœ has to be pinned down (ğœ essentially controls the scale at which we can detect individual clusters), estimating an appropriate value for it is not straightforward. This might be a limitation to using sketching in practice but, on the other side, any heuristic that could be developed in the future to estimate ğœ should be easy to make private as it releases a single scalar value.</p><p>Perspectives Finally, we expect that compressive learning will be extended to more learning tasks in future works. The private sketching framework presented here would be directly transferable to those new algorithms, although the sketch sensitivity would have to be re-computed for novel feature functions. The true potential of private sketching will depend on how well the general field of compressive learning will be able to answer this challenge in the coming years.</p></div>
<div><head>A Results on Nonresonant Frequencies</head><p>In order to prove the sharpness of the sensitivity computed in Lemma 7, we rely on some results from diophantine approximation theory. We recall the definition of nonresonant frequencies. Definition 23. The vectors (ğœ” ğ‘— ) 1â‰¤ğ‘—â‰¤ğ‘š âˆŠ R are called nonresonant frequencies if they are linearly independent over the rationals. The vectors (ğ›š ğ‘— ) 1â‰¤ğ‘—â‰¤ğ‘š âˆŠ R ğ‘‘ are called nonresonant frequency vectors if there exists a vector ğ¯ âˆŠ R ğ‘‘ such that the scalars (ğ›š ğ‘‡ ğ‘— ğ¯) 1â‰¤ğ‘—â‰¤ğ‘š are nonresonant frequencies. Before proving Lemma 6, we introduce a variant of the result in dimension 1.</p><p>Lemma 29. Let (ğœ” ğ‘— , ğœ‘ ğ‘— ) 1â‰¤ğ‘—â‰¤ğ‘š be real numbers, and ğ‘“ a 2ğœ‹-periodic function such that there exists ğ‘§ at which ğ‘“ is continuous and reaches its maximum. If the (ğœ” ğ‘— ) 1â‰¤ğ‘—â‰¤ğ‘š are linearly independent over the rationals, then sup ğ‘¥âˆŠâ„ inf ğ‘—âˆŠ 1;ğ‘š ğ‘“(ğœ” ğ‘— ğ‘¥ -ğœ‘ ğ‘— ) = sup ğ‘¥âˆŠR ğ‘“(ğ‘¥).</p><p>Proof. Let ğ‘§ âˆŠ [0, 2ğœ‹[ be a point at which ğ‘“ reaches its maximum, i.e., ğ‘§ âˆˆ arg max [0,2ğœ‹[ ğ‘“(ğ‘§), and at which ğ‘“ is continuous. Using this continuity assumption, the result amounts to saying that one can find ğ‘¡ âˆŠ R such that the (ğœ” ğ‘— ğ‘¡ -ğœ‘ ğ‘— -ğ‘§) 1â‰¤ğ‘—â‰¤ğ‘š are simultaneously arbitrary close to 2ğœ‹Z. Denoting ğ‘‘(ğ‘¥, ğ‘†) = inf{|ğ‘¥ -ğ‘ | âˆ¶ ğ‘  âˆŠ ğ‘†}, this is equivalent to saying that for any ğœ€ &gt; 0, we can find a real ğ‘¡ such that we simultaneously have ğ‘‘((ğœ” ğ‘— ğ‘¡ -ğœ‘ ğ‘— -ğ‘§)/(2ğœ‹), Z) &lt; ğœ€ for all ğ‘— âˆŠ 1, ğ‘š . This derives directly from Kronecker's theorem <ref type="bibr" target="#b37">[37]</ref> on diophantine approximation, given that the ğœ” ğ‘— /(2ğœ‹) are linearly independent over the rationals.</p><p>Proof of Lemma 6. We propose to convert the problem to its one-dimensional counterpart. sup</p><formula xml:id="formula_87">ğ±âˆŠR ğ‘‘ inf ğ‘—âˆŠ 1;ğ‘š ğ‘“(ğ›š ğ‘‡ ğ‘— ğ± -ğœ‘ ğ‘— ) = sup ğ¯âˆŠR ğ‘‘ âˆ¶â€–ğ¯â€–=1 sup ğ‘¥âˆŠR inf ğ‘—âˆŠ 1;ğ‘š ğ‘“(ğ‘¥ğ›š ğ‘‡ ğ‘— ğ¯ -ğœ‘ ğ‘— )<label>(30)</label></formula><p>Let ğ¯ be such that the scalars ğ‘ ğ‘— â‰œ ğ›š ğ‘‡ ğ‘— ğ¯ (for 1 â‰¤ ğ‘— â‰¤ ğ‘š) are nonresonant, which exists because the vectors (ğ›š ğ‘— ) 1â‰¤ğ‘—â‰¤ğ‘š are themselves nonresonant. The quantity <ref type="bibr" target="#b30">(30)</ref>  where the last equality comes from Lemma 29, the ğ‘ ğ‘— being nonresonant.</p></div>
<div><head>B Results without subsampling</head><p>Proof of Lemma 12. We have</p><formula xml:id="formula_88">Î” B 2 (ğšº RFF ) 2 = sup ğ±,ğ²âˆˆR ğ‘‘ â€–ğš½(ğ±) -ğš½(ğ²)â€– 2 2 = sup ğ±,ğ²âˆˆR ğ‘‘ ğ‘š âˆ‘ ğ‘—=1 |ğš½(ğ±) ğ‘— -ğš½(ğ²) ğ‘— | 2 = sup ğ±,ğ²âˆˆR ğ‘‘ ğ‘š âˆ‘ ğ‘—=1 |(ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— ) + ğ‘–ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— -ğœ‹ 2 )) -(ğœŒ(ğ›š ğ‘‡ ğ‘— ğ² + ğ‘¢ ğ‘— ) + ğ‘–ğœŒ(ğ›š ğ‘‡ ğ‘— ğ² + ğ‘¢ ğ‘— -ğœ‹ 2 ))| 2 = sup ğ±,ğ²âˆˆR ğ‘‘ ğ‘š âˆ‘ ğ‘—=1 (ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— ) -ğœŒ(ğ›š ğ‘‡ ğ‘— ğ² + ğ‘¢ ğ‘— )) 2 + (ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— -ğœ‹ 2 ) -ğœŒ(ğ›š ğ‘‡ ğ‘— ğ² + ğ‘¢ ğ‘— -ğœ‹ 2 )) 2 = sup ğ±,ğ²âˆˆR ğ‘‘ ğ‘š âˆ‘ ğ‘—=1 2(1 -(ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— )ğœŒ(ğ›š ğ‘‡ ğ‘— ğ² + ğ‘¢ ğ‘— ) + ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— -ğœ‹ 2 )ğœŒ(ğ›š ğ‘‡ ğ‘— ğ² + ğ‘¢ ğ‘— -ğœ‹ 2 ))<label>(31)</label></formula><p>â€¢ For unquantized features, we have ğœŒ = cos and ğœŒ(â€¢ -ğœ‹ 2 ) = sin, hence </p><formula xml:id="formula_89">Î” B 2 (ğšº RFF ) 2 = sup ğ±,ğ²âˆˆR ğ‘‘ ğ‘š âˆ‘ ğ‘—=1<label>2</label></formula><formula xml:id="formula_90">ğ² + ğ‘¢ ğ‘— -ğœ‹ 2 ) = 2 -1/2 inf ğ²âˆˆR ğ‘‘ ğ‘š âˆ‘ ğ‘—=1 Â±ğœŒ(ğ›š ğ‘‡ ğ‘— ğ² + ğ‘¢ ğ‘— ) Â± ğœŒ(ğ›š ğ‘‡ ğ‘— ğ² + ğ‘¢ ğ‘— -ğœ‹ 2 ) = 2 -1/2 inf ğ²âˆˆR ğ‘‘ ğ‘š âˆ‘ ğ‘—=1 ğ‘“(ğ›š ğ‘‡ ğ‘— ğ² + ğ‘¢ ğ‘— + ğœ‘ ğ‘— ) = -2 -1/2 sup ğ²âˆˆR ğ‘‘ ğ‘š âˆ‘ ğ‘—=1 ğ‘“ ğœ‹-ğ›—-ğ® (ğ›š ğ‘‡ ğ‘— ğ²) = -ğ‘š</formula><p>which is independent of the choice of ğ± and concludes the proof.</p></div>
<div><head>C Proofs on Sketching with Subsampling</head></div>
<div><head>C.1 General results</head><p>Proof of Lemma 15. We define the permutation of a set of masks as ğœ((ğ¡ 1 , â€¦, ğ¡ ğ‘› )) = (ğ¡ ğœ(1) , â€¦, ğ¡ ğœ(ğ‘›) ) for ğœ âˆŠ S ğ‘› . For any set of masks ğ» âˆŠ H ğ‘› , and any dataset X such that |X | = ğ‘›, we denote ğ‘ X (â€¢|ğ») = ğ‘ ğšº L,ğ» (X ) (â€¢) the density of ğšº L,ğ» (X ). Unless otherwise specified, ğ‘ X denotes the density of Ì… ğšº L (X ). We prove the result for a real-valued feature map ğš½, and discuss the complex case at the end of the proof. We will prove that sup </p><formula xml:id="formula_91">(ğ¬) = E ğ» [ğ‘ X (ğ¬|ğ»)] = E ğ» [ğ‘ X (ğ¬|ğœ -1 (ğ»))] = E ğ» [ğ‘ ğœ(X ) (ğ¬|ğ»)] = E ğ» ğ‘›-1 ,ğ¡ ğ‘› [ğ‘ ğœ(X ) (ğ¬|ğ» ğ‘›-1 , ğ¡ ğ‘› )] = E ğ¡ ğ‘› E ğ» ğ‘›-1 [ğ‘ ğœ(X ) (ğ¬|ğ» ğ‘›-1 , ğ¡ ğ‘› )] ğ‘ Y (ğ¬) = E ğ» ğ‘›-1 [ğ‘ Y (ğ¬|ğ» ğ‘›-1 )]</formula><p>As a consequence we have</p><formula xml:id="formula_92">ğ‘ X (ğ¬) ğ‘ Y (ğ¬) = E ğ¡ ğ‘› E ğ» ğ‘›-1 [ğ‘ ğœ(X ) (ğ¬|ğ» ğ‘›-1 , ğ¡ ğ‘› )] E ğ» ğ‘›-1 [ğ‘ Y (ğ¬|ğ» ğ‘›-1 )] = E ğ¡ ğ‘› E ğ» ğ‘›-1 exp(-1 ğ‘ â€–ğ¬ -Ì… ğšº ğ» ğ‘›-1 ,ğ¡ ğ‘› (ğœ(X ))â€– 1 ) E ğ» ğ‘›-1 exp(-1 ğ‘ â€–ğ¬ -Ì… ğšº ğ» ğ‘›-1 (Y)â€– 1 )<label>(32)</label></formula><p>Note that for any ğ» ğ‘›-1 , ğ¡ ğ‘› we have Ì… ğšº ğ» ğ‘›-1 ,ğ¡ ğ‘› (ğœ(X )) = Ì… ğšº ğ» ğ‘›-1 (Y) + 1 ğ›¼ ğš½(ğ± ğ‘› ) âŠ™ ğ¡ ğ‘› by definition of ğœ and thus for any ğ» ğ‘›-1 , ğ¡ ğ‘› , ğ¬ we have</p><formula xml:id="formula_93">-â€–ğ¬ -Ì… ğšº ğ» ğ‘›-1 ,ğ¡ ğ‘› (ğœ(X ))â€– 1 = -â€–ğ¬ -Ì… ğšº ğ» ğ‘›-1 (Y) - 1 ğ›¼ ğš½(ğ± ğ‘› ) âŠ™ ğ¡ ğ‘› â€– 1 â‰¤ -â€–ğ¬ -Ì… ğšº ğ» ğ‘›-1 (Y)â€– 1 + â€– 1 ğ›¼ ğš½(ğ± ğ‘› ) âŠ™ ğ¡ ğ‘› â€– 1 .<label>(33)</label></formula><p>Equality holds iff for all ğ‘— âˆŠ âŸ¦1, ğ‘šâŸ§, (ğ¬ -Ì… ğšº ğ» ğ‘›-1 ,ğ¡ ğ‘› (ğœ(X ))) ğ‘— and (ğš½(ğ± ğ‘› ) âŠ™ ğ¡ ğ‘› ) ğ‘— have the same sign or any of the two terms is null. Define ğ‘ â‰œ max 1â‰¤ğ‘–â‰¤ğ‘› â€–ğš½(ğ± ğ‘– )â€– âˆ . For any choice of binary masks ğ» ğ‘›-1 , we have</p><formula xml:id="formula_94">â€– Ì… ğšº ğ» ğ‘›-1 ,ğ¡ ğ‘› (ğœ(X ))â€– âˆ â‰¤ ğ‘›ğ‘ 1 ğ›¼ â‰œ ğ‘€.</formula><p>In particular, if we define s â‰œ ğ‘€ sign(ğš½(ğ± ğ‘› )), where sign is applied pointwise, s yields equality in Equation ( <ref type="formula" target="#formula_93">33</ref>) for all ğ» ğ‘›-1 , ğ¡ ğ‘› simultaneously. Using Equation <ref type="bibr" target="#b33">(33)</ref> in Equation ( <ref type="formula" target="#formula_92">32</ref>) and taking the supremum over ğ¬, we get</p><formula xml:id="formula_95">sup ğ¬âˆŠZ ğ‘ X (ğ¬) ğ‘ Y (ğ¬) â‰¤ sup ğ¬âˆŠZ E ğ¡ ğ‘› E ğ» ğ‘›-1 exp(-1 ğ‘ â€–ğ¬ -Ì… ğšº ğ» ğ‘›-1 (Y)â€– 1 + 1 ğ‘ â€– 1 ğ›¼ ğš½(ğ± ğ‘› ) âŠ™ ğ¡ ğ‘› â€– 1 ) E ğ» ğ‘›-1 exp(-1 ğ‘ â€–ğ¬ -Ì… ğšº ğ» ğ‘›-1 (Y)â€– 1 ) = E ğ¡ ğ‘› exp( 1 ğ‘ 1 ğ›¼ â€–ğš½(ğ± ğ‘› ) âŠ™ ğ¡ ğ‘› â€– 1 )</formula><p>but we also have</p><formula xml:id="formula_96">sup ğ¬âˆŠZ ğ‘ X (ğ¬) ğ‘ Y (ğ¬) â‰¥ ğ‘ X ( s ) ğ‘ Y ( s ) = E ğ¡ ğ‘› exp( 1 ğ‘ 1 ğ›¼ â€–ğš½(ğ± ğ‘› ) âŠ™ ğ¡ ğ‘› â€– 1 ),</formula><p>therefore equality holds.</p><formula xml:id="formula_97">Case |X | = |Y| -1 We assumed so far |X | = |Y| + 1, but now if |X | + 1 = |Y| = ğ‘›, there is ğœ such that ğœ(Y) U â‰ˆ X and we have Ì… ğšº ğ» ğ‘›-1 (X ) = Ì… ğšº ğ» ğ‘›-1 ,ğ¡ ğ‘› (ğœ(Y)) -1 ğ›¼ ğš½(ğ² ğ‘› ) âŠ™ ğ¡ ğ‘› . Another triangular inequality yields -â€–ğ¬ -Ì… ğšº ğ» ğ‘›-1 (X )â€– 1 = -â€–ğ¬ -Ì… ğšº ğ» ğ‘›-1 ,ğ¡ ğ‘› (ğœ(Y)) + 1 ğ›¼ ğš½(ğ² ğ‘› ) âŠ™ ğ¡ ğ‘› â€– 1 â‰¤ -â€–ğ¬ -Ì… ğšº ğ» ğ‘›-1 ,ğ¡ ğ‘› (ğœ(X ))â€– 1 + â€–- 1 ğ›¼ ğš½(ğ² ğ‘› ) âŠ™ ğ¡ ğ‘› â€– 1 .<label>(34)</label></formula><p>Using Jensen's inequality (all quantities are positive and ğ‘¥ â†¦ 1/ğ‘¥ is convex on R + ) we get </p><formula xml:id="formula_98">ğ‘ X (ğ¬) ğ‘ Y (ğ¬) = ğ‘ X (ğ¬) ğ‘ ğœ(Y) (ğ¬) = E ğ» ğ‘›-1 exp(-1 ğ‘ â€–ğ¬ -Ì… ğšº ğ» ğ‘›-1 (X ))â€– 1 E ğ¡ ğ‘› E ğ» ğ‘›-1 exp(-1 ğ‘ â€–ğ¬ -Ì… ğšº ğ» ğ‘›-1 ,ğ¡ ğ‘› (ğœ(Y))â€– 1 ) â‰¤ E ğ¡ ğ‘› E ğ» ğ‘›-1 exp(-1 ğ‘ â€–ğ¬ -Ì… ğšº ğ» ğ‘›-1 (X )â€– 1 ) E ğ» ğ‘›-1 exp(-1 ğ‘ â€–ğ¬ -Ì… ğšº ğ» ğ‘›-1 ,ğ¡ ğ‘› (ğœ(Y))â€– 1 ) â‰¤ E ğ¡ ğ‘› E ğ» ğ‘›-1 exp(-1 ğ‘ â€–ğ¬ -Ì… ğšº ğ» ğ‘›-1 ,ğ¡ ğ‘› (Y)â€– 1 ) exp( 1 ğ‘ â€– 1 ğ›¼ ğš½(ğ² ğ‘› ) âŠ™ ğ¡ ğ‘› â€– 1 ) E ğ» ğ‘›-1 exp(-1 ğ‘ â€–ğ¬ -Ì… ğšº ğ» ğ‘›-1 ,ğ¡ ğ‘› (ğœ(Y))â€– 1 ) = E ğ¡ ğ‘› exp(<label>1</label></formula><formula xml:id="formula_99">(ğ¬) = sup ğ±âˆŠğ¸ E ğ¡ ğ‘› exp( 1 ğ‘ 1 ğ›¼ â€–ğš½(ğ±) âŠ™ ğ¡ ğ‘› â€– 1 ) = sup ğ±âˆŠğ¸ E ğ¡ exp( 1 ğ‘ ğ‘„ U 1 (ğ±, ğ¡)),</formula><p>which concludes the proof.</p><p>Complex case If ğš½ is complex, the same proof holds using the canonical isomorphism between C ğ‘š and R 2ğ‘š . Indeed, an equivalent of Equation ( <ref type="formula" target="#formula_92">32</ref>) can be established using Definition 11 of a complex Laplace random variable. The triangle inequality Equation <ref type="bibr" target="#b33">(33)</ref> holds in a similar manner by considering complex and real parts independently, and s can be defined as s = ğ‘€ (sign(â„œğš½(ğ± ğ‘› )) + ğ‘– sign(â„‘ğš½(ğ± ğ‘› ))).</p><p>We get sup</p><formula xml:id="formula_100">X ,YâˆŠDâˆ¶X U âˆ¼Y sup ğ¬âˆŠZ ğ‘ X (ğ¬) ğ‘ Y (ğ¬) = sup ğ±âˆŠğ¸ E ğ¡ ğ‘› exp( 1 ğ‘ 1 ğ›¼ (â€–â„œğš½(ğ±) âŠ™ ğ¡ ğ‘› â€– 1 + â€–â„‘ğš½(ğ±) âŠ™ ğ¡ ğ‘› â€– 1 )) = sup ğ±âˆŠğ¸ E ğ¡ exp( 1 ğ‘ ğ‘„ U 1 (ğ±, ğ¡)),</formula><p>which concludes the proof.</p></div>
<div><head>C.2 Random Fourier Features</head><p>Proof of Lemma 17. This proof bears strong similarities with the proof of Lemma 7, and we therefore use the same notations and tools. In particular, we recall that ğ‘“(â€¢) â‰œ ğœŒ(â€¢) + ğœŒ(â€¢ -ğœ‹/2), and that sup ğ‘¥âˆˆR ğ‘“(ğ‘¥) = âˆš 2 for both complex exponential case and one-bit quantization. We also denote supp(ğ¡) = {ğ‘— âˆŠ âŸ¦1, ğ‘šâŸ§ | â„ ğ‘— â‰  0} the support of ğ¡.</p><p>By analogy with Equations ( <ref type="formula" target="#formula_33">10</ref>) and ( <ref type="formula" target="#formula_35">11</ref>), but summing only on the frequencies that appear in the mask ğ¡, denoting ğ‘“ ğ›—,ğ¡ (ğ±) â‰œ âˆ‘ ğ‘—âˆŠsupp(ğ¡) ğ‘“(ğ›š ğ‘‡ ğ‘— ğ± -ğœ‘ ğ‘— ), the quantities ğ‘„ U 1 (ğ±, ğ¡) and ğ‘„ B 1 (ğ±, ğ², ğ¡) can be expressed as ğ‘› -ğ‘› â€² ğ‘› -1 .</p><formula xml:id="formula_101">ğ‘„ U 1 (ğ±, ğ¡) = 1 ğ›¼ âˆ‘ ğ‘—âˆŠsupp(ğ¡) |ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— )| + |ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— - ğœ‹ 2 )| = 1 ğ›¼ sup ğ›—âˆŠ{0,ğœ‹/2,</formula><p>We can now give the proof.</p><p>Proof of Lemma 26. We define the error as ğ â‰œ ğ¬(X ) -ğ¬ for some reference signal ğ¬, which can be either ğ³ X or the true sketch ğ³. The noise level is Eâ€–ğâ€– 2  2 , and the noise-to-signal ratio is defined as NSR = Eâ€–ğâ€– 2  2 /â€–ğ¬â€– 2 2 . In these expressions, the expectations are taken w.r.t. the randomness of the sketching mechanism when ğ³ X is chosen as the reference signal, and w.r.t. both the randomness of the mechanism and the draw of X when ğ³ is the reference signal. We denote ğšº the clean sum of features, ğ‘› â€² = ğ›½ğ‘›, ğšº ğ‘› â€² the sum of features computed on a random subset of the collection, ğšº ğ»,ğ‘› â€² the mechanism combining both types of subsampling, i.e. Thus the error can be decomposed as</p></div>
<div><head>ğšº(X ) =</head><formula xml:id="formula_102">ğ = 1 ğ‘› Î£(X ) -ğ¬ = 1 ğ‘› Î£(X ) -ğ¬ âŸâŸâŸâŸâŸ ğ X + 1 ğ‘› â€² ğšº ğ‘› â€² (X ) -1 ğ‘› Î£(X ) âŸâŸâŸâŸâŸâŸâŸâŸâŸ ğ ğ‘› â€² + 1 ğ‘› â€² (ğšº ğ»,ğ‘› â€² (X ) -ğšº ğ‘› â€² (X )) âŸâŸâŸâŸâŸâŸâŸâŸâŸâŸâŸ ğ ğ» + 1 ğ‘› â€² ğ› âŸ ğ ğœ‰</formula><p>.</p><p>We now estimate the noise level of each of these components separately.</p><p>Without noise nor subsampling. When no noise is added (ğ› = ğœ = 0), and all features of all samples are used (ğ‘Ÿ = ğ‘š, no subsampling), then ğ¬(X ) = ğ³ X = A(ğœ‹ X ) = ğšº(X ) |X | . When the true sketch is chosen as the reference signal, we have:</p><formula xml:id="formula_103">ğ X = 1 ğ‘› âˆ‘ ğš½(ğ± ğ‘– ) -ğ³ E X ğ X = 0 â€–ğ X â€– 2 2 = â€–ğ³ X -ğ³â€– 2 2 E X â€–ğ X â€– 2 2 = ğ‘š âˆ‘ ğ‘—=1 Var X ( 1 ğ‘› ğ‘› âˆ‘ ğ‘–=1 ğš½(ğ± ğ‘– ) ğ‘— ) = 1 ğ‘› ğ‘š âˆ‘ ğ‘—=1</formula><p>Var ğ± (ğš½(ğ±) ğ‘— )</p><formula xml:id="formula_104">E X â€–ğ X â€– 2 2 = 1 ğ‘› (E ğ± â€–ğš½(ğ±)â€– 2 -â€–ğ³â€– 2 )</formula><p>If ğ³ X is chosen as the reference signal, then ğ X = 0, Eâ€–ğ X â€– 2 2 = 0 .</p><p>Additive noise (for privacy). The noise contribution due to the additive noise is ğ ğœ‰ = ğ›/ğ‘› â€² , thus</p><formula xml:id="formula_105">E ğœ‰ ğ ğœ‰ = 0 E ğœ‰ â€–ğ ğœ‰ â€– 2 2 = 1 (ğ‘› â€² ) 2 ğ‘šE[ğœ‰ 2 ğ‘– ] E ğœ‰ â€–ğ ğœ‰ â€– 2 2 = ğ‘š (ğ‘› â€² ) 2 ğœ 2 ğœ‰</formula><p>and is independent from the reference signal. Here ğœ 2  ğœ‰ is the noise level such that the whole mechanism (including the sampling step) is ğœ€-DP. It is thus computed using a privacy level ğœ€ â€² = log(1+(exp(ğœ€)-1)/ğ›½).</p></div>
<div><head>Samples subsampling</head><p>We consider here the noise contribution due to the dataset subsampling operation. We have</p><formula xml:id="formula_106">ğ ğ‘› â€² = 1 ğ‘› â€² ğšº ğ‘› â€² (X ) -1 ğ‘› ğšº(X ) E ğ  ğ ğ‘› â€² = 0</formula><p>The noise level here depends on the subsampling strategy. We consider two cases</p><p>â€¢ sampling of ğ‘› â€² samples out of ğ‘› without replacement (denoted WOR(ğ‘›, ğ‘› â€² )):</p><formula xml:id="formula_107">E ğ âˆ¼WOR(ğ‘›,ğ‘› â€² ) â€–ğ ğ‘› â€² â€– 2 = ğ‘š âˆ‘ ğ‘—=1 Var ğ  ( 1 ğ‘› â€² ğ‘› âˆ‘ ğ‘–=1 ğ‘” ğ‘– ğš½(ğ± ğ‘– ) ğ‘— ) = ğ‘š âˆ‘ ğ‘—=1 ( âˆ‘ ğ‘› ğ‘–=1 |ğš½(ğ± ğ‘– ) ğ‘— -(ğ³ X ) ğ‘— | 2 ğ‘›ğ‘› â€² ğ‘› -ğ‘› â€² ğ‘› -1</formula><p>) by Lemma 30</p><formula xml:id="formula_108">= 1 ğ‘›ğ‘› â€² ğ‘› -ğ‘› â€² ğ‘› -1 ğ‘› âˆ‘ ğ‘–=1 â€–ğš½(ğ± ğ‘– ) -ğ³ X â€– 2 2 E ğ âˆ¼WOR(ğ‘›,ğ‘› â€² ) â€–ğ ğ‘› â€² â€– 2 = 1 ğ‘› -1 ( ğ‘› ğ‘› â€² -1)( 1 ğ‘› ğ‘› âˆ‘ ğ‘–=1 â€–ğš½(ğ± ğ‘– )â€– 2 2 -â€–ğ³ X â€– 2 2 )</formula><p>Taking the expectation with respect to the draw of X as well we obtain Taking the expectation with respect to the draw of X as well we obtain</p><formula xml:id="formula_109">E X E ğ âˆ¼Bern(ğ›½) ğ‘› â€–ğ ğ‘› â€² â€– 2 = 1 ğ‘› ( 1 ğ›½ -1)E ğ± â€–ğš½(ğ±)â€– 2 2</formula><p>Frequencies subsampling. We define the noise contribution due to frequency subsampling as Thus the noise-to-signal ratio NSR ğœ of the whole mechanism (including noise ğœ) can be written as a function of the noise-to-signal ratio of Î£(X )/ğ‘› as computed in Lemma 26 (i.e. using the same parameters but without ğœ), which we denote simply NSR in the rest of the proof. </p><formula xml:id="formula_110">ğ</formula><p>For an unbiased estimator ğ‘“ (if there exists any), we have (Eğ‘“) 2 = 1/ğ‘› 2 and the variance can be bounded via a Cramer-Rao bound.</p><p>A bound on the variance of ğ‘“. Remember that ğœ is drawn as ğœ âˆ¼ L(0, ğ‘). We want to estimate ğœƒ = 1/ğ‘› from an observation ğ‘¥ drawn with probability density (and log-density)</p><p>ğ‘ ğœƒ (ğ‘¥) = 1 2ğ‘ ğœ ğ‘’ </p></div>
<div><head>E Heuristic for Splitting the Privacy Budget</head><p>Proof of Lemma 28. The noise level for ğœ is ğ‘ ğœ = 1/ğœ€ ğœ = 1/((1 -ğ›¾)ğœ€) for Laplacian noise according to Lemma 4. In the Laplacian-UDP setting, the lowest noise level yielding ğœ€-DP is ğœ ğœ‰ = 2ğ‘ = 2 âˆš 2ğ‘š/(ğ›¾ğœ€) (complex Laplace distribution). We then have</p><formula xml:id="formula_112">NSR RFF * = (1 + 1 ğ‘› 2 (1 -ğ›¾) 2 ğœ€ 2 )(1 - 1 ğ‘› + ğ‘š ğ‘›â€–ğ³â€– 2 ( 1 ğ›¼ + 1 ğ‘› 8ğ‘š 2 ğ›¾ 2 ğœ€ 2 )) -1,</formula><p>For succinctness in the derivation, denote ğ´ = 1/(ğ‘› 2 ğœ€ 2 ), ğµ = 1 -1/ğ‘› + ğ‘š 2 /(ğ‘›ğ‘Ÿâ€–ğ³â€– 2 ) and ğ¶ = where ğ´ğµ/ğ¶ = (1 -</p><formula xml:id="formula_113">1 ğ‘› 2 â€–ğ³â€– 2</formula><formula xml:id="formula_114">1 ğ‘› + ğ‘š 2 ğ‘›ğ‘Ÿâ€–ğ³â€– 2 ) â€–ğ³â€– 2 ğ‘š 1 8ğ‘š 2 â‰ª 1.</formula><p>Note that, if we start from the expression of the NSR which takes ğ³ X as a reference signal, we would get ğµ = 1-1/ğ‘›+ğ‘š 2 /(ğ‘›ğ‘Ÿâ€–ğ³â€– 2 ), but the same approximation would still hold. The only real root of ğ›¾ 3 -3ğ›¾ 2 +(2ğ´+3)ğ›¾-(ğ´+1) can be computed as ğ›¾ * = 1-1 3 ğ¸+ 2ğ´ ğ¸ = 1 + 6ğ´-ğ¸ 2 3ğ¸ , where</p><formula xml:id="formula_115">ğ¸ = 1 2 1/3 (27ğ´ + 3 âˆš 81ğ´ 2 + 96ğ´ 3 ) 1/3</formula><p>In this setting where ğœ€ â‰ª 1/ğ‘›, ğ´ â‰« 1 and we can use the following approximation.</p><formula xml:id="formula_116">ğ›¾ * = 1 + 6ğ´ -ğ¸ 2 3ğ¸ â‰ˆ 1 + 6ğ´ -6ğ´(1 + 27ğ´ 3 âˆš 96ğ´ 3/2 ) 2/3 âˆš 6ğ´ 1/2 â‰ˆ 1 2 .</formula><p>On the other side, if ğ´ â‰ª 1, we get ğ¸ â‰ˆ 3ğ´ 1/3 and ğ›¾ * = 1 + 6ğ´ -ğ¸ 2 3ğ¸ â‰ˆ 1 + 6ğ´ -9ğ´ 2/3 9ğ´ 1/3 â‰ˆ 1 -ğ´ 1/3 .</p></div><figure xml:id="fig_0"><head>ğ± 1 Figure 1 :</head><label>11</label><figDesc>Figure 1: Overview of sketching and parameter learning.</figDesc></figure>
<figure xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Attack model. The dataset is distributed between ğ¿ devices, each computing and releasing publicly a subsampled sketch ğ¬(X ğ‘– ).</figDesc></figure>
<figure xml:id="fig_2"><head /><label /><figDesc>according to Theorem 1. Releasing |X | has sensitivity 1, and thus releasing |X | + ğœ with ğœ âˆ¼ L(ğœ€ -1</figDesc></figure>
<figure xml:id="fig_3"><head /><label /><figDesc>ğœ‹,3ğœ‹/2} ğ‘“(â‹… -ğœ‘). Indeed, both ğœŒ = cos and ğœŒ = 2 -1/2 sign âˆ˜ cos satisfy the property âˆ€ğ‘¡ âˆ¶ ğœŒ(ğ‘¡) = -ğœŒ(ğ‘¡ -ğœ‹), hence for each ğ‘¡ âˆˆ R: +ğœŒ(ğ‘¡) + ğœŒ(ğ‘¡ -ğœ‹/2) = ğ‘“(ğ‘¡) +ğœŒ(ğ‘¡) -ğœŒ(ğ‘¡ -ğœ‹/2) = ğœŒ(ğ‘¡) + ğœŒ(ğ‘¡ + ğœ‹/2) = ğ‘“(ğ‘¡ + ğœ‹/2) -ğœŒ(ğ‘¡) -ğœŒ(ğ‘¡ -ğœ‹/2) = -ğ‘“(ğ‘¡) = ğ‘“(ğ‘¡ + ğœ‹) -ğœŒ(ğ‘¡) + ğœŒ(ğ‘¡ -ğœ‹/2) = -ğ‘“(ğ‘¡ + ğœ‹/2) = ğ‘“(ğ‘¡ + 3ğœ‹/2).</figDesc></figure>
<figure xml:id="fig_4"><head>Lemma 11 .</head><label>11</label><figDesc>The function ğšº RFF has sensitivity Î” U 2 (ğšº RFF ) = âˆš ğ‘š for both quantized and unquantized cases.</figDesc></figure>
<figure xml:id="fig_5"><head>Lemma 12 .Figure 3 :</head><label>123</label><figDesc>Figure 3: Overview of the sketching mechanism from Definition 21 with subsampling.</figDesc></figure>
<figure xml:id="fig_6"><head>Figure 4 : 1 Figure 5 :</head><label>415</label><figDesc>Figure 4: Correlation between relative (w.r.t. k-means with 3 trials) SSE and NSR. Medians of 100 trials and variance.</figDesc></figure>
<figure xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Variation of NSR ğœ‰ as a function of ğœ€ for different values of the data subsampling parameter ğ›½. This quantity is the only variable term of the NSR (cf Lemma 26) at constant computational complexity, i.e. when the product ğ›¼ğ›½ is constant. Displayed using<ref type="bibr" target="#b23">(23)</ref> with the convention ğ‘š 3 ğ‘› 2 â€–ğ³â€– 2 = 1 to fix a vertical scale.</figDesc></figure>
<figure xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Total NSR vs total subsampling factor ğ›¼ğ›½. (plain) pure feature subsampling (ğ›¼ âˆŠ [1/ğ‘š, 1], ğ›½ = 1); (dashed) pure data subsampling (ğ›¼ = 1, ğ›½ âˆŠ [1/ğ‘›, 1], Poisson sampling). All curves computed with analytic expressions of NSR, for ğ‘š = 10 3 , ğ‘› = 10 4 .</figDesc></figure>
<figure xml:id="fig_9"><head /><label /><figDesc>ğ‘› â‰³ ğ‘› 0 (ğ‘š, ğœ€) â‰œ ğ‘š max ( hence a possible heuristic is to choose ğ›¾(ğ‘š, ğœ€) â‰œ ğ›¾ â‹† (ğ‘› 0 (ğ‘š, ğœ€), ğœ€) â‰ˆ 1 -(ğ‘› 0 ğœ€) -2/3 .</figDesc></figure>
<figure xml:id="fig_10"><head>2 ğ‘› = 10 6 ,</head><label>26</label><figDesc>ğ›¿ = 10 -8</figDesc></figure>
<figure xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Performance of differentially private compressive k-means as a function of ğ‘š for ğ›¿ = 0 (top) and ğ›¿ = 10 -8 (bottom), ğ‘› = 10 4 , 10 5 , 10 6 and different values of ğœ€. Medians over 200 trials. Synthetic data, ğ‘˜ = 4, ğ‘‘ = 8.</figDesc></figure>
<figure xml:id="fig_12"><head /><label /><figDesc>is upper-bounded by sup ğ‘¥âˆˆR ğ‘“(ğ‘¥) = ğ‘“(ğ‘§), and can be lower-bounded by sup ğ‘¥âˆŠR inf ğ‘—âˆŠ 1;ğ‘š ğ‘“(ğ‘¥ğ‘ ğ‘— -ğœ‘ ğ‘— ) = sup ğ‘¥âˆŠR ğ‘“(ğ‘¥)</figDesc></figure>
<figure xml:id="fig_13"><head>ğ‘”</head><label /><figDesc>ğ‘– ğš½(ğ± ğ‘– ) ğšº ğ»,ğ‘› â€² (X ) = 1 ğ›¼ ğ‘› âˆ‘ ğ‘–=1 ğ‘” ğ‘– (ğš½(ğ± ğ‘– ) âŠ™ ğ¡ ğ‘– ) ğ¬(X ) = 1 ğ‘› â€² (ğšº ğ»,ğ‘› â€² (X ) + ğ›).</figDesc></figure>
<figure xml:id="fig_14"><head>ğ» = 1 ğ‘›ğ‘” 2 +</head><label>12</label><figDesc>â€² (ğšº ğ»,ğ‘› â€² (X ) -ğšº ğ‘› â€² (X ))where:ğšº ğ»,ğ‘› â€² = ğ‘š ğ‘Ÿ ğ‘› âˆ‘ ğ‘–=1 ğ‘” ğ‘– (ğš½(ğ± ğ‘– ) âŠ™ ğ¡ ğ‘– ) ğšº ğ‘› â€² = ğ‘› âˆ‘ ğ‘–=1 ğ‘” ğ‘– ğš½(ğ± ğ‘– ) E ğ» â€–ğ ğ» â€– ğ‘– â€–ğš½(ğ± ğ‘– )â€– 2Recall that the masks entries are in {0, 1}, thus âˆ€ğ‘–, ğ‘— (ğ¡ ğ‘– ) 2 ğ‘— = (ğ¡ ğ‘– ) ğ‘— , but also âˆ€ğ‘— E ğ¡âˆ¼ğ‘ ğ¡ ğ¡ ğ‘— = ğ‘Ÿ/ğ‘š because ğ‘ ğ¡ âˆŠ P ğ›¼ . Therefore we haveVar(ğ¡ ğ‘– ) ğ‘— = E ((ğ¡ ğ‘– ) ğ‘— ) 2 -(E (ğ¡ ğ‘– ) ğ‘— ) 2 = E (ğ¡ ğ‘– ) ğ‘— -ğ‘“ 2 )Var( Î£(X ) ğ‘— ) + Var(ğ‘“)|ğ¬ ğ‘— | 2 ğ‘› 2 ]= E(ğ‘“ 2 )ğ‘› 2 Eâ€– Î£(X )/ğ‘› -ğ¬â€– 2 Var(ğ‘“)â€–ğ¬â€– 2 ğ‘› 2</figDesc></figure>
<figure xml:id="fig_15"><head>NSR ğœ = E(ğ‘“ 2 )</head><label>2</label><figDesc>ğ‘› 2 NSR + Var(ğ‘“)ğ‘› 2 = ((Eğ‘“) 2 + Var(ğ‘“))ğ‘› 2 NSR + (Eğ‘“) 2 ğ‘› 2 Var(ğ‘“) (Eğ‘“) 2 = (Eğ‘“) 2 ğ‘› 2 [(1 + Var(ğ‘“) (Eğ‘“) 2 )(NSR + 1) -1].</figDesc></figure>
<figure xml:id="fig_16"><head>- 2 ğ‘ 1 = 2 ğœ2ğ‘› 2 )</head><label>2122</label><figDesc>ğœƒ (ğ‘¥)) = -log(2ğ‘ ğœ )Using the Cramer-Rao bound for an unbiased estimator ğ‘“, we haveVar(ğ‘“) â‰¥ E[( ğ‘‘(log ğ‘ ğœƒ (ğ‘¥)) ğ‘‘ğœƒ ) ğœƒ (ğ‘¥)ğ‘‘ğ‘¥] -ğ‘ 2 ğœ ğœƒ 4 = ğ‘ 2 ğœ /ğ‘› 4 = ğœ 2 ğœ /(2ğ‘› 4 ) = (Eğ‘“) 2 ğœ 2 ğœ /(2ğ‘› 2 ).ConclusionCombining this bound with Equation (35) yields for an unbiased estimator of minimal variance (if there exists any) NSR ğœ â‰¥ (1 + ğœ (NSR + 1) -1.</figDesc></figure>
<figure xml:id="fig_17"><head>8ğ‘š 3 ğœ€ 2 ,</head><label>32</label><figDesc>so that we try to minimize NSR RFF * Note that NSR RFF * diverges to +âˆ when ğ›¾ â†’ 0 + or ğ›¾ â†’ 1 -, but is continuous on ]0, 1[. Any minimizer on ]0, 1[ must cancel the quantity1 2ğ¶ ğ›¾ 3 (1 -ğ›¾) 3 ğ‘‘NSR RFF * ğ¶ğ›¾ 3 + ğ´ğ›¾ -(1 -ğ›¾) 3 -ğ´(1 -ğ›¾)= (ğ´ğµ/ğ¶ + 1)ğ›¾ 3 -3ğ›¾ 2 + (2ğ´ + 3)ğ›¾ -(ğ´ + 1)</figDesc></figure>
<figure type="table" xml:id="tab_0"><head /><label /><figDesc><ref type="bibr" target="#b0">1</ref> , â€¦, ğ± ğ‘› )) = (ğ± ğœ(1) , â€¦, ğ± ğœ(ğ‘›) ) for ğœ âˆŠ S ğ‘› . ğ± ğ‘– = ğ² ğ‘– ) and ğ± ğ‘› is arbitrary).</figDesc><table><row><cell cols="4">Definition 9. An algorithm provides ğœ€-unbounded DP (UDP) iff it provides ğœ€-DP for the "removal"</cell></row><row><cell>neighborhood relation</cell><cell cols="2">U âˆ¼, defined as</cell></row><row><cell /><cell>X</cell><cell>U âˆ¼ Y â‡” {</cell><cell>||X | -|Y|| = 1 (we can assume w.l.o.g. |X | = |Y| + 1 â‰œ ğ‘› â‰¥ 2) âˆƒ ğœ âˆŠ S |X | s.t. ğœ(X ) U â‰ˆ Y,</cell></row><row><cell cols="4">where (ğ± 1 , â€¦, ğ± ğ‘› ) â‰ˆ (ğ² 1 , â€¦, ğ² ğ‘›-1 ) â‡” ((âˆ€ğ‘– âˆŠ âŸ¦1, ğ‘› -1âŸ§, Definition 10. An algorithm provides ğœ€-bounded DP (BDP) iff it provides ğœ€-DP for the "replacement" U</cell></row><row><cell>neighborhood relation</cell><cell /><cell /></row></table><note><p><p>B</p>âˆ¼:</p></note></figure>
<figure type="table" xml:id="tab_1"><head /><label /><figDesc>Definition 11 (Complex Laplace distribution). A random variable ğ‘§ follows a centered complex Laplace distribution of parameter ğ‘ (denoted ğ‘§ âˆ¼ L C (ğ‘)) iff its real and imaginary parts follow independently a real Laplace distribution of parameter ğ‘. In that case, ğ‘§ admits a density ğ‘ ğ‘§ (ğ‘§) âˆ exp(-(|â„œğ‘§| + |â„‘ğ‘§|)/ğ‘) and has variance ğœ 2 ğ‘§ = E[|ğ‘§| 2 ] = 4ğ‘ 2 . Definition 12 (Laplace Mechanism). For any function ğ‘“ âˆ¶ D â†’ R ğ‘š (resp. C ğ‘š ), the Laplace mechanism with parameter ğ‘ âˆŠ R is the random mechanism X â†¦ ğ‘“(X ) + ğ› where (ğœ‰ ğ‘– ) 1â‰¤ğ‘–â‰¤ğ‘š</figDesc><table><row><cell>iid</cell></row></table></figure>
<figure type="table" xml:id="tab_2"><head /><label /><figDesc>if they are linearly independent over the rationals. The vectors (ğ›š ğ‘— ) 1â‰¤ğ‘—â‰¤ğ‘š âˆŠ R ğ‘‘ are called nonresonant frequency vectors if there exists a vector ğ¯ âˆŠ R ğ‘‘ such that the scalars (ğ›š ğ‘‡ The proof is in Appendix A. We can now compute the desired sensitivity. The function ğšº RFF built using ğ‘š frequencies has sensitivity Î” U 1 (ğšº RFF ) â‰¤ ğ‘š âˆš 2 for both quantized and unquantized cases. If the frequencies are non resonant then Î” U 1</figDesc><table><row><cell>Lemma 7.</cell></row></table><note><p>ğ‘— ğ¯) 1â‰¤ğ‘—â‰¤ğ‘š are nonresonant frequencies. Lemma 6. Let (ğœ‘ ğ‘— ) 1â‰¤ğ‘—â‰¤ğ‘š be real numbers, (ğ›š ğ‘— ) 1â‰¤ğ‘—â‰¤ğ‘š âˆŠ R ğ‘‘ nonresonant frequencies, and ğ‘“ a 2ğœ‹periodic function such that there exists ğ‘§ at which ğ‘“ is continuous and reaches its maximum. Then sup ğ±âˆŠR ğ‘‘ inf ğ‘—âˆŠ 1;ğ‘š ğ‘“(ğ›š ğ‘‡ ğ‘— ğ± -ğœ‘ ğ‘— ) = sup ğ‘¥âˆŠR ğ‘“(ğ‘¥).</p></note></figure>
<figure type="table" xml:id="tab_3"><head /><label /><figDesc>âˆ˜ cos, ğ‘“ is a piecewise constant function taking values 0, When the frequencies (ğ›š ğ‘— ) 1â‰¤ğ‘—â‰¤ğ‘š are nonresonant, ğ‘“ being 2ğœ‹ periodic and admitting (in both quantized/unquantized cases) a point ğ‘§ âˆŠ R at which it reaches its maximum and is continuous, we apply Lemma 6 and get according to Lemma 5:</figDesc><table><row><cell>have sup ğ‘¥âˆŠR ğ‘“(ğ‘¥) = that Î” U 1 (ğšº RFF ) â‰¤ ğ‘š</cell><cell>âˆš âˆš 2 as claimed. 2. We obtain sup ğ±âˆˆR ğ‘‘ ğ‘“ ğ›—-ğ® (ğ±) â‰¤ ğ‘š</cell><cell>âˆš</cell><cell>âˆš 2 for any ğ›—, ğ® hence, by Lemma 5, we get 2, 0, -âˆš 2. Thus in both cases we</cell></row></table></figure>
<figure type="table" xml:id="tab_4"><head>1.2 Random Quadratic Features In</head><label /><figDesc>The function ğšº RQF built using a matrix of frequenciesÎ© = [ğ›š 1 , â€¦, ğ›š ğ‘š ], has sensitivity Î” U 1 (ğšº RQF ) = â€–Î©â€– 2 2where â€– â€¢ â€– 2 denotes the spectral norm. Proof. Let ğœ† max denote the largest eigenvalue function. We have according toLemma 5    </figDesc><table /><note><p>Lemma 8. Frequency vectors drawn i.i.d. according to a distribution which is absolutely continuous w.r.t. the Lebesgue measure are almost surely nonresonant. Proof. The set of resonant frequencies has a zero Lebesgue measure. The reader can refer to [52, Corollary 9.3 p. 166] for a proof relying on strong incommensurability. 4.this section only, we restrict ourselves to datasets whose elements are bounded by 1 in ğ¿ 2 -norm. The domain is thus ğ¸ = B 2 â‰œ {ğ‘¥ âˆŠ R ğ‘‘ âˆ¶ â€–ğ‘¥â€– 2 â‰¤ 1}, and we still use the notations D ğ‘› â‰œ ğ¸ ğ‘› and D â‰œ âˆª ğ‘›âˆŠN D ğ‘› . Lemma 9.</p></note></figure>
<figure type="table" xml:id="tab_5"><head /><label /><figDesc>1 , ğœ€ 2 &gt; 0 such that ğœ€ 1 + ğœ€ 2 = ğœ€, if ğšº G has finite ğ¿ 2 sensitivity Î” U 2 (ğšº), then any mechanism to estimate ğ³ X using ğšº G (X ) with noise level ğœ = ğœ‚(ğœ€ 1 , ğ›¿) â‹… Î” U 2 (ğšº)/âˆš2ğœ€ 1 (where ğœ‚ refers to Theorem 2), and |X | + ğœ where ğœ âˆ¼ L(ğœ€ -1 2 ), is ğœ€-UDP. Proof. The Gaussian mechanism applied on ğšº with ğœ = ğœ‚(ğœ€ 1 , ğ›¿)Î” U 2 (ğšº)/âˆš2ğœ€ 1 is (ğœ€ 1 , ğ›¿)-UDP according to Theorem 2. As in lemma 4, releasing |X | + ğœ with ğœ âˆ¼ L(ğœ€ -1</figDesc><table /></figure>
<figure type="table" xml:id="tab_6"><head /><label /><figDesc>1 and denote P ğ›¼ the set of probability distributions ğ‘ ğ¡ on H satisfying âˆ€ğ‘— âˆŠ âŸ¦1, ğ‘šâŸ§ E ğ¡âˆ¼ğ‘ ğ¡ ğ¡ ğ‘— = ğ›¼. Particular examples of probability distributions belonging to P ğ›¼ include â€¢ Poisson feature sampling: the distribution (Bern (ğ›¼)) ğ‘š , corresponding to masks which ğ‘š entries are drawn i.i.d. according to a Bernoulli distribution with parameter ğ›¼; â€¢ Poisson data sampling: the masks are ğœ’ğŸ with ğœ’ âˆ¼ Bern (ğ›¼). This corresponds to subsampling the data rather than the features, which is a well known strategy as discussed above.</figDesc><table /><note><p>â€¢ Uniform feature sampling: the uniform distribution U(H ğ‘Ÿ ) over</p></note></figure>
<figure type="table" xml:id="tab_7"><head /><label /><figDesc>The bound is sharp if ğš½ RFF is built using nonresonant frequencies.</figDesc><table><row><cell cols="4">Proof. By Lemma 15 and Lemma 17 we have</cell><cell /></row><row><cell>sup X ,YâˆŠDâˆ¶X U âˆ¼Y</cell><cell cols="2">ğ¬âˆŠZ sup</cell><cell>ğ‘ Ì… ğšº RFF L</cell><cell /></row><row><cell>ğ‘„ U 1 (ğ±, ğ¡) â‰¤</cell><cell>âˆš</cell><cell cols="2">2ğ‘š and ğ‘„ B 1 (ğ±, ğ², ğ¡) â‰¤ 2</cell><cell>âˆš</cell><cell>2ğ‘š always hold, even for resonant frequencies.</cell></row><row><cell cols="6">The proof is quite similar to the proof of Lemma 7, and can be found in Appendix C.</cell></row><row><cell cols="6">We can now state the main result for random Fourier features.</cell></row></table><note><p>Lemma 18. Consider ğ‘Ÿ âˆŠ âŸ¦1, ğ‘šâŸ§, ğ›¼ = ğ‘Ÿ/ğ‘š, and a probability distribution ğ‘ ğ¡ âˆŠ P ğ›¼ such that ğ¡ âˆˆ H ğ‘Ÿ almost surely. Then for any ğœ€ &gt; 0, Ì… ğšº RFF L (X ) from Definition 21 with noise level ğ‘ = âˆš 2ğ‘š/ğœ€ and mask distribution ğ‘ ğ¡ is ğœ€-UDP.</p></note></figure>
<figure type="table" xml:id="tab_8"><head /><label /><figDesc>and D â‰œ âˆª ğ‘›âˆŠN D ğ‘› . We give a generic upper bound in Lemma 20, and below in Lemma 22 a sharp bound when Î© is a union of orthonormal bases. We first provide a simple lemma used in both results. For any mask ğ¡ âˆŠ H ğ‘Ÿ with ğ‘Ÿ non-zero entries at indexes ğ‘– 1 , â€¦, ğ‘– ğ‘Ÿ , and any matrix of frequencies Î© = [ğ›š 1 , â€¦, ğ›š ğ‘š ] âˆŠ R ğ‘‘Ã—ğ‘š , we denote Î© ğ¡ = [ğ›š ğ‘– 1 , â€¦, ğ›š ğ‘– ğ‘Ÿ ] the matrix obtained from Î© by keeping only the columns corresponding to nonzero indexes of ğ¡.</figDesc><table><row><cell /><cell cols="2">sup ğ±âˆŠğ¸</cell><cell cols="5">ğ‘„ U 1 (ğ±, ğ¡) = sup ğ±,ğ²âˆŠğ¸</cell><cell cols="2">ğ‘„ B 1 (ğ±, ğ², ğ¡) =</cell><cell>1 ğ›¼</cell><cell>â€–Î© ğ¡ â€– 2 2 .</cell></row><row><cell>Proof.</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell>sup ğ±,ğ²âˆŠğ¸</cell><cell cols="2">ğ‘„ B 1 (ğ±, ğ², ğ¡) = sup ğ±,ğ²âˆŠğ¸</cell><cell>1 ğ›¼</cell><cell cols="2">ğ‘—âˆŠsupp(ğ¡) âˆ‘</cell><cell cols="4">|(ğ›š ğ‘‡ ğ‘— ğ‘¥) 2 -(ğ›š ğ‘‡ ğ‘— ğ‘¦) 2 |</cell></row><row><cell /><cell>= sup ğ±âˆŠğ¸</cell><cell>1 ğ›¼</cell><cell cols="2">âˆ‘ ğ‘—âˆŠsupp(ğ¡)</cell><cell cols="4">(ğ›š ğ‘‡ ğ‘— ğ‘¥) 2 = sup ğ±âˆŠğ¸</cell><cell>ğ‘„ U 1 (ğ±, ğ¡)</cell></row><row><cell /><cell>= sup ğ±âˆŠğ¸</cell><cell>1 ğ›¼</cell><cell cols="4">ğ± ğ‘‡ ( âˆ‘ ğ‘—âˆŠsupp(ğ¡)</cell><cell cols="3">ğ›š ğ‘— ğ›š ğ‘‡ ğ‘— )ğ± =</cell><cell>1 ğ›¼</cell><cell>ğœ† max ( âˆ‘ ğ‘—âˆŠsupp(ğ¡)</cell><cell>ğ›š ğ‘— ğ›š ğ‘‡ ğ‘— ) =</cell><cell>1 ğ›¼</cell><cell>â€–Î© ğ¡ â€– 2 2 .</cell></row></table><note><p>Lemma 19. Consider the functions ğ‘„ U 1 , ğ‘„ B 1 associated to the feature map ğš½ RQF . For each ğ¡ âˆˆ H For any ğ‘ ğ¡ âˆŠ P ğ›¼ , we denote supp(ğ‘ ğ¡ ) the support of ğ‘ ğ¡ , that is the set of possible outcomes of ğ¡ âˆ¼ ğ‘ ğ¡ . Lemma 20. Let ğ‘ ğ¡ âˆŠ P ğ›¼ . For any ğœ€ &gt; 0, releasing Ì… ğšº RQF L (X ) from Definition 21 with noise parameter ğ‘ = ğ‘š ğ‘Ÿğœ€ sup ğ¡âˆŠsupp(ğ‘ ğ¡ ) â€–Î© ğ¡ â€– 2 2 and mask distribution ğ‘ ğ¡ is ğœ€-UDP. Proof. By Lemma 15, with ğ¸</p></note></figure>
<figure type="table" xml:id="tab_9"><head /><label /><figDesc>Consider ğ‘š a multiple of ğ‘‘ and Î© a concatenation of ğ‘š/ğ‘‘ orthonormal bases as described in Section 2.2. Let ğ‘Ÿ be a multiple of ğ‘‘, and ğ¡ âˆŠ H struct. Let us rewrite Î© = [ğµ 1 , â€¦, ğµ ğ‘š/ğ‘‘ ] where the (ğµ ğ‘– ) 1â‰¤ğ‘–â‰¤ğ‘š/ğ‘‘ are ğ‘‘ Ã— ğ‘‘ blocs corresponding to orthonormal bases. We have Î©Î© ğ‘‡ = âˆ‘ ğ‘š/ğ‘‘ ğ‘–=1 ğµ ğ‘– ğµ ğ‘‡ ğ‘– = ğ‘š/ğ‘‘ ğˆ ğ‘‘ . As ğ¡ âˆŠ H struct. ğ¡ Î© ğ‘‡ ğ¡ = (ğ‘Ÿ/ğ‘‘) ğˆ ğ‘‘ . As a result, for any ğ± âˆˆ ğ¸ we have ğ‘„ U 1 (ğ±, ğ¡) = 1 ğ›¼ (ğ‘Ÿ/ğ‘‘)â€–ğ±â€– 2 2 = (ğ‘š/ğ‘‘)â€–ğ±â€– 2 2 and the result follows from ğ¸ = B 2 . Given that ğš½ RFF takes only positive values and vanishes in 0, we have sup ğ±,ğ²âˆŠğ¸ ğ‘„ B 1 (ğ±, ğ², ğ¡) = sup ğ±,ğ²âˆŠğ¸ â€–ğš½ RFF (ğ±) -ğš½ RFF (ğ²)â€– 1 = sup ğ±âˆŠğ¸ â€–ğš½ RFF (ğ±)â€– 1 = ğ‘„ U 1 (ğ±, ğ¡). Lemma 22. Consider ğ‘š a multiple of ğ‘‘ and Î© a concatenation of ğ‘š/ğ‘‘ orthonormal bases as described in Section 2.2. Let ğ‘Ÿ be a multiple of ğ‘‘, and ğ‘ ğ¡ = U(H struct.</figDesc><table><row><cell>Proof.</cell><cell /><cell /><cell /></row><row><cell /><cell>2 ),</cell><cell /><cell /></row><row><cell>by Lemma 19, which concludes the proof.</cell><cell /><cell /><cell /></row><row><cell cols="4">Whether or when the bound of Lemma 20 is sharp in general is an open question. The finer bound E ğ¡ exp( 1 ğ‘ 1 2 ) holds, but does not yield explicit guarantees. A sharp and explicit bound can be ğ›¼ â€–Î© ğ¡ â€– 2 achieved in a specific case of interest.</cell></row><row><cell>Lemma 21. ğ±,ğ²âˆŠğ¸</cell><cell>ğ‘„ B 1 (ğ±, ğ², ğ¡) =</cell><cell>ğ‘š ğ‘‘</cell><cell>.</cell></row></table><note><p><p><p><p>ğ‘Ÿ</p>. Then for any ğ± such that â€–ğ±â€– 2 = 1, we have</p>ğ‘„ U 1 (ğ±, ğ¡) = sup ğ±âˆŠğ¸ ğ‘„ U 1 (ğ±, ğ¡) = sup ğ‘Ÿ</p>, we have for the same reason Î©</p></note></figure>
<figure type="table" xml:id="tab_10"><head /><label /><figDesc>Although we do not have an equivalent of Lemma 15 for approximate DP, we provide in Lemma 23 a generic upper bound, which holds for both pure and approximate DP, bounded and unbounded DP. In order to do so, we introduce the following definitions for ğ‘ âˆŠ {1, 2} and ğ¡ âˆˆ H Let ğ‘ ğ¡ âˆŠ P ğ›¼ be a mask distribution.â€¢ For any ğœ€ &gt; 0, the mechanism Ì… ğšº L from Definition 21 with mask distribution ğ‘ ğ¡ and noise level ğ‘ â‰¥ max ğ¡âˆŠsupp(ğ‘ ğ¡ ) ğ‘„ 1 (ğ¡)/ğœ€ is ğœ€-DP.â€¢ For any ğœ€, ğ›¿ &gt; 0, the mechanism Ì… ğšº G from Definition 22 with mask distribution ğ‘ ğ¡ and noise level ğœ â‰¥ ğœ‚(ğœ€, ğ›¿) max ğ¡âˆŠsupp(ğ‘ ğ¡ ) ğ‘„ 2 (ğ¡)/(2ğœ€) 1/2 (where ğœ‚(ğœ€, ğ›¿) refers to Theorem 2) is (ğœ€, ğ›¿)-DP. These hold for both BDP and UDP, with ğ‘„ ğ‘ (ğ¡) defined accordingly as ğ‘„ B ğ‘ (ğ¡) or ğ‘„ U ğ‘ (ğ¡). } be one of the two random mechanisms, and ğ‘… ğ» for any ğ» be the associated mechanism that uses the fixed masks ğ» but is randomized on ğ›. Let âˆ¼ âˆŠ { Fix ğ‘› &gt; 0 and an arbitrary set of masks ğ» = (ğ¡ 1 , â€¦, ğ¡ ğ‘› ) âˆŠ H ğ‘› , and consider the mechanism ğšº ğ» on D â€² â‰œ D ğ‘› (BDP case) or D â€² â‰œ D ğ‘› âˆª D ğ‘›-1 (UDP case; note that the expression of ğšº ğ» (X ) does not involve the last mask ğ¡ ğ‘› when |X | = ğ‘› -1 in this case) given in Definition 20. For a neighboring relation â‰ˆ, let Î” ğ‘,â‰ˆ denote the ğ¿ ğ‘ sensitivity computed according to â‰ˆ. For any ordered neighboring relation â‰ˆ âˆŠ { â‰ˆ}, according to Theorem 1 for pure DP and Theorem 2 for ADP applied on D â€² and w.r.t. â‰ˆ, if the noise level of ğ› in ğ‘… ğ» is chosen as ğ‘ â‰¥ ğ‘ * ğ» â‰œ Î” 1,â‰ˆ (ğšº ğ» )/ğœ€ or ğœ â‰¥ ğœ * ğ» â‰œ ğœ‚(ğœ€, ğ›¿)Î” 2,â‰ˆ (ğšº ğ» )/(2ğœ€) 1/2 , then we have for any X</figDesc><table><row><cell>ğ‘„ U ğ‘ (ğ¡) â‰œ sup ğ±âˆŠğ¸</cell><cell cols="2">ğ‘„ U ğ‘ (ğ±, ğ¡)</cell></row><row><cell cols="2">ğ‘„ B ğ‘ (ğ¡) â‰œ sup ğ±,ğ²âˆŠğ¸</cell><cell>ğ‘„ U ğ‘ (ğ±, ğ², ğ¡).</cell></row><row><cell cols="3">Lemma 23. Proof. Let ğœ€ &gt; 0, ğ‘… âˆŠ { Ì… ğšº L , Ì… ğšº G U âˆ¼,</cell><cell>B âˆ¼} denote the</cell></row><row><cell cols="3">considered neighborhood relation, and ğ›¿ be such that ğ›¿ = 0 for pure DP, ğ›¿ &gt; 0 for approximate DP. We</cell></row><row><cell>need to show that</cell><cell /></row></table><note><p><p><p>2 </p>) if ğš½ is real-valued , and</p>ğ» = (ğ¡ 1 , â€¦, ğ¡ ğ‘› ) with ğ¡ ğ‘– iid âˆ¼ ğ‘ ğ¡ . âˆ€X âˆ¼ Y âˆŠ D, ğ¬ âˆŠ Z âˆ¶ ğ‘ ğ‘…(X ) (ğ¬) â‰¤ exp(ğœ€)ğ‘ ğ‘…(Y) (ğ¬) + ğ›¿ U â‰ˆ, B</p></note></figure>
<figure type="table" xml:id="tab_11"><head /><label /><figDesc>We hence obtain the desired result by deriving an equivalent of Equation<ref type="bibr" target="#b19">(19)</ref> for the relation As for any ğ», we haveÎ” ğ‘, U â‰ˆ (ğšº ğ» ) = Î” ğ‘, U â‰ˆ ğ‘  (ğšº ğ» ) on D â€² ,we get the same result. Whether or nor the bounds from Lemma 23 are sharp for certain scenarios is a question left open for future work. Let ğ¡ âˆŠ H ğ‘Ÿ . Then for RQF we have sup ğ±,ğ²âˆŠğ¸ ğ‘„ B 2 (ğ±, ğ², ğ¡) = sup ğ±âˆŠğ¸ ğ‘„ U 2 (ğ±, ğ¡) = ğ‘š ğ‘Ÿ ğ‘† 4 (Î© ğ¡ ).</figDesc><table><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>ğ›¿,</cell></row><row><cell cols="3">which is the desired result.</cell><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell>Note that</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>âˆš ğ‘Ÿ .</cell></row><row><cell>Proof.</cell><cell /><cell>sup ğ±</cell><cell cols="3">ğ‘„ U 2 (ğ±, ğ¡) = sup ğ±âˆŠğ¸</cell><cell cols="3">ğ‘š ğ‘Ÿ â€–ğš½ RFF (ğ±)â€– 2 =</cell><cell>ğ‘š ğ‘Ÿ</cell><cell>âˆš</cell><cell>ğ‘Ÿ =</cell><cell>ğ‘š âˆš ğ‘Ÿ</cell><cell>.</cell></row><row><cell>Lemma 25. Proof.</cell><cell>sup ğ±</cell><cell cols="2">ğ‘„ U 2 (ğ±, ğ¡) = sup ğ±âˆŠğ¸</cell><cell>ğ‘š ğ‘Ÿ</cell><cell cols="2">â€–ğš½ RQF (ğ±)â€– 2 = sup ğ±âˆŠğ¸</cell><cell>ğ‘š ğ‘Ÿ</cell><cell cols="2">( âˆ‘ ğ‘–âˆŠsupp(ğ¡)</cell></row></table><note><p><p><p><p>U</p>â‰ˆ is not a symmetric relation, but in the UDP case with |Y| = |X |+1, we can still find</p>ğœ 1 , ğœ 2 such that ğœ 1 (Y) U â‰ˆ ğœ 2 (X ). U â‰ˆ ğ‘  , defined as X U â‰ˆ ğ‘  Y â‡” Y U â‰ˆ X . From</p>Lemma 23, one can get guarantees for (ğœ€, ğ›¿)-DP with the two simple following results. Lemma 24. Let ğ¡ âˆŠ H ğ‘Ÿ . Then for RFF we have sup ğ± ğ‘„ U 2 (ğ±, ğ¡) = ğ‘š</p></note></figure>
<figure type="table" xml:id="tab_13"><head /><label /><figDesc>2 ğœ and ğœ 2 ğœ‰ the values obtained previously for the UDP Laplacian setting, see Table 1, we get an expression of NSR RFF * as a function of ğ›¾, which can be minimized w.r.t. the parameter ğ›¾. For random Fourier features, an expression of the parameter ğ›¾ * minimizing NSR RFF * is given in Appendix E as a function of ğœ€ and ğ‘›. The following approximations can be derived when ğ‘› â‰ª 1/ğœ€, ğ›¾ * (ğ‘›, ğœ€) â‰ˆ 1/2</figDesc><table><row><cell>Lemma 28.</cell></row></table></figure>
<figure type="table" xml:id="tab_14"><head /><label /><figDesc>(1 -(cos(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— ) cos(ğ›š ğ‘‡ ğ‘— ğ² + ğ‘¢ ğ‘— ) + sin(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— ) sin(ğ›š ğ‘‡ ğ‘— ğ² + ğ‘¢ ğ‘— )) we reuse the quantities defined in the proof of Lemma 7, i.e. we denote ğ‘“(â€¢) â‰œ ğœŒ(â€¢) + ğœŒ(â€¢ -ğœ‹ 2 ) and, for any ğ›— = [ğœ‘ 1 , â€¦, ğœ‘ ğ‘š ], define ğ‘“ ğ›— (ğ±) = âˆ‘ ğ‘š ğ‘—=1 ğ‘“(ğ›š ğ‘‡ ğ‘— ğ± -ğœ‘ ğ‘— ). Starting from the generic expression (31) we getÎ” B 2 (ğšº RFF ) 2 = 2(ğ‘š -inf ğ±âˆˆR ğ‘‘ infFor any fixed ğ± âˆŠ R ğ‘‘ , we have ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— ) = Â±2 -1/2 and ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— -ğœ‹ 2 ) = Â±2 -1/2 , thus using the same arguments developed in the proof of Lemma 7, these are some ğœ‘ ğ‘— âˆŠ {0, ğœ‹/2, ğœ‹, 3ğœ‹/2} such that inf</figDesc><table><row><cell /><cell /><cell>ğ‘š</cell></row><row><cell cols="2">= sup ğ±,ğ²âˆˆR ğ‘‘</cell><cell>ğ‘—=1 âˆ‘</cell><cell>2(1 + cos(ğ›š ğ‘‡ ğ‘— (ğ± -ğ²) -ğœ‹))</cell></row><row><cell /><cell /><cell /><cell>ğ‘š</cell></row><row><cell cols="4">= 2(ğ‘š + sup ğ³âˆˆR ğ‘‘</cell><cell>ğ‘—=1 âˆ‘</cell><cell>cos(ğ›š ğ‘‡ ğ‘— ğ³ -ğœ‹))</cell></row><row><cell cols="2">= 4ğ‘š</cell><cell /></row><row><cell cols="4">by Lemma 6 using the nonresonant property of the frequencies.</cell></row><row><cell cols="4">â€¢ For quantized features, ğ²âˆˆR ğ‘‘</cell><cell>ğ‘š âˆ‘ ğ‘—=1 (ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— )ğœŒ(ğ›š ğ‘‡ ğ‘— ğ² + ğ‘¢ ğ‘— ) + ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— -ğœ‹ 2 )ğœŒ(ğ›š ğ‘‡ ğ‘— ğ² + ğ‘¢ ğ‘— -ğœ‹ 2 ))</cell></row><row><cell /><cell>ğ‘š</cell><cell /></row><row><cell>ğ²âˆˆR ğ‘‘</cell><cell cols="3">ğ‘—=1 âˆ‘ (ğœŒ(ğ›š ğ‘‡</cell></row></table><note><p>ğ‘— ğ± + ğ‘¢ ğ‘— )ğœŒ(ğ›š ğ‘‡ ğ‘— ğ² + ğ‘¢ ğ‘— ) + ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— -ğœ‹ 2 )ğœŒ(ğ›š ğ‘‡ ğ‘—</p></note></figure>
<figure type="table" xml:id="tab_15"><head /><label /><figDesc>X ,YâˆŠDâˆ¶X U âˆ¼Y sup ğ¬âˆŠZ ğ‘ X (ğ¬)/ğ‘ Y (ğ¬) = exp(ğœ€ * ), which is equivalent to the lemma statement. If ğ» ğ‘›-1 = (ğ¡ 1 , â€¦, ğ¡ ğ‘›-1 ) is a set of masks and ğ¡ ğ‘› a single mask, defining ğ» = (ğ¡ 1 , â€¦, ğ¡ ğ‘› ) we use the notations Ì… ğšº ğ» ğ‘›-1 ,ğ¡ ğ‘› (â€¢) â‰œ Ì… ğšº ğ» (â€¢) and ğ‘(ğ¬|ğ» ğ‘›-1 , ğ¡ ğ‘› ) â‰œ ğ‘(ğ¬|ğ»). In the following ğ¡ ğ‘› , ğ» ğ‘›-1 and ğ» are implicitly drawn (independently) from respectively ğ‘ ğ¡ , ğ‘ ğ‘›-1 ğ¡ and ğ‘ ğ‘› ğ¡ , where ğ‘ ğ¡ is the probability distribution of the masks from Definition 21. Considering X , Y âˆˆ D such that X Y we distinguish two cases, depending whether |X | = |Y| + 1 or |X | = |Y| -1. Y, denoting ğ‘› = |X | and assuming for now that |X | = |Y|+1, there is by Definition 9 a permutation ğœ âˆŠ S ğ‘› such that ğœ(X ) satisfies ğœ(X ) We have Ì… ğšº ğ» (ğœ(X )) = Ì… ğšº ğœ -1 (ğ») (X ), and as the masks are drawn i.i.d. according to ğ‘ ğ¡ , we obtain ğ‘ X</figDesc><table><row><cell>Case |X | = |Y|+1 For any X</cell></row></table><note><p>U âˆ¼ U âˆ¼ U â‰ˆ Y.</p></note></figure>
<figure type="table" xml:id="tab_16"><head /><label /><figDesc>ğ‘› ) âŠ™ ğ¡ ğ‘› â€– 1 ). Conclusion Previous results hold for any dataset size |X | âˆŠ N. We now take the supremum over X , Y, which includes both cases |X | = |Y| + 1 and |Y| = |X | + 1; the supremum is the same in both cases, and we have the equality from the first case. Thus</figDesc><table><row><cell>ğ‘ â€–ğš½(ğ² sup 1 ğ›¼ X ,YâˆŠDâˆ¶X U âˆ¼Y ğ‘ X (ğ¬) sup ğ¬âˆŠZ ğ‘ Y</cell></row></table></figure>
<figure type="table" xml:id="tab_17"><head /><label /><figDesc>ğ‘£ ğ‘— = ğ‘¢ ğ‘— + ğœ‹ for 1 â‰¤ ğ‘— â‰¤ ğ‘š. The frequencies being nonresonant, a direct consequence of Lemma 6 is that for each ğ›— âˆˆ R ğ‘š , sup ğ±âˆŠR ğ‘‘ inf ğ¡âˆŠH ğ‘Ÿ ğ‘“ ğ›—,ğ¡ (ğ±) = sup ğ±âˆŠR ğ‘‘ inf ğ¡âˆŠH ğ‘Ÿ âˆ‘ ğ‘—âˆŠsupp(ğ¡) ğ‘“(ğ›š ğ‘‡ ğ‘— ğ± -ğœ‘ ğ‘— ) = ğ‘Ÿ sup ğ‘¥âˆŠR ğ‘“(ğ‘¥) = ğ‘Ÿ âˆš 2. The supremum being independent of ğ›— this yields sup In the BDP setting, the supremum is taken independently on ğ± and ğ², thus for any ğ¡ we have sup ğ±,ğ²âˆŠR ğ‘‘ ğ‘„ B 1 (ğ±, ğ², ğ¡) = 2 sup ğ±âˆŠR ğ‘‘ ğ‘„ U 1 (ğ±, ğ¡) and Let ğ‘‹ denote the mean of ğ‘› â€² samples taken without replacement from a collection ğ‘¥ 1 , â€¦, ğ‘¥ ğ‘› . Let ğœ 2 = 1 ğ‘› âˆ‘ ğ‘› ğ‘–=1 |ğ‘¥ ğ‘– -ğœ‡| 2 , then we have Proof. Denote ğ‘‹ = 1 ğ‘› âˆ‘ ğ‘› ğ‘–=1 ğ‘” ğ‘– ğ‘¥ ğ‘– , with ğ‘” ğ‘– = 1 if ğ‘¥ ğ‘– is selected, and 0 otherwise (and as a consequence, âˆ‘ ğ‘› ğ‘–=1 ğ‘” ğ‘– = ğ‘› â€² ). For any 1 â‰¤ ğ‘– &lt; ğ‘— â‰¤ ğ‘›, the marginal of ğ‘” ğ‘– is uniform and E(ğ‘” ğ‘– ğ‘” ğ‘—) = ğ‘ƒ [ğ‘” ğ‘– ğ‘” ğ‘— = 1] = ğ‘ƒ [ğ‘” ğ‘– = 1 and ğ‘” ğ‘— = 1] = ğ‘ƒ [ğ‘§ = 2]for ğ‘§ a random variable having an hypergeometric law of parameters (ğ‘›, 2/ğ‘›, ğ‘› â€² ).Var(ğ‘”ğ‘– ) = E|ğ‘” ğ‘– | 2 -|Eğ‘” ğ‘– | 2 = Cov(ğ‘” ğ‘– , ğ‘” ğ‘— ) = E(ğ‘” ğ‘– ğ‘” ğ‘— ) -E(ğ‘” ğ‘– )E(ğ‘” ğ‘— ) ğ‘–=1 Var ğ  (ğ‘¥ ğ‘– ğ‘” ğ‘– ) + 2 âˆ‘ 1â‰¤ğ‘–&lt;ğ‘—â‰¤ğ‘› Cov(ğ‘¥ ğ‘– ğ‘” ğ‘– , ğ‘¥ ğ‘— ğ‘” ğ‘— ))Let ğœ‡ = 1 ğ‘› âˆ‘ ğ‘› ğ‘–=1 ğ‘¥ ğ‘– , and ğœ 2 = 1 ğ‘› âˆ‘ ğ‘› ğ‘–=1 (ğ‘¥ ğ‘– -ğœ‡) 2 . Note thatğ‘›ğœ 2 =</figDesc><table><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>ğ‘›</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>âˆ‘ (ğ‘¥ ğ‘– -ğœ‡) 2</cell></row><row><cell cols="2">ğ±âˆŠR ğ‘‘</cell><cell cols="2">inf ğ¡âˆŠH ğ‘Ÿ</cell><cell cols="6">sup ğ›—âˆŠ{0,ğœ‹/2,ğœ‹,3ğœ‹/2} ğ‘š = ğ‘“ ğ›—-ğ®,ğ¡ (ğ±) â‰¥ ğ‘–=1 ğ‘› âˆ‘ ğ‘¥ 2 ğ‘– -ğ‘›ğœ‡ 2 ğ›—âˆŠ{0,ğœ‹/2,ğœ‹,3ğœ‹/2} ğ‘š sup</cell><cell>sup ğ±âˆŠR ğ‘‘</cell><cell>inf ğ¡âˆŠH ğ‘Ÿ</cell><cell>ğ‘“ ğ›—-ğ®,ğ¡ (ğ±) = ğ‘Ÿ</cell><cell>âˆš</cell><cell>2.</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>ğ‘–=1</cell></row><row><cell cols="10">As we also have (even for resonant frequencies) the upper bound sup ğ±âˆŠR ğ‘‘ inf ğ¡âˆŠH ğ‘Ÿ sup ğ›—âˆŠR ğ‘š ğ‘“ ğ›—,ğ¡ (ğ±) â‰¤ sup ğ±âˆŠR ğ‘‘ sup ğ¡âˆŠH ğ‘Ÿ sup = ğ‘› âˆ‘ ğ‘–=1 ğ‘¥ 2 ğ‘– -1 (âˆ‘ ğ‘¥ 2 ğ‘– + 2 âˆ‘ ğ‘¥ ğ‘– ğ‘¥ ğ‘— ) ğ‘› ğ‘–&lt;ğ‘— ğ‘“ ğ›—,ğ¡ (ğ±) â‰¤ ğ‘Ÿ ğ›—âˆŠR ğ‘š we get for each ğ¡ âˆŠ H ğ‘Ÿ = ğ‘› -1 ğ‘› (âˆ‘ ğ‘¥ 2 ğ‘– -2 1 ğ‘–&lt;ğ‘— ğ‘› -1 âˆ‘ ğ‘¥ ğ‘– ğ‘¥ ğ‘— )</cell><cell>âˆš</cell><cell>2</cell></row><row><cell cols="3">ğ‘š âˆš As a consequence</cell><cell cols="3">2 â‰¤ sup ğ±âˆŠR ğ‘‘</cell><cell cols="2">inf ğ¡ â€² âˆŠH ğ‘Ÿ</cell><cell cols="2">ğ‘„ U 1 (ğ±, ğ¡ â€² ) â‰¤ sup ğ±âˆŠR ğ‘‘ Var( 1 ğ‘› ğ‘› âˆ‘ ğ‘–=1 ğ‘” ğ‘– ğ‘¥ ğ‘– ) = ğ‘„ U 1 (ğ±, ğ¡) â‰¤ sup ğ±âˆŠR ğ‘‘ 1 ğ‘› â€² ğ‘› -ğ‘› â€² ğ‘› 2 ğ‘› 2 ğ¡ â€² âˆŠH ğ‘Ÿ sup ğ‘› -1 ğœ 2</cell><cell>ğ‘„ U 1 (ğ±, ğ¡ â€² ) = ğ‘š</cell><cell>âˆš</cell><cell>2.</cell></row><row><cell>2ğ‘š âˆš</cell><cell cols="4">2 = sup ğ±,ğ²âˆŠR ğ‘‘</cell><cell cols="2">inf ğ¡ â€² âˆŠH ğ‘Ÿ</cell><cell cols="3">ğ‘„ B 1 (ğ±, ğ², ğ¡ â€² ) â‰¤ sup ğ±,ğ²âˆŠR ğ‘‘</cell><cell>ğ‘„ B 1 (ğ±, ğ², ğ¡) â‰¤ sup ğ±,ğ²âˆŠR ğ‘‘ ğœ 2 = ğ‘› â€²</cell><cell>sup ğ¡ â€² âˆŠH ğ‘Ÿ</cell><cell>ğ‘„ B 1 (ğ±, ğ², ğ¡ â€² ) = 2ğ‘š</cell><cell>âˆš</cell><cell>2.</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>Var(ğ‘‹) =</cell><cell>ğœ 2 ğ‘› â€²</cell><cell>ğ‘› -ğ‘› â€² ğ‘› -1</cell><cell>.</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>ğ‘› â€² ğ‘›</cell><cell>(1 -</cell><cell>ğ‘› â€² ğ‘›</cell><cell>) =</cell><cell>ğ‘› â€² (ğ‘› -ğ‘› â€² ) ğ‘› 2</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>=</cell><cell>ğ‘› â€² (ğ‘› â€² -1) ğ‘›(ğ‘› -1)</cell><cell>-</cell><cell>(ğ‘› â€² ) 2 ğ‘› 2 (hypergeometric law)</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>=</cell><cell>ğ‘› â€² ğ‘›</cell><cell>(</cell><cell>ğ‘› â€² -1 ğ‘› -1</cell><cell>-</cell><cell>ğ‘› â€² ğ‘›</cell><cell>) =</cell><cell>ğ‘› â€² (ğ‘› â€² -ğ‘›) ğ‘› 2 (ğ‘› -1)</cell></row><row><cell cols="10">ğœ‹,3ğœ‹/2} ğ‘š ğ‘” ğ‘– ğ‘¥ ğ‘– ) = (ğ‘› â€² ) 2 ( 1 ğ‘“ ğ›—-ğ®,ğ¡ (ğ±). âˆ‘ ğ‘› âˆ‘ ğ‘–=1 ğ‘—âˆŠsupp(ğ¡) 1 ğ‘› â€² 1 Var( 1 (ğ±, ğ², ğ¡) = ğ‘„ B ğ›¼ |ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— ) -ğœŒ(ğ›š ğ‘‡ ğ‘— ğ² + ğ‘¢ ğ‘— )| + |ğœŒ(ğ›š ğ‘‡ ğ‘— ğ± + ğ‘¢ ğ‘— -= 1 (ğ‘› â€² ) 2 ( ğ‘› â€² (ğ‘› -ğ‘› â€² ) ğ‘› 2 ğ‘› âˆ‘ ğ‘–=1 ğ‘¥ 2 ğ‘– + 2 ğ‘› â€² (ğ‘› â€² -ğ‘›) ğ‘› 2 (ğ‘› -1) 1â‰¤ğ‘–&lt;ğ‘—â‰¤ğ‘› ğœ‹ 2 ) -ğœŒ(ğ›š ğ‘‡ ğ‘— ğ² + ğ‘¢ ğ‘— -âˆ‘ ğ‘¥ ğ‘– ğ‘¥ ğ‘— ) = 1 ğ›¼ sup ğ›—âˆŠ{0,ğœ‹/2,ğœ‹,3ğœ‹/2} ğ‘š ğ‘“ ğ›—-ğ®,ğ¡ (ğ±) -ğ‘“ ğ›—-ğ®,ğ¡ (ğ²) = 1 ğ‘› â€² ğ‘› -ğ‘› â€² ğ‘› 2 ( ğ‘› âˆ‘ ğ‘–=1 ğ‘¥ 2 ğ‘– -2 1 ğ‘› -1 âˆ‘</cell><cell>ğœ‹ 2</cell><cell>)|</cell></row><row><cell /><cell /><cell>=</cell><cell>1 ğ›¼</cell><cell cols="6">sup ğ›—âˆŠ{0,ğœ‹/2,ğœ‹,3ğœ‹/2} ğ‘š</cell></row></table><note><p>ğ‘“ ğ›—-ğ®,ğ¡ (ğ±) + ğ‘“ ğ›—-ğ¯,ğ¡ (ğ²). where D Derivation of the noise-signal ratio Lemma 30. ğ‘› âˆ‘ 1â‰¤ğ‘–&lt;ğ‘—â‰¤ğ‘› ğ‘¥ ğ‘– ğ‘¥ ğ‘— )</p></note></figure>
<figure type="table" xml:id="tab_18"><head /><label /><figDesc>E X E ğ âˆ¼WOR(ğ‘›,ğ‘› â€² ) â€–ğ ğ‘› â€² â€– 2 = 1 ğ‘› -1 ( ğ‘› ğ‘› â€² -1)(E ğ± â€–ğš½(ğ±)â€– 2 2 -E X â€–ğ³ X â€– 2 2 ) ğ âˆ¼WOR(ğ‘›,ğ‘› â€² ) â€–ğ ğ‘› â€² â€– 2 = E ğ âˆ¼Bern(ğ›½) ğ‘› â€–ğ ğ‘› â€² â€– 2 = ğ± ğ‘– ) ğ‘— | 2 Var ğ  (ğ‘” ğ‘– ) E ğ âˆ¼Bern(ğ›½) ğ‘› â€–ğ ğ‘› â€² â€– 2 =</figDesc><table><row><cell>=</cell><cell cols="3">1 ğ‘› -1</cell><cell>(</cell><cell cols="2">ğ‘› ğ‘› â€² -1)(E ğ± â€–ğš½(ğ±)â€– 2 2 -(â€–ğ³â€– 2 2 +</cell><cell>ğ‘š ğ‘—=1 âˆ‘</cell><cell>Var((ğ³ X ) ğ‘— )))</cell></row><row><cell>=</cell><cell cols="3">1 ğ‘› -1</cell><cell>(</cell><cell cols="2">ğ‘› ğ‘› â€² -1)(E ğ± â€–ğš½(ğ±)â€– 2 2 -(â€–ğ³â€– 2 2 +</cell><cell>1 ğ‘›</cell><cell>(E ğ± â€–ğš½(ğ±)â€– 2 -â€–ğ³â€– 2 )))</cell></row><row><cell>=</cell><cell cols="3">1 ğ‘› -1</cell><cell>(</cell><cell>ğ‘› ğ‘› â€² -1)(1 -</cell><cell>1 ğ‘›</cell><cell>)(E ğ± â€–ğš½(ğ±)â€– 2 2 -â€–ğ³â€– 2 2 )</cell></row><row><cell cols="2">E X E 1 ğ‘›</cell><cell>(</cell><cell cols="4">ğ‘› ğ‘› â€² -1)(E ğ± â€–ğš½(ğ±)â€– 2 2 -â€–ğ³â€– 2 2 )</cell></row><row><cell cols="6">â€¢ i.i.d. Bernoulli sampling with parameter ğ›½:</cell></row><row><cell cols="7">ğ‘š âˆ‘ ğ‘—=1 |ğš½(1 Var ğ  ( 1 ğ›½ğ‘› ğ‘› âˆ‘ ğ‘–=1 ğ‘” ğ‘– ğš½(ğ± ğ‘– ) ğ‘— ) = ğ‘š ğ‘› 1 ğ›½ 2 ğ‘› 2 âˆ‘ ğ‘—=1 âˆ‘ ğ‘–=1 ğ‘› ( 1 ğ›½ 1 -1)( ğ‘›</cell></row></table><note><p>ğ‘› âˆ‘ ğ‘–=1 â€–ğš½(ğ± ğ‘– )â€– 2 )</p></note></figure>
			<note place="foot" n="1" xml:id="foot_0"><p>A first and reduced version of this work with privacy upper bounds and without the subsampling mechanism has been previously published<ref type="bibr" target="#b45">[45]</ref>.</p></note>
			<note place="foot" n="2" xml:id="foot_1"><p>An implementation can be found at https://github.com/BorjaBalle/analytic-gaussian-mechanism.</p></note>
			<note place="foot" xml:id="foot_2"><p>SummaryWe summarize the results obtained in this paper in the following tables, where ğœ‚ = ğœ‚(ğœ€, ğ›¿) refers to Theorem 2.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>* <rs type="person">V. Schellekens</rs> and <rs type="person">L. Jacques</rs> are funded by the <rs type="funder">"Fonds de la Recherche Scientifique" (F.R.S. -FNRS</rs>). Part of this work was supported by the <rs type="funder">FNRS</rs> Grant <rs type="grantNumber">T.0136.20</rs> (PDR).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gXNWUW5">
					<idno type="grant-number">T.0136.20</idno>
				</org>
			</listOrg>
			<div type="annex">
<div><p>Î” 2 (ğšº)</p><p>+ Î© union of orthogonal bases.</p><p>Lemma 9:</p><p>As a result</p><p>Taking the expectation w.r.t. the dataset, we get</p><p>Total noise level For conciseness, we use the notation ğ›½ = ğ‘› â€² /ğ‘› when sampling ğ‘› â€² samples without replacement, and ğ›¼ = ğ‘Ÿ/ğ‘š. The total noise level for Poisson sampling is ref.</p><p>For WOR sampling, we get ref.</p><p>Proof of Lemma 27. We rewrite ğ¬(X ) = ğ‘“(|X | + ğœ) Î£(X ), where ğ‘“(|X | + ğœ) is an estimator of 1/|X |. We define the reference signal as ğ¬ = ğ³ or ğ³ X , and the noise as ğ = ğ‘“(|X | + ğœ) Î£(X ) -ğ¬. In the following, the expectations are taken w.r.t. the randomness of the sketching mechanism when ğ³ X is chosen as the reference signal, and w.r.t. both the randomness of the mechanism and the draw of X when ğ³ is the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Differentially Private Covariance Estimation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="14190" to="14199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Differentially Private Robust Low-Rank Approximation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Upadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp;</forename><forename type="middle">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4137" to="4145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title />
		<author>
			<persName><forename type="first">Curran</forename><surname>Associates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inc</forename></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Differentially private clustering in high-dimensional Euclidean spaces</title>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Balcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="322" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Privacy Amplification by Subsampling: Tight Analyses via Couplings and Divergences</title>
		<author>
			<persName><forename type="first">B</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Barthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gaboardi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Improving the Gaussian Mechanism for Differential Privacy: Analytical Calibration and Optimal Denoising</title>
		<author>
			<persName><forename type="first">B</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Differentially Private Database Release via Kernel Mean Embeddings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01641</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>stat</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Private empirical risk minimization: Efficient algorithms and tight error bounds</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bassily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thakurta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 55th Annual Symposium on Foundations of Computer Science</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Practical privacy: the SuLQ framework</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems</title>
		<meeting>the twenty-fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="128" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A learning theory approach to noninteractive database privacy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ligett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fundamental Performance Limits for Ideal Decoders in High-Dimensional Linear Inverse Problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bourrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Peleg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>PÃ©rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="7928" to="7946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Burer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Monteiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="329" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-Scale High-Dimensional Clustering with Fast Sketching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chatalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Differentially private empirical risk minimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monteleoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Sarwate</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1069" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Near-optimal differentially private principal components</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarwate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="989" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Synopses for massive data: Samples, histograms, wavelets, sketches</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garofalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jermaine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Databases</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="1" to="294" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unique in the crowd: The privacy bounds of human mobility</title>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>De Montjoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Blondel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1376</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Revealing information while preserving privacy</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dinur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems</title>
		<meeting>the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="202" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Privacy aware learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">38</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Differential privacy: A survey of results</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Theory and Applications of Models of Computation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Calibrating Noise to Sensitivity in Private Data Analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of cryptography conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The Algorithmic Foundations of Differential Privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="211" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Analyze gauss: optimal bounds for privacy-preserving principal component analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thakurta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual ACM Symposium on Theory of Computing -STOC '14</title>
		<meeting>the 46th Annual ACM Symposium on Theory of Computing -STOC '14</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rappor: Randomized aggregatable privacypreserving ordinal response</title>
		<author>
			<persName><forename type="first">Ãš</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pihur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korolova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM SIGSAC conference on computer and communications security</title>
		<meeting>the 2014 ACM SIGSAC conference on computer and communications security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1054" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Private Coresets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fiat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Forty-first Annual ACM Symposium on Theory of Computing, STOC '09</title>
		<meeting>the Forty-first Annual ACM Symposium on Theory of Computing, STOC '09</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="361" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Coresets for differentially private k-means clustering and applications to privacy in mobile sensor networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Processing in Sensor Networks (IPSN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="3" to="16" />
		</imprint>
	</monogr>
	<note>16th ACM/IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Foucart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rauhut</surname></persName>
		</author>
		<title level="m">A mathematical introduction to compressive sensing</title>
		<meeting><address><addrLine>Basel</addrLine></address></meeting>
		<imprint>
			<publisher>BirkhÃ¤user</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Traonmilin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.07180</idno>
		<title level="m">Compressive statistical learning with random feature moments</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Beating Randomized Response on Incoherent Matrices</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page">1255</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Symmetric matrix perturbation for differentially-private principal component analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Imtiaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Sarwate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2339" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Wishart mechanism for differentially private principal components analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On differentially private low rank approximation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kapralov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting>the twenty-fourth annual ACM-SIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1395" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Privacy via the Johnson-Lindenstrauss Transform</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korolova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Privacy and Confidentiality</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sketching for large-scale learning of mixture models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bourrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>PÃ©rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Inference: A Journal of the IMA</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="508" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Sketching for large-scale learning of mixture models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bourrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>PÃ©rez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="447" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Blind Source Separation Using Mixtures of Alpha-Stable Distributions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="771" to="775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Compressive K-means</title>
		<author>
			<persName><forename type="first">N</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Traonmilin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6369" to="6373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">NÃ¤herungsweise ganzzahlige auflÃ¶sung linearer gleichungen</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kronecker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1884">1884</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A Convergent Differentially Private k-Means Clustering Algorithm</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Gong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M.-L</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">&amp; S.-J</forename><surname>Huang</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11439</biblScope>
			<biblScope unit="page" from="612" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Privacy integrated queries: an extensible platform for privacy-preserving data analysis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Mcsherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 ACM SIGMOD International Conference on Management of data</title>
		<meeting>the 2009 ACM SIGMOD International Conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Clustering algorithms for the centralized and local models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Stemmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Learning Theory</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="619" to="653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">k-variates++: more pluses in the k-means++</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Canyasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boreli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="145" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">DP-EM: Differentially Private Expectation Maximization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Foulds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Differentially private grids for geospatial data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Qardaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 29th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="757" to="768" />
		</imprint>
	</monogr>
	<note>Data Engineering (ICDE)</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Differentially Private Compressive k-Means</title>
		<author>
			<persName><forename type="first">V</forename><surname>Schellekens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chatalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Houssiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>De Montjoye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jacques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019 -44th International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Quantized Compressive K-Means</title>
		<author>
			<persName><forename type="first">V</forename><surname>Schellekens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jacques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1211" to="1215" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Differentially private k-means with constant multiplicative error</title>
		<author>
			<persName><forename type="first">U</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5431" to="5441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Differentially private k-means clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth ACM Conference on Data and Application Security and Privacy</title>
		<meeting>the Sixth ACM Conference on Data and Application Security and Privacy</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="26" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">k-anonymity: A model for protecting privacy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Sweeney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="557" to="570" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Learning with privacy at scale</title>
		<author>
			<persName><forename type="first">D</forename><surname>Team</surname></persName>
		</author>
		<ptr target="https://machinelearning.apple.com/2017/12/06/learning-with-privacy-at-scale.html" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Compressed Sensing for Privacy-Preserving Data Processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Valsesia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Magli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Introduction to the perturbation theory of Hamiltonian systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Treschev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Zubelevich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">The Price of Privacy for Low-rank Factorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Upadhyay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Technical privacy metrics: a systematic survey</title>
		<author>
			<persName><forename type="first">I</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eckhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Rank optimality for the Burer-Monteiro factorization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Waldspurger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waters</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Differentially private empirical risk minimization revisited: Faster and more general</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2722" to="2731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">On the Relation Between Identifiability, Differential Privacy, and Mutual-Information Privacy</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5018" to="5029" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Randomized response: A survey technique for eliminating evasive answer bias</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Warner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">309</biblScope>
			<biblScope unit="page" from="63" to="69" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Differentially private density estimation via Gaussian mixtures model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM 24th International Symposium on Quality of Service (IWQoS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">PrivGene: differentially private model fitting using genetic algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winslett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 international conference on Management of data -SIGMOD '13</title>
		<meeting>the 2013 international conference on Management of data -SIGMOD '13</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">665</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Differential privacy with compression</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ligett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Information Theory</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="2718" to="2722" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>