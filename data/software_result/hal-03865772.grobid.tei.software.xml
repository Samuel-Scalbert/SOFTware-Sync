<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" />
			</titleStmt>
			<publicationStmt>
				<publisher />
				<availability status="unknown"><licence /></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date />
						</imprint>
					</monogr>
					<idno type="MD5">468B7D5486A9995ED2702BCD67B7FBCD</idno>
					<idno type="DOI">10.1111/cgf.14635</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-07T09:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid" />
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts</term>
					<term>Computing methodologies → Motion capture</term>
					<term>Neural networks</term>
					<term>Motion processing</term>
				</keywords>
			</textClass>
			<abstract>
<div><p>Human motion synthesis and editing are essential to many applications like video games, virtual reality, and film postproduction. However, they often introduce artefacts in motion capture data, which can be detrimental to the perceived realism. In particular, footskating is a frequent and disturbing artefact, which requires knowledge of foot contacts to be cleaned up. Current approaches to obtain foot contact labels rely either on unreliable threshold-based heuristics or on tedious manual annotation. In this article, we address automatic foot contact label detection from motion capture data with a deep learning based method. To this end, we first publicly release <software>U NDER P RESSURE</software>, a novel motion capture database labelled with pressure insoles data serving as reliable knowledge of foot contact with the ground. Then, we design and train a deep neural network to estimate ground reaction forces exerted on the feet from motion data and then derive accurate foot contact labels. The evaluation of our model shows that we significantly outperform heuristic approaches based on height and velocity thresholds and that our approach is much more robust when applied on motion sequences suffering from perturbations like noise or footskate. We further propose a fully automatic workflow for footskate cleanup: foot contact labels are first derived from estimated ground reaction forces. Then, footskate is removed by solving foot constraints through an optimisation-based inverse kinematics (IK) approach that ensures consistency with the estimated ground reaction forces. Beyond footskate cleanup, both the database and the method we propose could help to improve many approaches based on foot contact labels or ground reaction forces, including inverse dynamics problems like motion reconstruction and learning of deep motion models in motion synthesis or character animation. Our implementation, pre-trained model as well as links to database can be found at github.com/InterDigitalInc/UnderPressure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div><head n="1.">Introduction</head><p>Perceived realism is central to human animation; however, artefacts are often introduced whenever editing or synthesising motion data. These include feet sliding on, passing through or floating above the ground, which are known to disturb human perception even at very low intensities <ref type="bibr" target="#b44">[PHO11]</ref>. In this paper we propose a fully automatic approach to foot contact detection and footskate artefacts removal.</p><p>As of today, foot contacts are derived from motion sequences using simple heuristics, most commonly hand-crafted thresholds over velocity and proximity relative to the ground. These approaches suffer from three main limitations. First, the terrain height map must be known, meaning that these approaches are most of the time not applicable to uneven or inclined ground. Second, optimal thresholds are not universal and must be manually tuned, ideally for every type of motion, morphology and contact location (e.g. heel or toe). Finally, even optimal thresholds fail at accurately detecting every foot contact, which implies that tedious manual checking is necessary when using these approaches. Finally, even optimal thresholds are far from being 100% accurate and lack of robustness, which implies that tedious manual checking is necessary when using these approaches.</p><p>In this paper, we propose a data-driven foot contact detection method from motion that outperforms traditional heuristic approaches. First, we introduce <software>U NDER P RESSURE</software>, a novel publicly available dataset composed of diverse motion capture data from 10 individuals, synchronised with pressure insoles data, from which corresponding vertical Ground Reaction Forces (vGRFs) and foot contact labels can be derived. Then, our key idea is to model vGRFs with a deep neural network, providing a finer representation of interactions of the feet with the ground than binary contact labels. Furthermore, vGRFs are related to the dynamics of motion unlike binary labels. To this end, we train a deep neural network to estimate the distribution of vGRFs over the feet, from which foot contact labels can be calculated, encouraging robustness and generalisability since the network needs a relatively high-level understanding of the motion dynamics to estimate accurate vGRFs.</p><p>We then provide a quantitative evaluation of our model against an optimal thresholds (OT) baseline that relies on thresholding the velocity and height above ground of foot joints, linear and nonlinear learned generalisations of the OT baseline, as well as an ablative study of the proposed architecture. We further experiment how these models behave in different challenging conditions representative of real-world applications.</p><p>Finally, we demonstrate the generalisability of our approach on motion sequences from other databases, as well as its integration in a fully automatic footskate cleanup workflow. The novelty of this workflow is to leverage our deep neural network by enforcing invariance between vGRFs estimated from input and optimised motion sequences to better guide the IK optimisation and maintain the consistency of interactions between feet and ground.</p><p>To the best of our knowledge, we propose the first method of foot contact detection for animation learned on a significant amount of diverse motion data labelled with foot pressures. Here is a summary of our key contributions:</p><p>• a unique and publicly available motion capture dataset, called <software>U NDER P RESSURE</software>, composed of 5.6 hours of diverse motions from 10 individuals and synchronized with pressure insoles data, • a deep neural network modelling part of the motion dynamics from joint positions, i.e. vGRF distribution over the feet, • a robust state-of-the-art method for foot contact detection, providing about 95% correct foot contact labels at 100 Hz on new subjects, • a fully automatic workflow for footskate cleanup built on top of our foot contact detection method, preserving vGRFs estimated by our deep neural network.</p></div>
<div><head n="2.">Related Work</head><p>Early works in human animation already considered kinematic constraints such as foot contacts [BB98, LCR *   foot contact information is also deeply involved in character control based on physical simulation [ALX * 19, KLvdP20, WL19, XLKvdP20, YTL18] where ground reaction forces are explicitly modelled in the physics engine, and also relates to footplacement strategies that are a real challenge for locomotion policies [PvdP17]. In the following, we overview existing approaches for foot contacts labels detection (2.1) and ground reaction forces estimation (2.2), as well as existing databases of motion data labelled with information on foot contacts (2.3).</p></div>
<div><head n="2.1.">Foot Contact Labels Detection</head><p>The most widespread family of approaches in both animation research and industry to extract foot contact labels from motion data relies on simple heuristics with hand-crafted thresholds, applied to velocity and proximity relative to the ground as proposed by Bindiganavale and Badler While addressing the unreliability of common heuristics-based methods due to zero velocity assumption invalidated by noise, this approach is not fully automatic. Moreover, the noise pattern is assumed to be uniform along time, which is not always true.</p><p>Researchers also investigated learned contact detection models. E.g. Ikemoto et al. <ref type="bibr" target="#b17">[IAF06]</ref> proposed a K-Nearest Neighbors (KNN) classifier to determine when the feet should be planted and claimed to be more accurate than heuristics-based approaches. However, a very low amount of manually annotated motion data was used (about 3 minutes, single subject) preventing the approach to generalise well. The proposed KNN classifier achieves 90.78% accuracy over about 33 seconds of hand-labelled motion data, compared to 57.45% and 57.00% for speed-based and height-based thresholding baselines, respectively. However, these relatively low accuracies suggest that the corresponding thresholds are not optimal.</p><p>More recently, researchers leveraged neural networks for exploring this problem.  *  20] to additionally detect at each frame whether the subject is stationary or not. However, in all these works the foot contact detection modules were trained with ground truth contacts obtained from simple heuristics, as previously described. Tedious manual screening has occasionally been used to correct or label motion sequences, but this approach limits the amount of labelled data which is crucial with neural networks.</p><p>Other works also embedded foot contact detection into their model with the goal of improving foot positioning consistency, but do not support detection from motion data. Yang et al. [YKL21] estimated lower-body pose and foot contacts from upper-body AR/VR tracking devices. Harvey et al. <ref type="bibr" target="#b16">[HYNP20]</ref> proposed a motion in-betweening method where pose and contact labels are interpolated between their known values at user-specified keyframes. Likewise, Holden et al. [HKS17] and Starke et al. <ref type="bibr" target="#b57">[SZKS19]</ref> include foot contact detection at the next frame in character control frameworks, which enables foot contact continuation. Min and Chain [MC12] modelled foot contacts into a graph-based framework called <software>Motion Graphs++</software>, by annotating individual motion primitives with embedded contact information, able to randomly synthesise motion along with contacts at runtime.</p></div>
<div><head n="2.2.">Ground Reaction Forces Estimation</head><p>In physics, the force exerted by the ground on a body in contact, such as the human body, is called a ground reaction force Later on, they extended their approach with additional neural networks [SGX * 21]: TPNet first regresses target 3D poses and contact states from 2D keypoints. Then, GRFNet and DyNet iteratively estimate GRFs and PD controller gain parameters in a dynamic cycle where the character pose is updated at each step after forward kinematics.</p><p>Different from motion reconstruction from images, Zell et al.</p><p>[ZRW20] proposed a weakly-supervised approach to inverse dynamics. An MLP is trained to estimate GRFs, moments and joint torques from motion such that the input motion is reconstructed using forward dynamics in an optimisation loop. Motion capture data synchronized with force plates enable supervision during training: reconstructed GRF+M and joint torque divergences are penalized while GRF are minimized whenever feet are not in contact with the ground.</p><p>To the best of our knowledge, the closest work to the proposed method is the deep learning approach to improve human pose estimation computing stability proposed by Scott et al. [SRF *  20]. In this work, body dynamics analysis from joint positions is investi-gated while most papers on human pose estimation only focus on skeleton kinematics. A convolutional neural network called Press-Net is proposed to estimate 2D foot pressure maps from joint positions and validated on a novel dataset of Tai Chi sequences (see Section 2.3). Center of Pressure (CoP) and Base of Support (BoS) are computed from pressure values and used for validation. Unlike Scott et al., our work specifically targets foot contact detection and footskate cleanup, tasks that are relevant to human character animation. We explore more diverse motion sequences including different types of locomotion at different paces performed in different ways (forward, backward, sideways) as well as sequences in non-flat environments such as stair climbing and stepping on solid obstacles.</p></div>
<div><head n="2.3.">Foot Contact &amp; Ground Reaction Force Databases</head><p>As of today, motion capture data annotated with accurate foot contact information are scarce. Researchers in biomechanics and biomedical engineering have released a few databases of motion capture with GRFs e.g. [KSN14], however most of them are not suitable for animation purposes as they typically focus on specific aspects of movement, or target stage and symptoms recognition of diseases in pathological subjects.</p><p>To the best of our knowledge, the closest database to the one we propose is PSU-TMM100. Recently released to the computer vision community by Scott et al. [SRF *  20], this dataset provides videos from two views, motion capture markers, body joints and foot pressure recorded with insole sensors. It contains about 7.6 hours of data during which 10 subjects are performing 24-form simplified Tai Chi. Although similar in terms of scale and nature of the captured data, the database we propose has quite different types of motion. While PSU-TMM100 contains specific Tai Chi sequences mostly composed of slow body movements with long and stable foot supports, <software>U NDER P RESSURE</software> provides diversified se- quences focused on but not restricted to locomotion at different paces including on non-flat environments (see Table <ref type="table" target="#tab_4">1</ref>), i.e. more challenging conditions for foot contacts detection.</p></div>
<div><head n="3.">Database</head><p>In this work, we release <software>U NDER P RESSURE</software>, a motion capture database annotated with pressure insoles data, designed primarily for character animation purposes. In the following, we provide information about the capture, the motion characteristics, and the preprocessing steps.</p><p>We recorded 10 healthy adult volunteers (2F, 8M) with diverse morphologies aged between 21 and 55 years (32 ± 11 yr), weighing between 65 and 91 kilograms (79 ± 9 kg), and measuring between 167 and 187 centimeters (177 ± 5 cm). Each subject performed the same set of activities, including forward and backward locomotion at different paces, sitting, standing, passing obstacles, climbing stairs, as well as motions on uneven terrain like going up and down stairs. The detailed composition of our dataset is provided in Table 1. Motion capture data for each subject last approximately 34 minutes, for a total of 5.6 hours of motion capture.  3). At runtime, our deep network augments motion data with vGRFs from which foot contact labels can be derived using the contact function Γ, both useful in many applications, e.g., reconstructing motion from images, cleaning footskate, finding suitable transition frames for motion blending, adapting animations to uneven terrain, and many more. As illustrated, both estimated vGRFs and detected contacts are evaluated in Sections 5.2 and 5.3, respectively.</p></div>
<div><head n="3.2.">Foot Pressure</head><p>In addition to motion capture, we recorded the spatial distribution of plantar foot pressures. To this end, subjects were also equipped with Moticon's OpenGo Sensor Insoles [AG21] placed into their shoes. Each insole has 16 plantar pressure sensors with a resolution of 0.25 N/cm 2 and a 6-axis inertial measurement unit (IMU), both running at 100 Hz (See Figure <ref type="figure">2a</ref>). Moreover, we weighed each subject with full equipment to enable vGRFs normalisation and equipped subjects with the same shoes whose soles are thin and flexible insole for accurate and faithful pressure measures as well as for controlling the grip.</p></div>
<div><head n="3.3.">Post-Capture Processing</head><p>Vertical GRFs Captured data include motion sequences, plantar pressure distribution and foot acceleration. Since pressure is defined as the perpendicular force per unit area, we additionally compute vertical GRF components (vGRFs) by multiplying pressure values by the corresponding cell areas. The motivation here is that groups of vGRFs are easier to aggregate (by summation). We also normalise these values to express them as subject weight proportions.</p><p>Foot Contact Labels We derive ground truth foot contact labels deterministically from vGRFs. In this work, we consider two contact locations per foot, i.e. heels and toes as commonly done in human animation [IAF06, HKS17, SGX * 21]. To compute contact labels, vGRF components are first smoothed with a Gaussian filter to avoid rapidly alternating labels due to threshold effects. Then, smoothed vGRF components are summed per contact location (see red and blue cells in Figure <ref type="figure">2a</ref>), rescaled such that the sum of blue and red cell vGRFs is equal to total vGRF (to properly ignore gray cells which particularly suffer from noised measures and can be activated during either toes or heels contact) and then a threshold at 5% of the body weight is applied to obtain raw labels. Finally, raw labels are discarded whenever per-foot total vGRF (including gray cells) is below 10% of the body weight to avoid false positives triggered by noise. Contact phases shorter than 0.1s are also discarded for the same reason. In the following, we refer to this binary contact labels calculation as the contact function Γ.</p><p>Synchronization Since we jointly capture motion and foot pressure data with separate devices, our records must be accurately synchronized in absence of a genlock signal. To this end, subjects were asked to perform a simple control movement at the beginning and end of each capture sequence, consisting in an in-place double-leg jump. This enables matching vertical acceleration peaks measured on pressure insole IMUs with peaks computed from motion captured foot positions. Although numerical differentiation is known to amplify high frequency noise, we found that the framerate of our motion capture data was sufficiently high and measurement noise was small enough for synchronization.</p></div>
<div><head>Downsampling and Trimming</head><p>After synchronizing our data, we downsampled motion capture data from 240 Hz to 100 Hz using spherical linear interpolation to match the framerate of pressure insole data. Original motion sequences at 240 Hz are also provided in our database. We also automatically trimmed the beginning and end of each sequence to remove the synchronization patterns.</p></div>
<div><head n="4.">Deep Neural Network</head><p>In this section, we describe the proposed method to learn a deep neural network model estimating vGRF distributions from motion capture data. Learning vGRFs instead of binary contact labels encourages our deep neural network to more accurately model interactions between feet and ground, and enforces motion dynamics understanding. See Figure <ref type="figure" target="#fig_0">3</ref> for an overview of our approach.</p></div>
<div><head n="4.1.">Data Representation</head><p>Input At each frame t, the human pose Xt ∈ R J×3 is represented by the position of its J = 23 joints in a global Euclidean space. We design our deep network Ψ θ to output vGRFs and contact labels at each frame from a few surrounding input frames with padding when needed. The full input pose sequence is then X ∈ R T ×J×3 , where T is a variable number of frames.</p><p>Output As previously described, our deep network Ψ θ estimates the vGRF distribution from motion data. At each frame, it outputs F = Ψ θ (X) with F ∈ R T ×2×16 , i.e. 16 positive real-valued vGRF components, corresponding to the 16 insole pressure cells for each foot, expressed proportionally to subject weight.</p></div>
<div><head n="4.2.">Network Architecture</head><p>We designed our network Ψ θ to process variable-length sequences.</p><p>To this end, the network is composed of four 1D temporal convolutional layers with 7-frame wide kernels, followed by three fullyconnected layers applied at each frame independently to preserve the support variable-length sequence. Each convolutional or fullyconnected layer is followed by exponential linear units (ELU) as activation, except for the last one which is a softplus activation to output nonnegative vGRF components.</p></div>
<div><head n="4.3.">Training and Inference</head><p>During training, our network is iteratively exposed to sequences of human poses and tries to estimate corresponding vGRF components as depicted in Figure <ref type="figure" target="#fig_0">3</ref>. To encourage robustness and smooth convergence, we make use of stochastic data augmentation. First, similar to random crops and rotations used on images in computer vision, we apply random vGRF-invariant transformations on input pose sequences including translations, horizontal rotations, scaling, and left-right mirroring.</p><p>For each sequence, we also randomly draw its skeleton which is then animated by joint angles to further robustify our network. To do so, we precomputed (offline) an SVD basis of the skeletons captured in our database. At training time, we draw new skeletons by linearly combining precomputed singular vectors with randomly sampled weights. We then further edit these skeletons by randomly moving joint relative positions and rescaling bone lengths to obtain morphology variations. The resulting input motion sequences purposely suffer from artefacts since kinematic chains (i.e. from root joint to feet) have been randomly edited, which encourages the network to be resilient w.r.t. perturbed inputs. Joint positions are finally computed through forward kinematics and fed to the network.</p><p>To train our deep network Ψ θ , we minimize a reconstruction loss of vGRF components. Instead of the standard mean squared error (MSE), we minimize the mean squared logarithmic error (MSLE). It has the property to only focus on the relative difference between target and estimated values (see right-hand side of Equation <ref type="formula">1</ref>), which is convenient when the values considered can be several orders of magnitude apart. In our case, actual vGRFs can be strictly positive and arbitrarily low (e.g. during transition from the double leg stance to the single leg stance) as well as very high (e.g. during jump landing). The loss function used to train our network is then</p><formula xml:id="formula_0">L = 1 N N ∑ i=1 ln(F i + 1) -ln( Fi + 1) 2 = 1 N N ∑ i=1 ln 2 F i + 1 Fi + 1 (1)</formula><p>where F i are the ground truth vGRF components, and Fi = Ψ θ (X) i their estimated counterpart. Adding 1 to both F and F ensures that the loss is defined when vGRF component value goes to zero.</p><p>At inference, our deep network estimates vGRF components from joint positions as inputs. Then, foot contact labels can be calculated from vGRF estimates using the contact function Γ (see Section 3.3).</p></div>
<div><head n="5.">Evaluation</head><p>In this section we present results of our method to assess estimated vGRFs and detected foot contacts. After providing implementation details necessary for reproducibility in Section 5.1, we assess foot contact labels detection in Section 5.2 then vGRFs estimation in Section 5.3 on ground truth motion sequences. In Section 5.4, we also evaluate foot contact detection performance on different perturbed motion sequences, simulating challenging conditions encountered in concrete applications. Qualitative results illustrating estimated vGRFs and foot contact labels as well as sample motion sequences of our database are available in the supplementary material.</p></div>
<div><head n="5.1.">Implementation Details</head><p>Data We split the proposed <software>U NDER P RESSURE</software> database into two subsets, i.e. a training set and a testing set. To ensure robust evaluation, the testing set is composed of the sequences performed by three out of the ten subjects (1F+2M, {S8, S9, S10}), representing approximately 30% of the overall database. For training, we further divide the remaining 70% to keep a validation set (∼ 10%) and use early stopping during training. Moreover, we split each training motion sequence into overlapping windows of T = 240 frames, i.e.</p></div>
<div><head>2.4s.</head><p>Architecture The four convolutional layers at the beginning of our network have respectively 128, 128, 256, and 256 7-frame wide filters while the back-end fully-connected layers have 256 neurons each. Dropout with probability p = 0.2 is applied before each fullyconnected layer. The total number of weights in our network is approximately 1.1 million.</p><p>Training We implemented our deep neural network using <software>Py-Torch</software>. Training and validation were executed on an NVidia Tesla V100 GPU while other results were obtained either on an NVidia <software>GeForce</software> RTX 2060 GPU or on CPU. We trained our deep neural network through stochastic gradient descent (see Equation <ref type="formula">1</ref>) for about 2500 epochs (about 10 days) at each of which a new version of the training set was randomly generated (see Section 4.3). We used Adam optimization with a batch size of 64, learning rate α = 3 * 10 -5 and hyperparameters β 1 = 0.9 and β 2 = 0.999.</p></div>
<div><head n="5.2.">Foot Contacts Detection</head><p>Metric To evaluate foot contact detection, we use the F 1 score which is the harmonic mean of precision (fraction of correctly detected labels among detected labels) and recall (fraction of correctly detected labels among expected contact labels).</p><p>Baseline To evaluate our model against commonly used heuristicsbased approaches using thresholds (see Section 2), we define an optimal thresholds (OT) baseline. This model has two parameters that are thresholds on foot height relative to the ground and velocity norm. To demonstrate the effectiveness of our approach, we set these two parameters to the values which maximize the F 1 score over the training set, computed by recursive grid search. Note that in practical applications, such optimal thresholds are not available and must be guessed (hence our naming heuristics-based). This OT baseline therefore constitutes an upper bound in terms of performance over the F 1 score of such threshold-based approaches.</p><p>We also investigate the relevance of the proposed architecture with an ablative study. We first consider two linear models taking as inputs joint positions and velocities, as learned generalisations of the OT baseline: the former, called Linear-Feet, takes only foot and ankle joints as inputs while the latter, called Linear, takes all joints. Then, we consider a 3-layer MLP model, i.e. the architecture of the foot contact label detection module proposed by Smith et al. <ref type="bibr" target="#b51">[SCNW19]</ref> in their style transfer framework, which has three 128-neurons-wide layers and ReLU activations, introducing nonlinearities w.r.t. Linear-Feet and Linear models. These three models have real-valued outputs, for which positive values are considered as foot contacts with the ground, and are trained with a binary cross-entropy (BCE) loss. Finally, we explore two variants of our deep network. First, the Ours-C variant has the architecture proposed in Section 4.2 except for the last layer and activation which are adapted (i.e. number of neurons and sigmoid instead of softplus) to detect foot contact labels instead of estimating vGRF components, and is trained with BCE loss. The second variant corresponds to the combination of Ours-C with the main proposed model Ours. In this extended variant called Ours-C&amp;F, our architecture is adapted to output both contacts and vGRF components (convolutional layers are shared while fully-connected layers are duplicated and separately learned for each output) and trained with both MSLE (see Section 4.2) over vGRF components and BCE over contact labels.  Feet (only foot and ankle) suggests that contact labels detection can also benefit from other body joints, pointing out one of the limitation of thresholds based approaches that typically process only foot joints. The higher F 1 score of the 3-layer MLP model w.r.t. linear models confirms their limitations for foot contact label detection. Finally the proposed architecture further increases the detection accuracy with relatively small differences among variants.</p><p>Temporal Analysis of Foot Contact Detection Up to now, we provided temporally global foot contact detection results. However, misdetected labels located closer to contact phase changes are less severe in the sense that they would result in less severe biases or artefacts in most applications, e.g. footskate cleanup. To this end, we provide a finer analysis of foot contact labels. In Figure <ref type="figure">4</ref>, we plot the F 1 score against an increasing temporal tolerance to detection errors. In Figure <ref type="figure" target="#fig_1">5</ref>, the distribution of false positive rates is given according to a normalised temporal frame where ground truth off-contact phase intervals are mapped to [0, 1]. Both figures confirm the limitations of heuristics approaches represented by the OT baseline whose false positive rate is significant in the middle of off-contact phases. On the contrary, learned models show convergence of accuracy close to 100% with increasing tolerance as well as low false positive rates in the middle of off-contact phases.</p></div>
<div><head n="5.3.">vGRFs Estimation</head><p>Metrics In this section we assess performances of our method on vGRFs estimation from motion capture data. We use the root mean squared error (RMSE) of per-foot total vGRF which is mainly sensitive to global biases (at foot scale). In complement, we also evaluated the Center of Pressure (CoP) computed from estimated vGRF components, which is mostly sensitive to local errors (at pressure cell scale) but we provide the corresponding results in the supplementary material.</p><p>Table <ref type="table" target="#tab_7">3</ref> gives the RMSE of the estimated vGRFs proportionally to the subjects' body weight. As for contact detection, these results suggest that learning to model foot contact labels in addition to vGRF (Ours-C&amp;F) reduces accuracy, although the performances of both variants are close. Note that other variants evaluated on contact detection in the previous section only detect contact labels and hence cannot be evaluated on vGRF estimation.</p><p>In biomechanics, force plates are considered gold standard to measure vGRF and CoP; however, the environment in which we captured the proposed database (significant area including obstacles and stairs) prevented us to use force plates. For this reason, our evaluation considers pressure insole measures as ground truth for evaluation purposes. Existing works in biomechanics [NSM11,JMNB19] evaluated pressure insoles accuracy w.r.t. force plates, and tell us that vGRFs measured with pressure insoles suffer from a RMSE up to approximately 10% of the subject weight, being subject to variations depending on experimental conditions. Thus, Table <ref type="table" target="#tab_7">3</ref> indicate that vGRF estimation errors of our deep neural network are approximately of the same order of magnitude as the measurement error expected with pressure insoles.</p></div>
<div><head n="5.4.">Foot Contacts Detection in Challenging Conditions</head><p>As explained in Section 2, many applications requiring foot contact labels detection consist in quantifying or correcting foot artefacts. Figure <ref type="figure">6</ref>: F 1 score on foot contacts detection from motion sequences purposely noised with additive isotropic Gaussian noise. Each curve represents the F 1 score against the amount of noise introduced, measured with the MPJPE in centimeters indicated by the bottom horizontal axis, while the top horizontal axis gives the corresponding standard deviations of the Gaussian noise. The results indicate that our method is more robust to noise. By definition, motion sequences to be processed in such applications are not expected to be flawless. As a consequence, contact detection performance is not solely relevant for clean motion capture data (as evaluated in Section 5.2), but also for perturbed sequences. In this section, we evaluate the performance of our model on motion sequences in which we introduce three types of artefacts: additive Gaussian noise, distorsions caused by going through a partially trained autoencoder, and artefacts (mostly footskate) obtained by blending different motion sequences.</p><p>Motions Perturbed with Gaussian Noise First, we simply evaluate how the contact detection performance of the various models evaluated in Section 5.2 is affected by an increasing amount of Gaussian noise added to joint positions. Figure <ref type="figure">6</ref> displays the F 1 score obtained by the different models against the amount of noise introduced.</p></div>
<div><head>Motions Distorted by an Autoencoder</head><p>To simulate perturbations more faithful to what is encountered in real-world applications, we trained an autoencoder to reconstruct motions from our testset. We used a similar architecture to the convolutional autoencoder proposed by Holden et al. <ref type="bibr" target="#b15">[HSK16]</ref> and trained it for 100 epochs (about 10 minutes). We kept a selection of epochs at which the amount of perturbations introduced by the autoencoder follows a nice monotonically decreasing curve to avoid outlier epochs. At each of these epochs, we evaluate foot contact detection on motion sequences that have been encoded and then decoded to introduce increasing perturbations. Again we use the F 1 score as a foot contact labels detection performance metric and display in Figure <ref type="figure" target="#fig_3">7</ref> the results against the observed amount of noise introduced by the autoencoder at different epochs.</p><p>Footskate Generated with Motion Blending To push further our evaluation toward concrete applications, we additionally evaluate contact detection on motion sequences that have been blended. Motion blending (or motion interpolation) is a well-known technique widely used in animation that consists in mixing existing motions with dynamic blending weights to create new motions. In our case, motion blending is interesting because it is known to easily produce footskate in resulting motions, which can then help us to evaluate contact detection on such perturbed motions. To do so, we randomly picked in our testset pairs of motion subsequences having identical foot contact patterns on either left or right foot. This approach enables us to make the hypothesis that foot contact patterns are preserved in blended motions, and thus serve as ground truth. We constituted a set of approximately 40 ′ 000 blended motions of 80 frames long (i.e. 0.8s) and quantified the amount of footskate introduced using the mean horizontal velocity of the feet during contact phases [SZKS19, SZKZ20, ZSKS18, LZCvdP20]. We then evaluate once again foot contact detection using the F 1 score with these blended motions as inputs. We obtain the curves plotted in Figure <ref type="figure">8</ref> using simple moving average.</p><p>As depicted in Figures 6 to 8, the proposed approach for foot contact labels detection is much more robust than threshold-based heuristics approaches represented by the OT baseline, regardless of the type of perturbation applied to motion sequences. Moreover, the improvement of modelling vGRFs instead of foot contact labels (Ours vs Ours-C) is much larger when facing perturbed motion sequences. Since vGRFs are much more related to motion dynamics than binary contact label, vGRFs estimation requires a deeper understanding of motion than contact labels detection. Indeed, approximately 90% of contact labels can be correctly detected with foot position and velocity thresholding (see Table <ref type="table" target="#tab_5">2</ref>), i.e. with almost no understanding. Then, in challenging conditions like perturbed input motion sequences, the performances of the variant modelling vGRFs are logically more stable since a deeper understanding of motion is intuitively more robust.</p></div>
<div><head n="6.">Footskate Cleanup</head><p>In this section, we leverage our robust foot contact detection for a downstream task and propose a novel fully automatic workflow for footskate cleanup: the foot contact labels lead to kinematic constraints used in a dedicated optimisation-based inverse kinematics algorithm, where foot joints are constrained at ground contact Figure <ref type="figure">8</ref>: F 1 score against amount of footskate. Test motions with matching foot contact patterns have been blended to purposely introduce footskate while preserving contact labels. Foot contact detection is then evaluated on such motions suffering from footskate, and compared to the ground truth contact labels. The amount of footskate introduced through blending is measure using the mean velocity of the feet during contact phases. Non-linear learned detection models keep reasonably high accuracy while accuracy of linear models significantly decrease and accuracy of OT baseline quickly collapse.</p><p>points. The novelty of our approach is that the optimisation also includes the preservation of forces estimated by our network.</p><p>Let X = ( Q, S, P) be the input motion sequence suffering from footskate, where Q denote joint angles, S the skeleton and P the global trajectory positions of the root joint. Our goal is to find Q and P such that X = (Q, S, P) is the footskate-cleaned version. First, we compute F = Ψ Θ (FK( Q, S, P)) and C = Γ( F), being respectively the vGRF components estimated by our deep neural network Ψ Θ and the foot contact labels calculated using the contact function Γ defined in Section 3.3. The function FK(•) refers to forward kinematics, i.e. the computation of joint positions from joint angles, skeleton and global trajectory. After initialising Q and P with Q and P, we iteratively optimise both for a small fixed number N of iterations to best satisfy foot contact constraints into a gradientbased optimisation loop. In the following paragraphs we describe each term L i of the our loss function</p><formula xml:id="formula_1">L = ωqLquat + ω f L f oot + ωt L tra j + ωvL vGRFs<label>(2)</label></formula><p>where weights ω i are hyperparameters to balance our objectives.</p><p>Quaternions loss Lquat : we parameterise joint angles with unit quaternions, which are well-suited for such an optimisation since they are free of singularities, computationally efficient, and numerically stable [MHLC * 21]. To keep valid rotations with quaternions throughout the optimisation, Lquat penalise quaternion norm deviations from 1:</p><formula xml:id="formula_2">Lquat = ∑ t ∑ j Q j (t) -1 2 (3)</formula><p>where Q j (t) is the quaternion representing the orientation of joint j at time t.</p><p>Foot contacts loss L f oot : satisfying foot contact constraints is not an easy task since we do not have directly access to actual contact locations w.r.t. the skeleton. Moreover, since foot joints are lo- cated above actual contact points, they are allowed to rotate around the latter during part of contact phases (e.g. when the heel is in contact with the ground, the ankle is constrained to rotate around). To alleviate these issues, we artificially insert contact joints in S corresponding to contact points, i.e. under foot joints at the ground level (see figure <ref type="figure" target="#fig_4">9</ref>). We assume that each contact joint is located vertically under its foot joint in the T-pose at the ground height, has constant orientation w.r.t. its foot joint, and is assumed to be static during contact phases. Then, imposing zero velocity on contact joints instead of foot joints during the contact phases better reflects the kinematics of the interaction with the ground, allowing slight rotation of foot joints located above the ground level. To this end, we minimise the mean squared distance from contact joints to contact positions during contact phases to faithfully constrain foot joint positions through forward kinematics:</p><formula xml:id="formula_3">L f oot = C ∑ c=1 t1(c) ∑ t=t0(c)</formula><p>Π(t) FK(Q, S, P) jc (t)pc 2 (4) where jc and pc are respectively the joint in contact with the ground during contact phase c and its contact position during the contact phase spanning from t 0 (c) to t 1 (c). Moreover, function Π(•) is a rectangular wave function with smoothed edges (synchronised with contact phases) to avoid sharp foot position changes.</p><p>Trajectory loss L tra j : since foot contact constraints are not guaranteed to be reachable by leg extensions, the global trajectory might be affected. However, it is closely related to body dynamics and its optimisation must be carefully controlled to avoid artefacts. In particular, constraining trajectory positions might introduce implausible velocity changes, hurting motion dynamics realism, e.g. slowing down to reach some contact position might lead to speeding up afterwards to catch up the target trajectory. To this end, we minimise the trajectory velocity deviations instead of position deviations:</p><formula xml:id="formula_4">L tra j = ∑ t ∥V (t)∥ -∥ Ṽ (t)∥ 2 , V (t) = P(t + ∆T ) -P(t) ∆T<label>(5)</label></formula><p>where V (t) is the global velocity of root joint at time t, computed from global trajectory position P by numerical differentiation.</p><p>vGRFs invariance loss L vGRFs : the main novelty in our gradient-based optimisation approach is the use of our deep neural network through which gradients can flow. Since it is able to estimate consistent vGRF components from either clean or perturbed inputs, we further guide the optimisation by minimising the deviation between initially estimated vGRFs components F and dynamically estimated vGRFs components F = Ψ Θ (FK(Q, S, P)):</p><formula xml:id="formula_5">L vGRFs = MSLE(F, F) (6)</formula><p>where MSLE is the mean squared logarithmic error, an alternative to mean squared error suited for vGRFs (see Section 4.3).</p><p>We used the same motion sequences blended for evaluation purposes in Section 5.4 to test the proposed footskate cleanup workflow. With parameters N = 100, ωq = 10 -3 , ω f = 10 -5 , ωt = 10 2 and ωv = 5 • 10 -5 (see Equation <ref type="formula" target="#formula_1">2</ref>), we achieve to significantly reduce the amount of footskate (the velocity of the feet during contact phases is approximately halved) while noticeably improving the realism of the sequences, assessed by visual inspection. We provide corresponding visual results of our footskate cleanup workflow in the supplementary video.</p></div>
<div><head n="7.">Conclusion</head><p>In this article, we proposed a novel approach that improves the state-of-art of foot contact detection and footskate cleanup in human character animation. Building on <software>U NDER P RESSURE</software>, a novel motion capture database synchronised with pressure insoles which we release with this article, we proposed to learn the relationship between human motion and interactions between feet and ground with a deep neural network estimating vertical ground reaction forces, from which foot contact constraints can be derived and exploited for an effective removal of foot sliding artefacts.</p><p>As of today, the proposed database is one of a kind in animation as it provides synchronised motion capture and pressure insoles data for a wide variety of human motion. Our approach significantly outperforms thresholds-based heuristic approaches in detecting foot contact labels, which are limited in their capability to generalise. We show its robustness to perturbed input motion sequences, which is crucial in concrete use cases. We have shown that estimating forces greatly helps the accurate and robust detection of contacts. Finally, we demonstrate the usefulness of our foot contact detection approach for footskate cleanup, leveraging vGRF estimations to improve the quality of the results.</p><p>As described in Section 2, foot contact labels are relevant to a lot of tasks in human animation and are typically obtained with simple heuristics, or manually annotated on an exceptional basis. Thus, many of these downstream tasks could probably benefit from the improved accuracy of our foot contact detection approach. Moreover, motion reconstruction relies more and more on modelling physical interaction of the feet with the ground (see Section 2.2). Resolving inverse dynamics problems like ground reaction forces estimation from motion capture data could provide a valuable regularization for such underconstrained problems as well as other tasks involving physics-based models. We believe our approach is a step in that direction.</p></div><figure xml:id="fig_0"><head>ΓFigure 3 :</head><label>3</label><figDesc>Figure3: Overview of our approach. Synchronized vGRFS and motion capture database (green rectangle) provide inputs and targets to train deep network Ψ θ depicted in red. The blue trapezoid represents the contact function Γ (see Section 3.3). At runtime, our deep network augments motion data with vGRFs from which foot contact labels can be derived using the contact function Γ, both useful in many applications, e.g., reconstructing motion from images, cleaning footskate, finding suitable transition frames for motion blending, adapting animations to uneven terrain, and many more. As illustrated, both estimated vGRFs and detected contacts are evaluated in Sections 5.2 and 5.3, respectively.</figDesc></figure>
<figure xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: False positive rate distributions w.r.t. normalised offcontact phases. Ground truth off-contact phase intervals are mapped to [0, 1] and false positives are aggregated in this normalised temporal frame. Intuitively, the farther false positives are from the closest contact phase (i.e. 0 or 1 in the normalised temporal frame), the more severe they are. False positive rates of learned models like ours quickly decrease while the OT baseline keeps it significantly higher in-between contacts.</figDesc></figure>
<figure xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: F 1 score on foot contacts detection from motion sequences distorted by a motion autoencoder early-stopped at different epochs to emulate different amounts of distorsion. Each curve displays the F 1 score against the amount of distorsion introduced, measured with the MPJPE in centimeters. The results indicate that our method is more robust to distorted sequences.</figDesc></figure>
<figure xml:id="fig_4"><head>LFigure 9 :</head><label>9</label><figDesc>Figure 9: Illustration of contact joints (green dots) inserted under corresponding foot joints at the ground level in the skeleton to properly enforce foot contact constraints (See details below).</figDesc><graphic coords="11,180.66,77.15,95.70,95.70" type="bitmap" /></figure>
<figure type="table" xml:id="tab_0"><head /><label /><figDesc>02, KSG02, IAF06, LCB06]. Foot contact labels are helpful in numerous applications: they are often necessary to clean up foot artefacts such as foot sliding [KSG02], e.g. by enforcing foot constraints via Inverse Kinematics (IK) [MC12,HKS17,SZKS19,ALL</figDesc><table /><note><p>* 20,SZKZ20, HYNP20]; likewise, they are required to quantify such foot artefacts, e.g. for evaluation purposes [SZKS19, SZKZ20, ZSKS18, LZCvdP20]; they are also helpful to mitigate the well-known problem of mean collapse / drift away in human motion prediction, particularly with deterministic and recurrent models [HAB20,MBR17, WCX19, HYNP20], to disambiguate human motion modelling [XWCC15, HSK16, PGA18, PFAG20, HKS17], and more generally to leverage contact-based loss functions for increased quality and robustness [HKS17, LLL18, WHSZ19, PRL * 19, SAA * 20];</p></note></figure>
<figure type="table" xml:id="tab_2"><head /><label /><figDesc>Smith et al. [SCNW19] detected foot contact labels using a dedicated Multilayer Perceptron (MLP) to remove footskate artefacts through IK in a motion style transfer pipeline. Zou et al. [ZYC * 20], Shi et al. [SAA * 20] and Rempe et al. [RGH * 20] concurrently proposed to leverage foot contact detection to refine human motion estimation. In these works, foot contact labels are detected from 2D keypoints, themselves estimated from images using OpenPose [CHS * 19], a stateof-the-art 2D pose estimator. Both Zou et al. [ZYC * 20] and Rempe et al. [RGH * 20] used a dedicated module for contact detection while Shi et al. [SAA * 20] detected them together with 3D joint rotations and global root positions. Shimada et al. [SGXT20] extended the contact detection module from Zou et al. [ZYC</figDesc><table /></figure>
<figure type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Motion sequence categories in U NDER PRESSURE.   </figDesc><table><row><cell /><cell cols="3">Category</cell><cell /><cell /><cell /><cell cols="3">Motion Type</cell><cell /><cell /><cell cols="3">Duration [mn]</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell cols="3">slow walking</cell><cell /><cell /><cell /><cell /><cell>43.9</cell></row><row><cell cols="5">Locomotion,</cell><cell /><cell /><cell cols="3">normal walking</cell><cell /><cell /><cell /><cell /><cell>42.0</cell></row><row><cell /><cell /><cell cols="2">forwards</cell><cell /><cell /><cell /><cell cols="3">fast walking</cell><cell /><cell /><cell /><cell /><cell>42.9</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>running</cell><cell /><cell /><cell /><cell /><cell /><cell>30.1</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell cols="3">slow walking</cell><cell /><cell /><cell /><cell /><cell>21.1</cell></row><row><cell cols="5">Locomotion,</cell><cell /><cell /><cell cols="3">normal walking</cell><cell /><cell /><cell /><cell /><cell>21.8</cell></row><row><cell /><cell cols="3">backward</cell><cell /><cell /><cell /><cell cols="3">fast walking</cell><cell /><cell /><cell /><cell /><cell>20.8</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>running</cell><cell /><cell /><cell /><cell /><cell /><cell>15.5</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell cols="3">running sideways</cell><cell /><cell /><cell /><cell /><cell>12.3</cell></row><row><cell cols="5">Locomotion,</cell><cell /><cell /><cell /><cell>hopping</cell><cell /><cell /><cell /><cell /><cell /><cell>13.9</cell></row><row><cell cols="5">miscellaneous</cell><cell /><cell /><cell cols="3">stairs 1 at a time</cell><cell /><cell /><cell /><cell /><cell>18.0</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell cols="3">stairs 2 at a time</cell><cell /><cell /><cell /><cell /><cell>13.9</cell></row><row><cell cols="6">Locomotion with obstacles</cell><cell /><cell cols="3">stepping on obstacles stepping over obstacles jumping over obstacles</cell><cell /><cell /><cell /><cell /><cell>5.7 11.4 10.5</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell cols="3">leg stand-up</cell><cell /><cell /><cell /><cell /><cell>4.8</cell></row><row><cell /><cell /><cell>Idle</cell><cell /><cell /><cell /><cell /><cell cols="3">sit-down crouched down</cell><cell /><cell /><cell /><cell /><cell>4.8 4.8</cell></row><row><cell /><cell /><cell>Total</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>338.2</cell></row><row><cell>Left toe</cell><cell /><cell /><cell cols="4">14 15 16</cell><cell>Right toe</cell><cell>Head Right Clavicle Right Shoulder Right Elbow Right Hand</cell><cell>7 8 9 10 11</cell><cell>9</cell><cell>7 8</cell><cell>6 5</cell><cell>12</cell><cell>13</cell><cell>6 Neck 12 Left Clavicle 13 Left Shoulder 14 Left Elbow 15 Left Hand</cell></row><row><cell /><cell /><cell /><cell cols="5">9 10 11 12 13</cell><cell /><cell /><cell /><cell /><cell>4 3</cell><cell /><cell>14</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>10</cell><cell /><cell /><cell /><cell /></row><row><cell>Z</cell><cell>X</cell><cell>Y</cell><cell>7</cell><cell>Y</cell><cell>X</cell><cell>Z</cell><cell>8</cell><cell /><cell>11</cell><cell>16</cell><cell /><cell>2 1</cell><cell>20</cell><cell>15</cell><cell>5 T8 4 T12 3 L3</cell></row><row><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell /><cell>2 L5</cell></row><row><cell /><cell /><cell /><cell /><cell>5</cell><cell /><cell>6</cell><cell /><cell /><cell /><cell /><cell>17</cell><cell /><cell>21</cell><cell>1 Pelvis</cell></row><row><cell /><cell /><cell /><cell /><cell>1 3</cell><cell /><cell>2 4</cell><cell /><cell>Right Hip Right Knee Right Foot Right Toe</cell><cell>16 17 18 19</cell><cell /><cell /><cell /><cell>18</cell><cell>23 22</cell><cell>20 Left Hip 21 Left Knee 22 Left Foot 23 Left Toe</cell></row><row><cell>Left heel</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell>Right heel</cell><cell /><cell /><cell /><cell>19</cell><cell /><cell /></row><row><cell /><cell /><cell cols="3">(a) Insoles</cell><cell /><cell /><cell /><cell /><cell cols="6">(b) Skeleton</cell></row><row><cell cols="7">3.1. Motion Capture</cell><cell /><cell /><cell /><cell /><cell /><cell /><cell /></row><row><cell cols="15">Subjects were equipped with an Xsens MVN Link motion capture</cell></row><row><cell cols="15">system [SGB18]. The hardware consists of 17 inertial measure-</cell></row><row><cell cols="15">ment units (IMUs) running at 240 Hz embedded in the MVN Link</cell></row><row><cell cols="15">suit. Each IMU contains a 3D accelerometer, a gyroscope, and a</cell></row><row><cell cols="15">magnetometer. Capturing was performed using the Xsens MVN An-</cell></row><row><cell cols="15">imate software, an engine customised for 3D character animation</cell></row><row><cell cols="15">that combines tracking data of the 17 individual IMUs with a 23-</cell></row><row><cell cols="15">segment biomechanical model to obtain segment positions and ori-</cell></row><row><cell cols="15">entations. We calibrated MVN Animate for each subject with height,</cell></row><row><cell cols="15">arm span and shoe length measurements as inputs while other body</cell></row><row><cell cols="15">dimensions and proportions were estimated through the calibration,</cell></row><row><cell cols="15">as well as the orientation of motion trackers w.r.t. the correspond-</cell></row><row><cell cols="15">ing segments. After MVN Animate processing, motion data consist</cell></row><row><cell cols="15">of pose sequences with 23 segments sampled at 240 Hz.</cell></row></table><note><p>Figure 2: Left) Pressure cell layout of Moticon's OpenGo Sensor Insoles [AG21]. Blue (1 to 4) and red (9 to 16) cells are the groups of cells used to compute heel and toe contacts, respectively. Axes at insole centers represent inertial measurement units (IMUs). Right) Xsens MVN's [SGB18] motion capture skeleton with 23 joints.</p></note></figure>
<figure type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>F 1 score on foot contact labels detection of our method, its variants for ablative study purposes, and the OT baseline. Bold and underline respectively indicate per-column best and second best. Our method outperforms the OT baseline and its linear generalisations, and the proposed architecture seems relevant w.r.t. the 3layer MLP.</figDesc><table><row><cell>Model</cell><cell>Walking</cell><cell>Running</cell><cell>Obstacles</cell><cell>Hopping</cell><cell>Stairs</cell><cell>Idle</cell><cell>Overall</cell></row><row><cell>OT baseline</cell><cell cols="6">0.927 0.869 0.926 0.859 0.882 0.826</cell><cell>0.909</cell></row><row><cell>Linear-Feet</cell><cell cols="6">0.936 0.863 0.921 0.824 0.906 0.812</cell><cell>0.913</cell></row><row><cell>Linear</cell><cell cols="6">0.937 0.868 0.926 0.855 0.925 0.912</cell><cell>0.923</cell></row><row><cell>3-layer MLP</cell><cell cols="6">0.940 0.883 0.926 0.882 0.947 0.921</cell><cell>0.930</cell></row><row><cell>Ours-C</cell><cell cols="6">0.946 0.917 0.941 0.930 0.956 0.941</cell><cell>0.942</cell></row><row><cell>Ours-C&amp;F</cell><cell cols="6">0.948 0.918 0.942 0.923 0.954 0.946</cell><cell>0.943</cell></row><row><cell>Ours</cell><cell cols="6">0.949 0.930 0.944 0.931 0.959 0.948</cell><cell>0.947</cell></row></table></figure>
<figure type="table" xml:id="tab_6"><head>Table 2</head><label>2</label><figDesc>reports the F 1 score for each model and each motion category as well as overall results (right-most column). First, learned linear regressions show improved performances w.r.t. the OT baseline, which tends to confirm that thresholds based approaches lack complexity. The linear model (all joints) improvement w.r.t. Linear-Figure4: F 1 score curves against temporal tolerance. At any given tolerance t, the F 1 score is computed with contact labels considered as incorrect if and only if they are wrong and located at least t seconds away from the closest contact phase. Without any tolerance (left-most), F 1 scores correspond to the Overall column in table 2. The OT baseline and its linear generalisations have large errors far from contact phase changes, resulting in relatively low F 1 scores compared to our model even with high temporal tolerances.</figDesc><table><row><cell /><cell>1.00</cell><cell /><cell /><cell /><cell /></row><row><cell /><cell>0.99</cell><cell /><cell /><cell /><cell /></row><row><cell /><cell>0.98</cell><cell /><cell /><cell /><cell /></row><row><cell /><cell>0.97</cell><cell /><cell /><cell /><cell /></row><row><cell>F 1 score</cell><cell>0.92 0.93 0.94 0.95 0.96</cell><cell /><cell /><cell /><cell cols="2">3-layer MLP Linear Linear-Feet OT baseline</cell></row><row><cell /><cell>0.91</cell><cell /><cell /><cell /><cell>Ours</cell></row><row><cell /><cell>0.00 0.90</cell><cell>0.05</cell><cell>0.10</cell><cell>0.15</cell><cell>0.20</cell><cell>0.25</cell></row><row><cell /><cell /><cell /><cell>Tolerance [s]</cell><cell /><cell /></row></table></figure>
<figure type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Root mean squared error of the estimated vGRF normalised by body weight. Estimating foot contacts in addition to vGRFs (Ours-C&amp;F) seems slightly detrimental compared to estimating only vGRFs (Ours).</figDesc><table><row><cell>Model</cell><cell>Walking</cell><cell>Running</cell><cell>Obstacles</cell><cell>Hopping</cell><cell>Stairs</cell><cell>Idle</cell><cell>Overall</cell></row><row><cell>Ours-C&amp;F</cell><cell cols="6">9.5% 14.8% 12.4% 14.3% 11.8% 13.1%</cell><cell>11.4%</cell></row><row><cell>Ours</cell><cell cols="6">9.1% 14.3% 11.6% 14.1% 10.9% 11.9%</cell><cell>10.9%</cell></row></table></figure>
			<note place="foot" xml:id="foot_0"><p>© 2022 The Author(s) Computer Graphics Forum © 2022 The Eurographics Association and John Wiley &amp; Sons Ltd.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep Learning for Foot Contact Detection, Ground Reaction Force Estimation and Footskate Cleanup OpenGo-Sensor-Insole-Specification-A4SQ-RGB-EN-03</title>
		<author>
			<persName><forename type="first">Ag</forename><forename type="middle">M R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Le Clerc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">&amp;</forename><forename type="middle">P</forename><surname>Hellier</surname></persName>
		</author>
		<ptr target="https://moticon.com/wp-content/uploads/2021/09/L" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Opengo sensor insole specification</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Skeleton-aware networks for deep motion retargeting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3386569.3392462</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title />
		<author>
			<persName><surname>Alx</surname></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On learning symmetric locomotion</title>
		<author>
			<persName><forename type="first">F</forename><surname>Abdolhosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van De Panne</surname></persName>
		</author>
		<idno type="DOI">10.1145/3359566.3360070</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Motion, Interaction and Games</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-10">Oct. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Indirect measurement of ground reaction forces and moments by means of wearable inertial sensors: A systematic review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ancillao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tedesco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flynn</forename><forename type="middle">B</forename></persName>
		</author>
		<idno type="DOI">10.3390/s18082564</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018-08">Aug. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Motion abstraction and mapping with spatial constraints</title>
		<author>
			<persName><surname>Bindiganavale R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Badler</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-49384-0_6</idno>
	</analytic>
	<monogr>
		<title level="m">Modelling and Motion Capture Techniques for Virtual Environments</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="1998-03">Mar. 1998</date>
			<biblScope unit="page" from="70" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><surname>Wei S.-E</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2019.2929257</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="172" to="186" />
			<date type="published" when="2019-01">Jan. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title />
		<author>
			<persName><surname>Eka</surname></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Prediction of ground reaction forces for parkinson's disease patients using a kinect-driven musculoskeletal gait analysis model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eltoukhy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kuenze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sig-Norile</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.medengphy.2017.10.004</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Engineering &amp; Physics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="75" to="82" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title />
		<author>
			<persName><surname>Fck</surname></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimating 3d l5/s1 moments and ground reaction forces during trunk bending using a full-body ambulatory inertial motion capture system</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang C.-C</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dennerlein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Van Dieën</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.jbiomech.2015.11.042</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomechanics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="904" to="912" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Validation of a motion capture system for deriving accurate ground reaction forces without a force plate</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Fry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Herda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Sterczala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Andre</surname></persName>
		</author>
		<idno type="DOI">10.1186/s41044-016-0008-y</idno>
	</analytic>
	<monogr>
		<title level="j">Big Data Analytics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2016-09">Sept. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Moglow: Probabilistic and controllable motion synthesis using normalising flows</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alexanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
		<idno type="DOI">10.1145/3414685.3417836</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020-11">Nov. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human motion reconstruction from force sensors</title>
		<author>
			<persName><forename type="first">Ha</forename><forename type="middle">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename></persName>
		</author>
		<idno type="DOI">10.1145/2019406.2019424</idno>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH/Eurographics Symposium on Computer Animation</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011-08">2011. Aug. 2011</date>
			<biblScope unit="page" from="129" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Phase-functioned neural networks for character control</title>
		<author>
			<persName><forename type="first">D</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saito</surname></persName>
		</author>
		<idno type="DOI">10.1145/3072959.3073663</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A deep learning framework for character motion synthesis and editing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Holden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<idno type="DOI">10.1145/2897824.2925975</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016-07">July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust motion in-betweening</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yurick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<idno type="DOI">10.1145/3386569.3392480</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowing when to put your foot down</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ikemoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Arikan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<idno type="DOI">10.1145/1111411.1111420</idno>
	</analytic>
	<monogr>
		<title level="m">Symposium on Interactive 3D Graphics and Games</title>
		<imprint>
			<publisher>Association for Computing Machinery (ACM</publisher>
			<date type="published" when="2006-03">2006. Mar. 2006</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ground reaction force estimation using an insole-type pressure mat and joint kinematics during walking</title>
		<author>
			<persName><forename type="first">Jung</forename><forename type="middle">Y</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Koo</forename><forename type="middle">S</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.jbiomech.2014.05.007</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomechanics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2693" to="2699" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Foot centre of pressure and ground reaction force during quadriceps resistance exercises; a comparison between force plates and a pressure insole system</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jönsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Munkhammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Norrbrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Berg</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbiomech.2019.03.004</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomechanics</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="206" to="210" />
			<date type="published" when="2019-03">Mar. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title />
		<author>
			<persName><surname>Kbs</surname></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimation of ground reaction forces and moments during gait using only inertial motion capture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karatsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bellusci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Schepers</surname></persName>
		</author>
		<author>
			<persName><surname>De Zee M</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Veltink</surname></persName>
		</author>
		<idno type="DOI">10.3390/s17010075</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016-12">Dec. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast and flexible multilegged locomotion using learned centroidal dynamics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van De Panne</surname></persName>
		</author>
		<idno type="DOI">10.1145/3386569.3392432</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Footskate cleanup for motion capture editing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kovar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schreiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleicher</surname></persName>
		</author>
		<idno type="DOI">10.1145/545261.545277</idno>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH/Eurographics symposium on Computer animation</title>
		<imprint>
			<publisher>Association for Computing Machinery (ACM</publisher>
			<date type="published" when="2002-03">2002. Mar. 2002</date>
			<biblScope unit="page" from="97" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">4gait: Synchronized mocap, video, grf and emg datasets: Acquisition, management and applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kulbacki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Segen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Nowacki</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-05458-2_57</idno>
	</analytic>
	<monogr>
		<title level="m">6th Asian Conference on Intelligent Information and Database Systems</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014-04">Apr. 2014</date>
			<biblScope unit="page" from="555" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Residual analysis of ground reaction forces simulation during gait using neural networks with different configurations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Leporace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metsavaht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nadal</surname></persName>
		</author>
		<idno type="DOI">10.1109/EMBC.2015.7318976</idno>
	</analytic>
	<monogr>
		<title level="m">2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-11">Nov. 2015</date>
			<biblScope unit="page" from="2812" to="2815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust kinematic constraint detection for motion data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Boulic R</surname></persName>
		</author>
		<idno type="DOI">10.5555/1218064.1218103</idno>
		<ptr target="https://dl.acm.org/doi/10.5555/1218064.1218103.2" />
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH/Eurographics symposium on Computer animation</title>
		<imprint>
			<publisher>Association for Computing Machinery (ACM</publisher>
			<date type="published" when="2006-03">2006. Mar. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title />
		<author>
			<persName><surname>Lcr</surname></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Interactive control of avatars animated with human motion data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S A</forename><surname>Reitsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Pol-Lard</surname></persName>
		</author>
		<idno type="DOI">10.1145/566570.566607</idno>
	</analytic>
	<monogr>
		<title level="m">29th International Conference on Computer Graphics and Interactive Techniques</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002-03">Mar. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Interactive character animation by learning multi-objective control</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1145/3272127.3275071</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018-12">Dec. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title />
		<author>
			<orgName type="collaboration">LSC</orgName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Estimating 3d motion and forces of person-object interactions from monocular video</title>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">Z</forename><surname>Sedlar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carpentier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mansard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00884</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE Computer Society/Computer Vision Foundation (CVF</publisher>
			<date type="published" when="2019-03">Mar. 2019</date>
			<biblScope unit="page" from="8640" to="8649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Character controllers using motion vaes</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zinno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van De Panne</surname></persName>
		</author>
		<idno type="DOI">10.1145/3386569.3392422</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.497</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="4674" to="4683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Motion graphs++: a compact generative model for semantic motion analysis and synthesis</title>
		<author>
			<persName><forename type="first">Min</forename><forename type="middle">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.1145/2366145.2366172</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
	<note>MHLC *</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A survey on deep learning for skeleton-based human animation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mourot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hoyet</surname></persName>
		</author>
		<author>
			<persName><surname>Le Clerc F</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schnitzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hellier</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.14426</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021-11">Nov. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title />
		<author>
			<persName><surname>Mkd</surname></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prediction of ground reaction force and joint moments based on optical motion capture data during gait</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koeppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Potthast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Markert</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.medengphy.2020.10.001</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Engineering &amp; Physics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="29" to="34" />
			<date type="published" when="2020-10">Oct. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Foot contact detection through pressure insoles for the estimation of external forces and moments: application to running and walking</title>
		<author>
			<persName><forename type="first">P</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pontonnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dumont</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-03273616/document.3" />
	</analytic>
	<monogr>
		<title level="j">Computer Methods in Biomechanics and Biomedical Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2021-06">June 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title />
		<author>
			<persName><surname>Msz</surname></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Few-shot learning of homogeneous human locomotion styles</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13555</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="143" to="153" />
			<date type="published" when="2018-10">Oct. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A comparison of ground reaction forces determined by portable force-plate and pressureinsole systems in alpine skiing</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nakazato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Scheiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Müller</surname></persName>
		</author>
		<ptr target="https://pubmed.ncbi.nlm.nih.gov/24149570/.8" />
	</analytic>
	<monogr>
		<title level="j">Journal of Sports Science &amp; Medicine</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="754" to="762" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Modeling human motion with quaternion-based neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-019-01245-6</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="855" to="872" />
			<date type="published" when="2020-04">Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Quaternet: A quaternion-based recurrent model for human motion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<ptr target="https://dblp.org/rec/conf/bmvc/PavlloGA18.2" />
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference 2018</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2018-09">Sept. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of footskate cleanup</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pražák</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hoyet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sullivan</surname></persName>
		</author>
		<idno type="DOI">10.1145/2019406.2019444</idno>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH/Eurographics Symposium on Computer Animation</title>
		<imprint>
			<publisher>Association for Computing Machinery (ACM</publisher>
			<date type="published" when="2011-08">2011. Aug. 2011</date>
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning predictand-simulate policies from unorganized human motion data</title>
		<author>
			<persName><forename type="first">Park</forename><forename type="middle">S</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="DOI">10.1145/3355089.3356501</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning locomotion skills using deeprl: Does the choice of action space matter</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van De Panne</surname></persName>
		</author>
		<idno type="DOI">10.1145/3099564.3099567</idno>
	</analytic>
	<monogr>
		<title level="m">16th ACM SIGGRAPH/Eurographics Symposium on Computer Animation</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ambulatory assessment of 3d ground reaction force using plantar pressure distribution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rouhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Favre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Crevoisier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aminian</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.gaitpost.2010.05.014</idno>
	</analytic>
	<monogr>
		<title level="j">Gait &amp; Posture</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="316" />
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title />
		<author>
			<persName><surname>Rgh</surname></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Contact and human dynamics from monocular video</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rempe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><surname>Villegas R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58558-7_5</idno>
	</analytic>
	<monogr>
		<title level="m">16th European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020-09">Sept. 2020</date>
			<biblScope unit="page" from="71" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Motionet: 3d human motion reconstruction from monocular video with skeleton consistency</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aristidou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3407659</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020-09">Sept. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Efficient neural networks for real-time motion style transfer</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340254</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Computer Graphics and Interactive Techniques</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019-07">July 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Xsens MVN: Consistent Tracking of Human Motion Using Inertial Sensing</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Schepers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Giuberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bellusci</surname></persName>
		</author>
		<ptr target="https://www.researchgate.net/publication/324007368_Xsens_MVN_Consistent_Tracking_of_Human_Motion_Using_Inertial_Sensing.4" />
		<imprint>
			<date type="published" when="2018-03">Mar. 2018</date>
			<publisher>Xsens Technologies B.V</publisher>
			<biblScope unit="volume">559</biblScope>
			<biblScope unit="page">7500</biblScope>
			<pubPlace>Enschede, The Netherlands</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Tech. rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title />
		<author>
			<persName><surname>Sgx</surname></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neural monocular 3d human motion capture with physical awareness</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1145/3450626.3459825</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2021-07">July 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Physcap: Physically plausible monocular 3d motion capture in real time</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<idno type="DOI">10.1145/3414685.3417877</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2020-12">Dec. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">From image to stability: Learning dynamics from human pose</title>
		<author>
			<persName><forename type="first">J</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">T</forename><surname>Collins R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58592-1_32</idno>
	</analytic>
	<monogr>
		<title level="m">16th European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020-09">Sept. 2020</date>
			<biblScope unit="page" from="536" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neural state machine for character-scene interactions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saito</surname></persName>
		</author>
		<idno type="DOI">10.1145/3355089.3356505</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Local motion phases for learning multi-contact character movements</title>
		<author>
			<persName><forename type="first">S</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zama</surname></persName>
		</author>
		<idno type="DOI">10.1145/3386569.3392450</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Combining recurrent neural networks and adversarial training for human motion synthesis and control</title>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">Z</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2938520</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="28" />
			<date type="published" when="2019-09">Sept. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Spatiotemporal manifold learning for human motions via long-horizon modeling</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-L</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2936810</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2019-08">Aug. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning body shape variation in physics-based characters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1145/3355089.3356499</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Allsteps: Curriculum-driven learning of stepping stone skills</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van De Panne</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.14115</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2020-10">Oct. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Realtime style transfer for unlabeled heterogeneous human motion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<idno type="DOI">10.1145/2766999</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2015-07">July 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Lobstr: Real-time lower-body pose prediction from sparse upper-body tracking signals</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee S.-H</forename></persName>
		</author>
		<idno type="DOI">10.1111/cgf.142631</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="265" to="275" />
			<date type="published" when="2021-06">June 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning symmetric and lowenergy locomotion</title>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">W</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename></persName>
		</author>
		<idno type="DOI">10.1145/3197517.3201397</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Weakly-supervised learning of human dynamics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58574-7_5</idno>
	</analytic>
	<monogr>
		<title level="m">16th European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020-09">Sept. 2020</date>
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Modeadaptive neural networks for quadruped motion control</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saito</surname></persName>
		</author>
		<idno type="DOI">10.1145/3197517.3201366</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018-07">July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title />
		<author>
			<persName><surname>Zsz</surname></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Leveraging depth cameras and wearable pressure sensors for full-body kinematics and dynamics capture</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
		<idno type="DOI">10.1145/2661229.2661286</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Joint 3d human motion capture and physical analysis from monocular videos</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2017.9</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title />
		<author>
			<persName><surname>Zyc</surname></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Reducing footskate in human motion reconstruction with</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName><surname>Huang J.-B</surname></persName>
		</author>
		<imprint />
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>