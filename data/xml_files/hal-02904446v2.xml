<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MAGMA: Inference and Prediction using Multi-Task Gaussian Processes with Common Mean</title>
				<funder>
					<orgName type="full">U.S. Army Research Office</orgName>
				</funder>
				<funder>
					<orgName type="full">U.K. Ministry of Defence</orgName>
				</funder>
				<funder>
					<orgName type="full">U.S. Army Research Laboratory</orgName>
				</funder>
				<funder ref="#_4dgEsak">
					<orgName type="full">U.K. Engineering and Physical Sciences Research Council</orgName>
					<orgName type="abbreviated">EPSRC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-05-24">May 24, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Arthur</forename><surname>Leroy</surname></persName>
							<email>arthur.leroy.pro@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Latouche</surname></persName>
							<email>pierre.latouche@math.cnrs.fr</email>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Guedj</surname></persName>
							<email>benjamin.guedj@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Servane</forename><surname>Gey</surname></persName>
							<email>servane.gey@parisdescartes.fr</email>
						</author>
						<title level="a" type="main">MAGMA: Inference and Prediction using Multi-Task Gaussian Processes with Common Mean</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-24">May 24, 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">C130B896B2E19831681C91379690720B</idno>
					<idno type="DOI">10.1007/s10994-</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1-SNAPSHOT" ident="GROBID" when="2024-02-22T10:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multi-task learning</term>
					<term>Gaussian process</term>
					<term>EM algorithm</term>
					<term>Common mean process</term>
					<term>Functional data analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1 as a GP for which the hyper-posterior distribution is tractable. Therefore an EM algorithm is derived for handling both hyper-parameters optimisation and hyper-posterior computation. Unlike previous approaches in the literature, the model fully accounts for uncertainty and can handle irregular grids of observations while maintaining explicit formulations, by modelling the mean process in a unified GP framework. Predictive analytical equations are provided, integrating information shared across tasks through a relevant prior mean. This approach greatly improves the predictive performances, even far from observations, and may reduce significantly the computational complexity compared to traditional multi-task GP models. Our overall algorithm is called Magma (standing for Multi tAsk Gaussian processes with common MeAn). The quality of the mean process estimation, predictive performances, and comparisons to alternatives are assessed in various simulated scenarios and on real datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A novel multi-task Gaussian process (GP) framework is proposed, by using a common mean process for sharing information across tasks. In particular, we investigate the problem of time series forecasting, with the objective to improve multiple-step-ahead predictions. The common mean process is defined</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Gaussian processes (GPs) are a powerful tool, widely used in machine learning <ref type="bibr" target="#b6">(Bishop, 2006;</ref><ref type="bibr" target="#b26">Rasmussen and Williams, 2006)</ref>. The classic context of regression aims at inferring the underlying mapping function associating input to output data. In a probabilistic framework, a typical strategy is to assume that this function is drawn from a prior GP. Doing so, we may enforce some properties for the function solely by characterising the mean and covariance functions of the process, the latter often being associated with a specific kernel. This covariance function plays a central role and GPs are an example of kernel methods. We refer to <ref type="bibr">Álvarez et al. (2012)</ref> for a comprehensive review. On the other hand, the mean function is generally set to 0 for all entries assuming that the covariance structure already integrates the desired relationship between observed data and prediction targets. In this paper, we consider a novel multi-task learning framework where a series of GPs share a common mean, expressed as a GP as well. We demonstrate that modelling the mean function as such can be key to obtain more relevant predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>The multi-task framework consists in using data from several tasks (or individuals) to improve learning or predictive capacities compared to an isolated model. It has been introduced by <ref type="bibr" target="#b9">Caruana (1997)</ref> and then adapted in many fields of machine learning. GP versions of such models were introduced in <ref type="bibr" target="#b28">Schwaighofer et al. (2004)</ref>, which proposed an Expectation-Maximisation (EM) algorithm for learning. Similar techniques can be found in <ref type="bibr" target="#b32">Shi et al. (2005)</ref>. Meanwhile, <ref type="bibr" target="#b43">Yu et al. (2005)</ref> offered an extensive study of the relationships between the linear model and GPs to develop a multi-task GP formulation. However, since the introduction in <ref type="bibr">Bonilla et al. (2008)</ref> of the idea of two matrices, modelling covariance between inputs and tasks respectively, the term multi-task Gaussian process has mostly referred to the choice made regarding the covariance structure. Some further developments were discussed by <ref type="bibr" target="#b16">Hayashi et al. (2012)</ref>, <ref type="bibr">Rakitsch et al. (2013)</ref> and <ref type="bibr" target="#b44">Zhu and Sun (2014)</ref>. In particular, an interesting approach in Nguyen and Bonilla (2014) proposed a sparse approximation for multi-task GP inference. More generally, these approaches are known as examples of linear models of coregionalization (LMC) in the geostatistics literature, and <ref type="bibr" target="#b3">Álvarez and Lawrence (2011)</ref> provides a unified view on the topic as well as an efficient strategy for constructing computationally efficient approximations. Let us emphasise that the present paper is not based on the same assumptions and principles, and aims at defining a different multi-task paradigm for GPs, focusing on sharing information through the mean function rather than the covariance structure. Besides, the work of <ref type="bibr">Swersky et al. (2013)</ref> on Bayesian hyper-parameter optimisation in such LMC models is also worth a mention. Real applications were tackled by similar models in <ref type="bibr">Williams et al. (2009)</ref> and <ref type="bibr">Alaa and van der Schaar (2017)</ref>, while <ref type="bibr" target="#b11">Clingerman and Eaton (2017)</ref> and <ref type="bibr" target="#b19">Moreno-Muñoz et al. (2019)</ref> developed continual learning methods for multi-task GP.</p><p>As we focus on multi-task time series forecasting, a connection can be drawn to the study of multiple curves, or functional data analysis (FDA). As initially proposed in <ref type="bibr" target="#b27">Rice and Silverman (1991)</ref>, it is possible to model and learn mean and covariance structures simultaneously in this context. We refer to the monographs <ref type="bibr" target="#b25">Ramsay and Silverman (2005)</ref> and <ref type="bibr" target="#b15">Ferraty and Vieu (2006)</ref> for a comprehensive introduction to FDA. In particular, these books introduced several usual ways for modelling a set of functional objects in frequentist frameworks, for example by using a decomposition in a basis of functions (such as <ref type="bibr">Bsplines, wavelets, Fourier)</ref>. This kind of B-splines decomposition was used in <ref type="bibr" target="#b29">Shi et al. (2007)</ref> for modelling the mean function in a generative model that somehow resembles ours. Subsequently, some Bayesian alternatives were developed in <ref type="bibr" target="#b36">Thompson and</ref><ref type="bibr" target="#b36">Rosen (2008), and</ref><ref type="bibr" target="#b12">Crainiceanu and</ref><ref type="bibr" target="#b12">Goldsmith (2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our contributions</head><p>A multi-task GP framework with a common mean process is introduced, allowing reliable probabilistic forecasts even in multiple-step-ahead problems, or for sparsely observed individuals. For this purpose, (i) we introduce a GP model where the specific covariance structure of each task is defined through a separate kernel and its associated set of hyperparameters, whereas the common mean function µ 0 allows sharing information across tasks and overcomes the weaknesses of classic GPs in making predictions far from observed data. To account for uncertainty, we propose a hierarchical formulation to define the common mean process µ 0 as a GP as well. (ii) We derive an algorithm called Magma (available as an R package at https://github.com/ArthurLeroy/MagmaClustR) to compute µ 0 's hyper-posterior distribution together with the estimation of hyper-parameters in an EM fashion, and discuss its computational complexity. (iii) We enrich Magma with explicit formulas to make predictions for any new, partially observed, task. The hyper-posterior distribution of µ 0 provides a prior belief on what we would expect to observe before seeing any new data, acting as an already-informed mean process, integrating both trend and uncertainty coming from other tasks. (iv) We illustrate the performance of our method on synthetic and two real-life datasets and obtain state-of-the-art results compared to alternative approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Outline</head><p>The paper is organised as follows. We introduce our multi-task Gaussian process model in Sec. 2, along with notation. Sec. 3 is devoted to the inference procedure, with an Expectation-Maximisation (EM) algorithm to estimate the Gaussian process hyperparameters and µ 0 's hyper-posterior. We leverage this strategy in Sec. 4 and derive a prediction algorithm. In Sec. 5, we analyse and discuss the computational complexity of both the inference and prediction procedures. Our methodology is illustrated in Sec. 6, with a series of experiments on both synthetic and real-life datasets, and a comparison to competing state-of-the-art algorithms. On those tasks, we provide empirical evidence that our algorithm outperforms other approaches. Sec. 7 draws perspectives for future work, and we defer some proofs to original results claimed in the paper to Sec. 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation</head><p>While GPs can handle many types of data, their continuous nature makes them particularly well suited to study temporal phenomena. Throughout, the term individual is used as a synonym of task or batch, and we adopt notation and vocabulary of time series to remain consistent with the application on real dataset provided in Sec. 6.5, which addresses young swimmers performances' forecast.</p><p>We are provided with functional data coming from M ∈ I different individuals, where I ⊂ N. For each individual i, we observe a set of inputs {t 1 i , . . . , t N i i } and associated outputs {y i (t 1 i ), . . . , y i (t N i i )}, where N i is the number of data points for the i-th individual. Since many objects are defined for all individuals, we shorten our notation as follows: for any object x existing for all i, we denote {x i } i = {x 1 , . . . , x M }. Moreover, as we work in a temporal context, the inputs are referred to as timestamps. In the specific case where all individuals are observed at the same timestamps, we call the grid of observations common. On the contrary, a grid of observations is uncommon if the timestamps are different in number and/or location among the individuals. Some convenient notation follows:</p><formula xml:id="formula_0">• t i = {t 1 i , . . . , t N i i }</formula><p>, the set of timestamps for the i-th individual,</p><p>• y i = y i (t i ), the vector of outputs for the i-th individual,</p><p>• t = M i=1 t i , the pooled set of timestamps among individuals,</p><p>• N = card(t), the total number of observed timestamps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model and hypotheses</head><p>Suppose that functional data are coming from the sum of a mean process, common to all individuals, and an individual-specific centred process. To clarify relationships in the generative model, we illustrate our graphical model in Fig. <ref type="figure">1</ref>. Let T be the input space, our model is</p><formula xml:id="formula_1">y i (t) = µ 0 (t) + f i (t) + ϵ i (t), ∀t ∈ T , ∀i ∈ I, where µ 0 (•) ∼ GP(m 0 (•), k θ 0 (•, •)) and f i (•) ∼ GP (0, c θ i (•, •))</formula><p>are respectively the common mean and individual specific processes. Moreover, the error term is supposed to be ϵ</p><formula xml:id="formula_2">i (•) ∼ N (0, σ 2 i I).</formula><p>The following notation is used for parameters:</p><p>• m 0 (•), an arbitrary prior mean function,</p><p>• k θ 0 (•, •), a covariance kernel of hyper-parameters θ 0 ,</p><p>• ∀i ∈ I, c θ i (•, •), a covariance kernel with hyper-parameters θ i ,</p><p>• σ 2 i ∈ R + , the noise variance associated with the i-th individual,</p><p>• ∀i ∈ I, we define the shorthand</p><formula xml:id="formula_3">ψ θ i ,σ 2 i (•, •) = c θ i (•, •) + σ 2 i I, y i µ 0 m 0 θ 0 f i θ i ϵ i σ 2 i N N N ∀i ∈ I</formula><p>Figure <ref type="figure">1</ref>: Graphical model of dependencies between variables in the Multi-task Gaussian Process model.</p><p>• Θ = {θ 0 , {θ i } i , σ 2 i i }, the set of all hyper-parameters to learn in the model.</p><p>We also assume that:</p><formula xml:id="formula_4">• {f i } i are independent,</formula><p>• {ϵ i } i are independent,</p><p>• ∀i ∈ I, µ 0 , f i and ϵ i are independent.</p><p>It follows that {y i | µ 0 } i=1,...,M are independent from one another, and for all i ∈ I:</p><formula xml:id="formula_5">y i (•) | µ 0 (•) ∼ GP(µ 0 (•), ψ θ i ,σ 2 i (•, •)).</formula><p>Let us emphasise that this property only holds conditionally to µ 0 . Otherwise, once µ 0 is integrated out, the y i are no longer independent. Here, we do not assume any specific covariance structure between individuals contrarily to standard LMC approaches. As we shall see in the next sections, the process µ 0 will be key to handle the dependencies and share information across the individuals.</p><p>Although this model is based on infinite-dimensional GPs, the inference will be conducted on a finite grid of observations. According to the aforementioned notation, we observe {(t i , y i )} i , and the corresponding likelihoods are Gaussian:</p><formula xml:id="formula_6">y i | µ 0 (t i ) ∼ N (y i ; µ 0 (t i ), Ψ t i θ i ,σ 2 i ),</formula><p>where</p><formula xml:id="formula_7">Ψ t i θ i ,σ 2 i = ψ θ i ,σ 2 i (t i , t i ) = ψ θ i ,σ 2 i (k, l) k,ℓ∈t i is a N i × N i covariance matrix. Since t i</formula><p>might be different among individuals, we also need to evaluate µ 0 on the pooled grid of timestamps t:</p><formula xml:id="formula_8">µ 0 (t) ∼ N µ 0 (t); m 0 (t), K t θ 0 , where K t θ 0 = k θ 0 (t, t) = [k θ 0 (k, ℓ)] k,l∈t is a N × N covariance matrix.</formula><p>An alternative hypothesis consists in considering hyper-parameters {θ i } i and σ 2 i i equal for all individuals. We call this hypothesis Common HP (where HP stands for hyper-parameters) in the Sec. 6. This particular case represents a context where individuals correspond to different trajectories of the same process, whereas different hyper-parameters indicate different covariance structures and thus a more flexible model. For the sake of generality, the remainder of the paper is written with θ i and σ 2 i notation, when there are no differences in the procedure. Moreover, the model above and the subsequent algorithm may use any form of covariance function, often parametrised by a finite set (usually small) of hyper-parameters. For example, a common kernel in the GP literature is known as the Exponentiated Quadratic kernel (also called sometimes Squared Exponential or Radial Basis Function kernel). It solely depends on two hyper-parameters θ = {v, ℓ} and is defined as:</p><formula xml:id="formula_9">k EQ x, x ′ = v 2 exp - (x -x ′ ) 2 2ℓ 2 . (<label>1</label></formula><formula xml:id="formula_10">)</formula><p>The Exponentiated Quadratic kernel is simple and enjoys useful smoothness properties. This is the kernel used in the current version of our implementation (see Sec. 6 for details). Note that there is a rich literature on kernel choice, their construction and properties, which is beyond the scope of the present work: we refer to Rasmussen and <ref type="bibr" target="#b26">Williams (2006)</ref> or <ref type="bibr" target="#b14">Duvenaud (2014)</ref> for comprehensive studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning</head><p>Several approaches to learn hyper-parameters for Gaussian processes have been proposed in the literature, we refer to Rasmussen and <ref type="bibr" target="#b26">Williams (2006)</ref> for a comprehensive study. One classical approach, called empirical Bayes <ref type="bibr" target="#b10">(Casella, 1985)</ref>, is based on the maximisation of an explicit likelihood to estimate hyper-parameters. This procedure avoids sampling from intractable distributions, usually resulting in additional computational cost and complicating practical use in moderate to large sample sizes. As previously stated, once µ 0 is marginalised out, the log-likelihood cannot be written as a sum of Gaussian log-likelihoods any more. Therefore, we propose an EM algorithm (see the pseudocode in Algorithm 1) to learn the hyper-parameters Θ in this context. The procedure alternatively computes the hyper-posterior distribution p(µ 0 | (y i ) i , Θ) with current hyper-parameters, and then optimises Θ according to this hyper-posterior distribution. This EM algorithm converges to local maxima <ref type="bibr" target="#b13">(Dempster et al., 1977)</ref>, typically in a handful of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E step</head><p>For the sake of simplicity, we assume in that section that ∀i, j ∈ I, t i = t j = t, i.e. the individuals are observed on a common grid of timestamps. We provide a generalisation of the following proposition in Sec. 4 (Proposition 4.1), where the result holds for uncommon grids. The E step then consists in computing the hyper-posterior distribution of µ 0 (t).</p><p>Proposition 3.1. Assume the hyper-parameters Θ known from initialisation or estimated from a previous M step. The hyper-posterior distribution of µ 0 remains Gaussian:</p><formula xml:id="formula_11">p µ 0 (t) | {y i } i , Θ = N µ 0 (t); m 0 (t), K t ,<label>(2)</label></formula><p>with</p><formula xml:id="formula_12">• K t = K t θ 0 -1 + M i=1 Ψ t θ i , σ 2 i -1 -1 , • m 0 (t) = K t K t θ 0 -1 m 0 (t) + M i=1 Ψ t θ i , σ 2 i -1 y i .</formula><p>Proof.</p><p>We omit specifying timestamps in what follows since each process is evaluated on t. Therefore, we can write:</p><formula xml:id="formula_13">p µ 0 | {y i } i , Θ ∝ p {y i } i | µ 0 , Θ p µ 0 | Θ ∝ M i=1 p y i | µ 0 , θ i , σ 2 i p µ 0 | θ 0 ∝ M i=1 N y i ; µ 0 , Ψ θ i , σ 2 i ) N µ 0 ; m 0 , K θ 0 .</formula><p>The term L 1 = -(1/2) log p(µ 0 | {y i } i , Θ) may then be written as</p><formula xml:id="formula_14">L 1 = - 1 2 log p(µ 0 | {y i } i , Θ) = M i=1 (y i -µ 0 ) ⊺ Ψ -1 θ i , σ 2 i (y i -µ 0 ) + (µ 0 -m 0 ) ⊺ K -1 θ 0 (µ 0 -m 0 ) + C 1 = M i=1 µ ⊺ 0 Ψ -1 θ i , σ 2 i µ 0 -2µ ⊺ 0 Ψ -1 θ i , σ 2 i y i + µ ⊺ 0 K -1 θ 0 µ 0 -2µ ⊺ 0 K -1 θ 0 m 0 + C 2 = µ ⊺ 0 K -1 θ 0 + M i=1 Ψ -1 θ i , σ 2 i µ 0 -2µ ⊺ 0 K -1 θ 0 m 0 + M i=1 Ψ -1 θ i , σ 2 i y i + C 2 ,</formula><p>where the constant terms are gathered into C 1 , C 2 ∈ R. Identifying terms in the quadratic form with the Gaussian likelihood, we get the desired result.</p><p>The maximisation step depends on the assumptions on the generative model, resulting in two versions for the EM algorithm (the E step is common to both, the branching point is here).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M step: different hyper-parameters</head><p>Assuming each individual has its own set of hyper-parameters {θ i , σ 2 i }, the M step is given by the following procedure.</p><formula xml:id="formula_15">Proposition 3.2. Assume p(µ 0 | {y i } i ) = N µ 0 (t); m 0 (t), K t computed in a previous E step. For a set of hyper-parameters Θ = {θ 0 , {θ i } i , σ 2 i i }, optimal values are given by Θ = argmax Θ E µ 0 |{y i } i [ p({y i } i , µ 0 (t) | Θ) ] ,</formula><p>inducing M + 1 independent maximisation problems:</p><formula xml:id="formula_16">θ 0 = argmax θ 0 L t m 0 (t); m 0 (t), K t θ 0 , ( θ i , σ 2 i ) = argmax θ i ,σ 2 i L t i (y i ; m 0 (t), Ψ t i θ i ,σ 2 i ), ∀i, where L t (x; m, S) = log N (x; m, S) - 1 2 Tr K t S -1 .</formula><p>Proof.One simply has to distribute the conditional expectation in order to get the right likelihood to maximise, and then notice that the function can be written as a sum of M +1 independent (with respect to the hyper-parameters) terms. Moreover, by rearranging, one can observe that each independent term is the sum of a Gaussian likelihood and a correction trace term. See Sec. 8.2 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M step: common hyper-parameters</head><p>Alternatively, assuming all individuals share the same set of hyper-parameters {θ, σ 2 }, the M step is given by the following procedure.</p><formula xml:id="formula_17">Proposition 3.3. Assume p(µ 0 | {y i } i ) = N µ 0 (t); m 0 (t), K t computed in a previous E step.</formula><p>For a set of hyper-parameters Θ = {θ 0 , θ, σ 2 }, optimal values are given by</p><formula xml:id="formula_18">Θ = argmax Θ E µ 0 |{y i } i [ p({y i } i , µ 0 (t) | Θ) ] ,</formula><p>inducing two independent maximisation problems:</p><formula xml:id="formula_19">θ 0 = argmax θ 0 L t m 0 (t); m 0 (t), K t θ 0 , ( θ, σ 2 ) = argmax θ,σ 2 L M (θ, σ 2 ),</formula><p>where</p><formula xml:id="formula_20">L M (θ, σ 2 ) = M i=1 L t i (y i ; m 0 (t), Ψ t i θ,σ 2 ).</formula><p>Proof.We use the same strategy as for Proposition 3.2, see Sec. 8.2 for details.</p><p>In both cases, explicit gradients associated with the likelihoods to maximise are available, facilitating the optimisation with gradient-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Initialisation</head><p>To implement the EM algorithm described above, several constants must be (appropriately) initialised:</p><p>• m 0 (•), the mean parameter from the hyper-prior distribution of the process µ 0 (•). A somewhat classical choice in GP is to set its value to a constant function, typically 0 in the absence of external knowledge. Notice that, in our multi-task framework, the influence of m 0 (•) in hyper-posterior computation decreases as M grows anyway (see Proposition 3.1).</p><p>• Initial values for kernel parameters θ 0 and {θ i } i . Those strongly depend on the chosen kernel and its properties. We advise initiating θ 0 and {θ i } i with close values, as a too large difference might induce nearly singular covariance matrices and result in numerical instability (typical in GPs applications). In such pathological regime, the influence of a specific individual tends to overtake others in the calculus of µ 0 's hyper-posterior distribution.</p><p>• Initial values for the variance of the error terms σ 2 i i . This choice mostly depends on the context and properties of the dataset. We suggest avoiding initial values with more than an order of magnitude different from the variability of data. In particular, a too high value might result in a model mostly capturing noise.</p><p>As a final note, let us stress that the EM algorithm depends on the initialisation and is only guaranteed to converge to local maxima of the likelihood function <ref type="bibr" target="#b17">(McLachlan and Krishnan, 2007)</ref>. Several strategies have been considered in the literature to tackle this issue such as simulated annealing <ref type="bibr" target="#b38">(Ueda and Nakano, 1998)</ref> or repeated short runs <ref type="bibr" target="#b5">(Biernacki et al., 2003)</ref>. In this work, we chose the latter option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pseudocode</head><p>We wrap up this section with the pseudocode of the EM component of our complete algorithm, which we call Magma (standing for Multi tAsk Gaussian processes with common MeAn). The corresponding code is available at https://github.com/ArthurLeroy/ MAGMA.</p><formula xml:id="formula_21">Algorithm 1 Magma: EM component Initialise m 0 and Θ = θ 0 , {θ i } i , σ 2 i i . while not converged do E step: Compute the hyper-posterior distribution p(µ 0 | {y i } i , Θ) = N ( m 0 , K).</formula><p>M step: Estimate hyper-parameter by maximising Θ = argmax</p><formula xml:id="formula_22">Θ E µ 0 |{y i } i [ p({y i } i , µ 0 | Θ) ] .</formula><p>end while return Θ, m 0 , K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion of EM algorithms and alternatives</head><p>Let us stress that even though we focus on prediction purpose in this paper, the output of the EM algorithm already provides results on related FDA problems. The generative model in <ref type="bibr" target="#b41">Yang et al. (2016)</ref> describes a Bayesian framework that resembles ours to smooth multiple curves simultaneously. However, modelling variance structure with an Inverse-Wishart process forces the use of an MCMC algorithm for inference or the introduction of a more tractable approximation in <ref type="bibr" target="#b42">Yang et al. (2017)</ref>. One can think of the learning through Magma and applying a single task GP regression on each individual as an empirical Bayes counterpart to their approach. Meanwhile, µ 0 's hyper-posterior distribution also provides the probabilistic estimation of a mean curve from a set of functional data. The closest method to our approach can be found in <ref type="bibr" target="#b29">Shi et al. (2007)</ref> and the following book <ref type="bibr" target="#b31">Shi and Choi (2011)</ref>.The authors also work in the context of a multi-task GP model, and one can retrieve the idea of defining a mean function µ 0 to overcome the weaknesses of classic GPs in making predictions far from observed data. However, since their model uses B-splines to estimate this mean function, the method only works if all individuals share the same grid of observations, and does not account for uncertainty over µ 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Prediction</head><p>Once the hyper-parameters of the model have been learned, we can focus on our main goal: prediction for new individuals at unobserved timestamps. Since Θ is known and for the sake of concision, we omit conditioning on Θ in the sequel. Note there are two cases for prediction (referred to as Type I and Type II in Shi and Cheng, 2014, Section 3.2.1), depending on whether we observe some data or not for any new individual we wish to predict on. We denote by the index * a new individual for whom we want to make a prediction, say at timestamps t p . If there are no available data for this individual, we have no * -specific information, and the prediction is merely given by p(µ 0 (t p ) | {y i } i ). This quantity may be considered as the 'generic' (or Type II ) prediction according to the trained model, and only informs us through the mean process. Computing p(µ 0 (t p ) | {y i } i ) is also one of the steps leading to the prediction for a partially observed new individual (Type I ). The latter being the most compelling case, we consider Type II prediction as a particular case of the full Type I procedure, described below.</p><p>If we observe {t * , y * (t * )} for the new individual, the multi-task GP prediction is obtained in our model by computing the posterior distribution p(y * (t p ) | y * (t * ), {y i } i ). Note that the conditioning is taken over y * (t * ), as for any GP regression, but also on {y i } i , which is specific to our multi-task setting. The procedure for computing this distribution requires to successively complete the following steps:</p><p>1. choose a grid of prediction t p and define the pooled vector of timestamps t </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Posterior inference on the mean process</head><p>As mentioned above, we observed a new individual at timestamps t * . The GP regression consists in arbitrarily choosing a vector t p of timestamps for which we aim at making predictions. Then, we define new notation for the pooled vector of timestamps t p * = t p t * , which will serve as a working grid to define the prior and posterior distributions involved in the prediction process. One can note that, although not mandatory in theory, it is often a good idea to include the observed timestamps of training individuals, t, within t p * since they match locations that contain information for the mean process to 'help' the prediction. In particular, if t p * = t, the computation of µ 0 's hyper-posterior distribution is not necessary since p(µ 0 (t) | {y i } i ) has previously been obtained from the EM algorithm. However, in general, it is necessary to compute the hyper-posterior p(µ 0 (t p * ) | {y i } i ) at the new timestamps. The idea remains similar to the E step aforementioned, and we obtain the following result.</p><formula xml:id="formula_23">• K p * = K-1 + M i=1 Ψ-1 i -1 , • m 0 (t p * ) = K p * K-1 m 0 (t p * ) + M i=1 Ψ-1 i ỹi ,</formula><p>where we used the shortening notation:</p><formula xml:id="formula_24">• K = k θ 0 (t p * , t p * ) ( Ñ × Ñ matrix), • ỹi = 1 [t∈t i ] × y i (t) t∈t p * ( Ñ -size vector), • Ψi = 1 [t,t ′ ∈t i ] × ψ θ i , σ 2 i (t, t ′ ) t,t ′ ∈t p * ( Ñ × Ñ matrix).</formula><p>Proof.The sketch of the proof is similar to Proposition 3.1 in the E step. The only technicality consists in dealing carefully with the dimensions of vectors and matrices involved, and whenever relevant, to define augmented versions of y i and Ψ θ i , σ 2 i with 0 elements at unobserved timestamps' position for the i-th individual. Note that if we pick a vector t p Proof.To compute this prior, we need to integrate out the mean process µ 0 in p(y * | µ 0 , {y i } i ), whereas the multi-task aspect remains through the conditioning over {y i } i . We omit the writing of timestamps, by using the simplified notation µ 0 and y * instead of µ 0 (t p * ) and y * (t p * ), respectively. We first use the assumption that {y i | µ 0 } i∈{1,...,M } ⊥ ⊥ y * | µ 0 , i.e., the individuals are independent conditionally to µ 0 . Then, one can notice that the two distributions involved within the integral are Gaussian, which leads to the explicit Gaussian target distribution after integration.</p><formula xml:id="formula_25">p(y * | {y i } i ) = p (y * , µ 0 | {y i } i ) dµ 0 = p (y * | µ 0 , {y i } i )p(µ 0 | {y i } i ) dµ 0 = p (y * | µ 0 )) N y * ;µ 0 ,Ψ t p * θ * ,σ 2 * p(µ 0 | {y i } i ) N (µ0; m 0 , K p * ) dµ 0 .</formula><p>This convolution of two Gaussians remains Gaussian <ref type="bibr">(Bishop, 2006, Chapter 2.3.3)</ref>. The mean parameter is then given by</p><formula xml:id="formula_26">E y * |{y i } i [ y * ] = y * p (y * | {y i } i ) dy * = y * p (y * | µ 0 ) p(µ 0 | {y i } i ) dµ 0 dy * = y * p (y * | µ 0 ) dy * p(µ 0 | {y i } i ) dµ 0 = E y * |µ 0 [ y * ] p(µ 0 | {y i } i ) dµ 0 = E µ 0 |{y i } i E y * |µ 0 [ y * ] = E µ 0 |{y i } i [ µ 0 ] = m 0 .</formula><p>Following the same idea, the second-order moment is given by</p><formula xml:id="formula_27">E y * |{y i } i y 2 * = E µ 0 |{y i } i E y * |µ 0 y 2 * = E µ 0 |{y i } i V y * |µ 0 [ y * ] + E y * |µ 0 [ y * ] 2 = Ψ θ * ,σ 2 * + E µ 0 |{y i } i µ 2 0 = Ψ θ * ,σ 2 * + V µ 0 |{y i } i [ µ 0 ] + E µ 0 |{y i } i [ µ 0 ] 2 = Ψ θ * ,σ 2 * + K + m 2 0 , hence V y * |{y i } i [ y * ] = E y * |{y i } i y 2 * -E y * |{y i } i [ y * ] 2 = Ψ θ * ,σ 2 * + K + m 2 0 -m 2 0 = Ψ θ * ,σ 2 * + K.</formula><p>Note that the process y * (•) | {y i } i is not strictly a GP, although its finite-dimensional evaluation (3) remains Gaussian. The covariance structure cannot be expressed as a kernel that could be directly evaluated at any timestamps: the process is known as a degenerated GP. In practice however, this does not bear much consequence as any arbitrary vector of timestamps τ can be chosen at first, and computing hyper-posterior p(µ 0 (τ ) | {y i } i ) still yields to the Gaussian distribution p(y * (τ ) | {y i } i ) as above. For the sake of simplicity, we now rename the covariance matrix of the multi-task prior distribution:</p><formula xml:id="formula_28">K p * + Ψ t p * θ * ,σ 2 * = Γ p * = Γ pp Γ p * Γ * p Γ * * ,</formula><p>where the indices in the blocks of the matrix correspond to the associated timestamps t p and t * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Learning the new hyper-parameters</head><p>When we collect data points for a new individual, as in the single-task GPs setting, we would need to learn the hyper-parameters of its covariance kernel before making predictions. A salient fact in our multi-task approach is that we consider this step being part of the prediction process, for two main reasons. First, the model is already trained for individuals i = 1, . . . , M , and this training is independent of the future individual * or the choice of prediction timestamps. Since learning these new hyper-parameters requires knowledge of µ(t p * ) and thus of the prediction timestamps, we cannot compute them beforehand. Second, learning these hyper-parameters with the empirical Bayes approach only requires maximisation of a Gaussian likelihood which is negligible in computing time compared to the previous EM algorithm. As for single-task GP, we have the following estimates for hyper-parameters:</p><formula xml:id="formula_29">Θ * = argmax Θ * p(y * (t * ) | {y i } i , Θ * ) = argmax Θ * N y * (t * ); m 0 (t * ), Γ Θ * * * .</formula><p>Note that this step is optional depending on the modelling assumption: in the common hyper-parameters model (i.e. (θ, σ 2 ) = (θ i , σ 2 i ), ∀i ∈ I), any new individual will also share the same hyper-parameters and we already have Θ * = ( θ * , σ 2 * ) = ( θ, σ 2 ) from the EM algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Prediction</head><p>We can rewrite the multi-task prior distribution, by separating observed and prediction timestamps, as:</p><formula xml:id="formula_30">p(y * (t p * ) | {y i } i ) = p(y * (t p ), y * (t * ) | {y i } i ) = N (y * (t p * ); m 0 (t p * ), Γ p * ) = N y * (t p ) y * (t * ) ; m 0 (t p ) m 0 (t * ) , Γ pp Γ p * Γ * p Γ * * .</formula><p>As usual, the conditional distribution remains Gaussian, and the multi-task posterior distribution is given by:</p><formula xml:id="formula_31">p(y * (t p ) | y * (t * ), {y i } i ) = N y * (t p ); µ p 0 , Γ p ,</formula><p>where:</p><formula xml:id="formula_32">• µ p 0 = m 0 (t p ) + Γ p * Γ -1 * * (y * (t * ) -m 0 (t * )) , • Γ p = Γ pp -Γ p * Γ -1 * * Γ * p .</formula><p>Although this predictive distribution presents a formulation nicely analogous to standard GPs, let us emphasise on the terms m 0 (t p * ) and Γ p * , which embed crucial information from training individuals for the mean prediction to be more relevant even in far from the observed points y * (t * ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Complexity analysis for training and prediction</head><p>Computational complexity is of paramount importance in GPs as it quickly scales with large datasets. The classical cost to train a GP is O(N 3 ), and O(N 2 ) for prediction (Rasmussen and <ref type="bibr" target="#b26">Williams, 2006)</ref> where N is the number of data points (although there exist various sparse approximations, see Sec. 7 for references). Moreover, multi-task GP models lying on LMC approaches typically present a complexity of O(M 3 N 3 ) in training, which can be diminished when using sparse approximations <ref type="bibr" target="#b3">( Álvarez and Lawrence, 2011)</ref>. As detailed below, our model reaches a reduction to O((M +1)N 3 ) for the training complexity in a similar context (common grid of timestamps for all individuals), without using any sparse approximation.</p><p>More specifically, since Magma uses information from M individuals, each of them providing N i observations, these quantities determine the overall complexity of the algorithm.</p><p>If we recall that N is the number of distinct timestamps (i.e. N ≤</p><formula xml:id="formula_33">M i=1 N i ), the training complexity is O M × N 3 i + N 3 (i.</formula><p>e. the complexity of each EM iteration). As usual with GPs, the cubic costs come from the inversion of the corresponding matrices, and here, the constant is proportional to the number of iterations of the EM algorithm. The dominating term in this expression depends on the values of M , relatively to N . For a large number of individuals with many common timestamps (M N i ≳ N ), the first term dominates. For diverse timestamps among individuals (M N i ≲ N ), the second term becomes the primary burden, as in any GP problem. During the prediction step, the re-computation of µ 0 's hyper-posterior implies the inversion of a Ñ × Ñ (dimension of t p * ) which has a O( Ñ 3 ) complexity while the new hyper-parameters estimation's cost is O(N 3 * ). In practice, the most computationally-expensive steps can be performed in advance to allow for quick onthe-fly prediction when collecting new data. If we observe the training dataset once and pre-compute the hyper-posterior of µ 0 on a fine grid on which to predict later, the immediate computational cost for each new individual is identical to the one of the single-task GP regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experimental results</head><p>We evaluate our Magma algorithm on synthetic data and two real datasets. The classical GP regression on single tasks separately is used as the baseline alternative for predictions. While it is not expected to perform well on the dataset used, the comparison highlights the interest of multi-task approaches. To our knowledge, the only alternative to Magma is the GPFDA algorithm from <ref type="bibr" target="#b29">Shi et al. (2007)</ref>; <ref type="bibr" target="#b31">Shi and Choi (2011)</ref>, described in Sec. 3.4, and the associated R package GPFDA, which is applied during the experiments. Throughout the section, the standard Exponentiated Quadratic kernel (see Eq. ( <ref type="formula" target="#formula_9">1</ref>)) is used both for simulating the data and for modelling the covariance structures in the three algorithms.</p><p>Hence, each kernel is associated with θ = {v, ℓ}, v, ℓ ∈ R + , a set of variance and lengthscale hyper-parameters, respectively. Each simulated dataset has been drawn from the sampling scheme below:  6. ∀i ∈ I, draw a subset t i ⊂ t of N i = 30 timestamps uniformly, and draw</p><formula xml:id="formula_34">y i ∼ N µ 0 (t i ), Ψ t i θ i ,σ 2 i .</formula><p>This procedure provides a synthetic dataset {t i , y i } i , and its associated mean process µ 0 (t). Those quantities are used to train the model, make predictions with each algorithm, and then compute errors in µ 0 estimation and forecasts. We recall that the Magma algorithm enables two different settings depending on the model's assumption over hyper-parameters (HP), and we refer to them as Common HP and Different HP in the following. In order to test these two contexts, differentiated datasets have been generated, by drawing Common HP data or Different HP data for each individual at step 5. We previously presented the idea of the model used in GPFDA, and, although the algorithm has many features (in particular about the type and number of input variables), it is not yet usable when timestamps are different among individuals. Therefore, two frameworks are considered, Common grid and Uncommon grid, to take this specification into account. Thus, the comparison between the different methods can only be performed on data generated under the settings Common HP and Common grid, and the effect of those different settings on Magma is analysed separately. Moreover, the initialisation for the prior mean function, m 0 (•), is set to be constant, equal to 0 for each algorithm. Except in some experiments, where the influence of the number of individuals is analysed, the generic value is M = 20. In the case of prediction on unobserved timestamps for a new individual, the first 20 data points are used as observations, and the remaining 10 are taken as test values. Optimisation of the hyper-parameters is performed by likelihood maximisation, using the L-BFGS-B algorithm <ref type="bibr" target="#b21">(Nocedal, 1980;</ref><ref type="bibr" target="#b18">Morales and Nocedal, 2011</ref>) in all methods. The convergence criterion for all algorithms is reached if the difference of log-likelihood between two iterations is lower than 10 -2 . In general, the EM algorithm in Magma converges in a few iterations, typically fewer than 5 with the Common HP setting, and rarely more than 15 even with the Different HP setting.  <ref type="table" target="#tab_4">Magma 18.7 (31.4) 93.8 (13.5) 1.3 (2) 94</ref> <ref type="bibr">.3 (11.3) GPFDA 31.8 (49.4)</ref> 90.4 (18.1) 2.4 (3.6) ⋆ GP 87.5 (151.9) 74.0 (32.7)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Illustration on a simple example</head><p>To illustrate the multi-task approach of Magma, Fig. <ref type="figure" target="#fig_1">2</ref> displays a comparison between standard GP regression and Magma on a simple example, from a dataset simulated according to the scheme above and using the Uncommon grid /Common HP setting. Given the observed data (in black), values on a thin grid of unobserved timestamps are predicted and compared, in particular, with the true test values (in red). As expected, the GP regression provides a good fit close to the data points and then dives rapidly to the prior 0 with increasing uncertainty. Conversely, although the initialisation for the prior mean is 0 in Magma as well, the hyper-posterior distribution of µ 0 (dashed line) is estimated thanks to all individuals in the training dataset. This process acts as an informed prior helping GP prediction for the new individual, even far from its own observations. More precisely, 3 phases can be distinguished according to the level of information coming from the data: in the first one, close to the observed data (t ∈ [ 1, 7 ]), the two processes behave similarly, except for a slight increase in the variance for Magma, which is logical since the prediction also takes uncertainty over µ 0 into account (see Eq. ( <ref type="formula">3</ref>)); in the second one, on intervals of unobserved timestamps containing data points from the training dataset (t ∈ [ 0, 1 ]∪[ 7, 10 ]), the prediction is guided by the information coming from other individuals through µ 0 . In this context, the mean trajectory remains coherent and the uncertainty increases only slightly. In the third phase, where no observations are available, neither from the new individual nor from the training dataset (t ∈ [ 10, 12 ]), the prediction behaves as expected, with a slow drifting to the prior mean 0, with highly increasing variance. Overall, the multi-task framework provides reliable probabilistic predictions on a wider range of timestamps, potentially outside of the usual scope of GPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Performance comparison on simulated datasets</head><p>We confront the performance of Magma to alternatives in several situations and for different datasets. In the first place, the classical GP regression (GP), GPFDA and Magma are compared through their performance in prediction and estimation of the true mean process µ 0 . In the prediction context, the performances are evaluated according to the following indicators:</p><p>• the mean squared error (MSE) which compares the predicted values to the true test values of the 10 last timestamps:</p><p>1 10</p><formula xml:id="formula_35">30 k=21 y pred * (t k * ) -y true * (t k * )</formula><p>Figure <ref type="figure">3</ref>: MSE with respect to the number M of training individuals (boxplots are displayed from 100 runs in each case). Left: prediction error on 10 testing points.</p><p>Right: estimation error of the true mean process µ 0 . The CIC 95 provides insights on the reliability of the predictive variance and should be as close to the value 95% as possible. Other values would indicate a tendency to underestimate or overestimate the uncertainty. Let us recall that GPFDA uses B-splines to estimate the mean process and does not account for uncertainty, contrarily to a probabilistic framework as Magma. However, a measure of uncertainty based on an empirical variance estimated from training curves is proposed (see <ref type="bibr">Shi and Cheng, 2014, Section 3.2.1)</ref>. In practice, this measure constantly overestimates the true variance, and their 95% empirical interval coverage is generally equal or close to 100%.</p><p>In the estimation context, the performances are evaluated thanks to another MSE, which compares the estimations to the true values of µ 0 at all timestamps:</p><formula xml:id="formula_36">1 M M i=1 1 N i N i k=1 µ pred 0 (t k i ) -µ true 0 (t k i ) 2 .</formula><p>Table <ref type="table" target="#tab_1">1</ref> presents the results obtained over 100 datasets, where the models are trained on M = 20 individuals, each of them observed on N = 30 common timestamps. As expected, both multi-task methods lead to better results than GP. However, Magma outperforms GPFDA, both in the estimation of µ 0 and in predictive performance. In terms of error as well as in uncertainty quantification, Magma provides more accurate results, in particular with a CI 95 coverage close to the 95% expected value. Each method presents a quite high standard deviation for MSE in prediction, which is due to some datasets with particularly difficult values to predict, although most of the cases lead to small errors. This behaviour is reasonably expected since such 10-timestamps-ahead forecasts might sometimes be tricky. It can also be noticed on Fig. <ref type="figure">3</ref> that Magma consistently provides lower errors as well as less pathological behaviour, as it may sometimes occur with the B-splines modelling used in GPFDA.</p><p>To highlight the effect of the number of individuals M on the performance, Fig. <ref type="figure">3</ref> provides the same 100 runs trial as previously, for different values of M . The boxplots exhibit, for each method, the behaviour of the prediction and estimation MSE as information is added in the training dataset. Let us mention the absence of discernible changes as soon as M &gt; 200. As expected, we notice on the right panel that adding information from new individuals improves the estimation of µ 0 , leading to shallow errors for high values of M , in particular for Magma. Meanwhile, the left panel exhibits reasonably unchanged prediction performance with respect to the values of M , excepted for some random fluctuations. This property is expected for GP regression since no external information is used from the training dataset in this context. For both multi-tasks algorithms though, the estimation of µ 0 improves the prediction by one order of magnitude below the typical errors, even with only a few training individuals. Furthermore, since a new individual behaves independently through f * , it is natural for a 10-points-ahead forecast to present intrinsic variations, despite an adequate estimation of the shared mean process.</p><p>To illustrate the advantage of multi-task methods, even for M = 20, we display on Fig. <ref type="figure" target="#fig_2">4</ref> the evolution of MSE according to the number of timestamps N that are assumed to be observed for the new individual on which we make predictions. These predictions remain computed on the last 10 timestamps, although in this experiment, we only observe the first 5, 10, 15, or 20 timestamps, in order to change the volume of information and the distance from training observations to targets. We observe on Fig. <ref type="figure" target="#fig_2">4</ref> that, as expected in a GP framework, the closer observations are to targets, the better the results. However, for multi-tasks approaches and in particular for Magma, the prediction remains consistently adequate even with few observations. Once more, sharing information across individuals significantly helps the prediction, even for small values of M or few observed data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Magma's specific settings</head><p>As we previously discussed, different settings are available for Magma according to the nature of data and the model hypotheses. First, the Common grid setting corresponds to cases where all individuals share the same timestamps, whereas Uncommon grid is used otherwise. Moreover, Magma enables to consider identical hyper-parameters for all individuals or specific ones, as previously discussed in Sec. 2.2. To evaluate the effect of the different settings, performances in prediction and µ 0 's estimation are evaluated in the following cases in Table <ref type="table" target="#tab_2">2:</ref> • Common HP, when data are simulated with a common set of hyper-parameters for all individuals, and Proposition 3.3 is used for inference in Magma,</p><p>• Different HP, when data are simulated with its own set of hyper-parameters for each individual, and Proposition 3.2 is used for inference in Magma,</p><p>• Common HP on different HP data, when data are simulated with its own set of hyper-parameters for each individual, and Proposition 3.3 is used for inference in Magma.</p><p>Note that the first line of the table (Common grid / Common HP ) of Table <ref type="table" target="#tab_2">2</ref> is identical to the corresponding results in Table <ref type="table" target="#tab_1">1</ref>, providing reference values, significantly better than for other methods. The results obtained in Table <ref type="table" target="#tab_2">2</ref> indicate that the Magma performance is not significantly altered by the settings used or the nature of the simulated data. To confirm the robustness of the method, the setting Common HP was applied to data generated by drawing different values of hyper-parameters for each individual (Different HP data). In this case, performances in prediction and estimation of µ 0 are slightly deteriorated, although Magma still provides quite reliable forecasts. This experience also highlights a particularity of the Different HP setting: looking at the estimation of µ 0 performance, we observe a significant decrease in the CI 95 coverage, due to numerical instability in some pathological cases. Numerical issues, in particular during matrix inversions, are classical problems in the GP literature and, because of the potentially large number of different hyper-parameters to train, the probability for at least one of them to lead to a nearly singular matrix increases. In this case, one individual might overwhelm others in the calculus of µ 0 's hyper-posterior (see Proposition 4.1), and thus lead to an underestimated posterior variance. This problem does not occur in the Common HP settings, since sharing the same hyper-parameters prevents the associated covariance matrices from running over each other. Thus, except if one specifically wants to smooth multiple curves presenting really different behaviours, keeping Common HP as a default setting appears as a reasonable choice. Let us notice that the estimation of µ 0 is slightly 92.5 (15.9) 3.2 (4.5) 93.4 (9.8) better for common than for uncommon grid since the estimation problem on the union of different timestamps is generally more difficult. However, this feature only depends on the nature of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Running times comparisons</head><p>The counterpart of the more accurate and general results provided by Magma is a natural increase in running time. Table <ref type="table" target="#tab_3">3</ref> exhibits the raw and relative training times for GPFDA and Magma (prediction times are negligible and comparable in both cases), on data coming from the simulation scheme with varying values of M on a Common grid of N = 30 timestamps. The algorithms were run under the 3.6.1 R version, on a laptop with a dualcore processor cadenced at 2.90GHz and an 8GB RAM. The reported computing times are in seconds, and for small to moderate datasets (N ≃ 10 3 , M ≃ 10 4 ) the procedures ran in few minutes to few hours. The difference between the two algorithms is due to GPFDA modelling µ 0 as a deterministic function through B-splines smoothing, whereas Magma accounts for uncertainty. The ratio of computing times between the two methods tends to decrease as M increases, and stabilises around 2 for higher numbers of training individuals. This behaviour comes from the E step in Magma, which is incompressible and quite insensitive to the value of M . Roughly speaking, one needs to pay twice the computing price of GPFDA for Magma to provide (significantly) more accurate predictions and uncertainty over µ 0 . Table <ref type="table">4</ref> provides running times of Magma according to its different settings, with M = 20. Because the complexity is linear in M in each case, the ratio in running times would remain roughly similar no matter the value of M . Prediction time appears negligible compared to training time, and generally takes less than one second to run. Besides, the Different HP setting increases the running time since in this context M maximisations (instead of one for Common HP ) are required at each EM iteration. In this case, the prediction also takes slightly longer because of the necessity to optimise hyper-parameters for the new individual. Although the nature of the grid of timestamps does not matter in itself, a key limitation lies in the dimension N of the pooled set of timestamps, which tends to get bigger when individuals have different timestamps from one another. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Application of Magma on swimmers' progression curves</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data and problematic</head><p>We consider the problem of performance prediction in competition for french swimmers. The French Swimming Federation provided us with an anonymised dataset, compiling the age and results of its members between 2000 and 2016. For each competitor, the race times are registered for competitions of 100m freestyle (50m swimming-pool). The database contains results from 1731 women and 7876 men, each of them compiling an average of 22.2 data points (min = 15, max = 61) and 12 data points (min = 5, max = 57), respectively. In the following, age of the i-th swimmer is considered as the input variable (timestamp t) and the performance (in seconds) on a 100m freestyle as the output (y i (t)). For reasons of confidentiality and property, the raw dataset cannot be published. The analysis focuses on the youth period, from 10 to 20 years, where the progression is the most noticeable. In order to get relevant time series, we retained only individuals having a sufficient number of data points (N i ≥ 5) on the considered time period. For a young swimmer, observed during its first years of competition, we aim at modelling its progression curve and make predictions on its future performance in the subsequent years. Since we consider a decision-making problem involving irregular time series, the GP probabilistic framework is a natural choice to work on. Thereby, assuming that each swimmer in the database is a realisation y i defined as previously, we expect Magma to provide multi-task predictions for a new young swimmer, that will benefit from information of other swimmers already observed at older ages. To study such modelling, and validate its efficiency in practice, we split the individuals into training and testing datasets with respective sizes:</p><p>• M F train = 1039, for the female training set, • M F test = 692, for the female testing set,</p><p>• M M train = 4726, for the male training set, • M M test = 3150, for the male testing set.</p><p>Inference on the hyper-parameters is performed thanks to the training dataset in both cases. Considering the different timestamps and the relative monotony of the progression curves, the settings Uncommon grid /Common HP has been used for Magma. The overall training lasted around 2 hours with the same hardware configuration as for simulations. To compute MSE and the CI 95 coverage, the data points of each individual in the testing set has been split into observed and testing timestamps. Since each individual has a different number of data points, the first 80% of timestamps are taken as observed, while the remaining 20% are considered as testing timestamps. Magma's predictions are compared with the true values of y i at testing timestamps. As previously, both GP and Magma have been initialised with a constant 0 mean function. Initial values for hyper-parameters are also similar for all i, θ ini 0 = θ ini i = (exp(1), exp(1)) and σ ini i = 0.4. Those values are the default in Magma and remain adequate in the context of these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and interpretation</head><p>The overall performance and comparison are summarised in Table <ref type="table" target="#tab_4">5</ref>. We observe that Magma still provides excellent results in this context, and naturally outperforms predictions provided by a standard GP regression. As the progression curves present relatively monotonic variations and thus avoid pathological behaviours that could occur with synthetic data, the MSE in prediction remains very low. The CI 95 coverage sticks close to the 95% expected value for Magma, indicating an adequate quantification of uncertainty. To illustrate these results, an example is displayed on Fig. <ref type="figure" target="#fig_3">5</ref> for both men and women. For a randomly chosen testing individual, we plot its predicted progression curve (in blue), where its first 15 data points are used as observations (in black), while the remaining true data points (in red) are displayed for comparison purpose. As previously observed in the simulation study, the simple GP quickly drifts to the prior 0 mean, as soon as data lack. However, for both men and women, the Magma predictions remain close to the true data, which also lie within the 95% credible interval. Even for long term forecast, where the mean prediction curve tends to overlap the mean process (dashed line), the true data remain in our range of uncertainty, as the credible interval widens far from observations. For clarity, we displayed only a few individuals from the training dataset (colourful points) in the background. The mean process (dashed line) seems to represent the main trend of progression among swimmers correctly, even though we cannot numerically compare µ 0 to any real-life analogous quantity. From a more sportrelated perspective, we can note that both genders present similar patterns of progression. However, while performances are roughly similar in mean trend before the age of 14, they start to differentiate afterwards and then converge to average times with approximatively a 5 seconds gap. Interestingly, the difference between men and women in terms of world records in swimming competitions for the 100m freestyle is currently 4.8 seconds (46.91 versus 51.71). These results, obtained under reasonable hypotheses on several hundreds of swimmers, seem to indicate that Magma would give quite reliable predictions for a new young swimmer. Furthermore, the uncertainty provided through the predictive posterior distribution offers an adequate degree of caution in a decision-making process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>We have introduced a unified multi-task framework integrating a mean Gaussian process prior in the context of GP regression. While we believe that this process is an interesting object in itself, it also allows individuals to borrow information from each other and provides more accurate predictions, even far from data points. Furthermore, our method accounts for uncertainty in the mean process and remains applicable no matter eventual irregular timestamps in the grid of observations. The proposed algorithm, Magma, also presents a reduced computational complexity compared to previous multi-task GPs frameworks. Both on simulated and real-life datasets, we exhibited the efficiency of such an approach and studied some of its properties and possible settings. Magma outperforms the alternatives in estimation of the mean process as well as in prediction, and leads to a reliable quantification of uncertainty. We also displayed evidence of its predictive efficiency for real-life problems and provided some insights on practical interpretations about the mean process.</p><p>Despite the extensive literature on these aspects of GPs, our model does not yet include sparse approximations. While these aspects remain beyond the scope of the present paper, we might aim at adapting existing approaches <ref type="bibr" target="#b33">(Snelson and Ghahramani, 2006;</ref><ref type="bibr" target="#b22">Quiñonero-Candela et al., 2007;</ref><ref type="bibr" target="#b37">Titsias, 2009)</ref> in our model to widen its applicability. Another possible avenue is an adaptation to the classification context, which is presented in Rasmussen and <ref type="bibr">Williams (2006, Chapter 3)</ref>. Besides, this work leaves the door open to improvement as we tackled here the problem of unidimensional regression: enabling either multidimensional or mixed type of inputs as in <ref type="bibr" target="#b31">Shi and Choi (2011)</ref> would be of interest. To conclude, the hypothesis of a unique underlying mean process might be considered too restrictive for some datasets, and enabling cluster-specific mean processes would be a relevant extension.</p><formula xml:id="formula_37">• K = K -1 + M i=1 Ψ-1 i -1 , • m 0 (τ ) = K K -1 m τ 0 + M i=1 Ψ-1 i ỹτ i .</formula><p>8.2 Proof of Proposition 3.2 and Proposition 3.3</p><p>Since the central part of the proofs is similar for both propositions, we detail the calculus by denoting Θ = {θ 0 , {θ i } i , σ 2 i i } for generality, and dissociating the two cases only when necessary. Before considering the maximisation, we notice that the joint density can be developed as: ) N (µ 0 (t); m 0 (t), K t θ 0 ).</p><p>The expectation is taken over p(µ 0 (t) | {y i } i ) though we write it E for simplicity. We have:</p><formula xml:id="formula_38">f (Θ) = E [ log p({y i } i , µ 0 (t) | Θ) ]</formula><p>= -1 2 E (µ 0 (t) -m 0 (t)) ⊺ K t θ 0</p><p>-1 (µ 0 (t) -m 0 (t)) -log K t θ 0</p><p>-1</p><formula xml:id="formula_39">+ M i=1 (y i -µ 0 (t i )) ⊺ Ψ t i θ i ,σ 2 i -1 (y i -µ 0 (t i )) -log Ψ t i θ i ,σ 2 i -1 + C 1 .</formula><p>Lemma 8.1. Let X ∈ R N be a random Gaussian vector X ∼ N (m, K), b ∈ R N , and S, a N × N covariance matrix. Then: As we note that X and b play symmetrical roles in the calculus of the conditional expectation, we can apply the lemma regardless of the position of µ 0 in the M +1 equalities involved. Applying Lemma 8.1 to our previous expression of f (Θ), we obtain:</p><formula xml:id="formula_40">E = E X (X -b) ⊺ S -</formula><formula xml:id="formula_41">f (Θ) = - 1 2 ( m 0 (t) -m 0 (t)) ⊺ K t θ 0</formula><p>-1 ( m 0 (t) -m 0 (t))</p><formula xml:id="formula_42">+ M i=1 (y i -m 0 (t i )) ⊺ Ψ t i θ i ,σ 2 i -1 (y i -m 0 (t i )) + Tr K t K t θ 0 -1 + M i=1 Tr K t i Ψ t i θ i ,σ 2 i -1 -log K t θ 0 -1 - M i=1 log Ψ t i θ i ,σ 2 i -1 + C 1 .</formula><p>We recall that, at the M step, m 0 (t) is a known constant, computed at the previous E step. Thus, we identify here the characteristic expression of several Gaussian loglikelihoods and associated correction trace terms. Moreover, each set of hyper-parameters is merely involved in independent terms of the whole function to maximise. Hence, the global maximisation problem can be separated into several maximisations of sub-functions according to the hyper-parameters getting optimised. Regardless to additional assumptions, the hyper-parameters θ 0 , controlling the covariance matrix of the mean process, appears in a function which is exactly a Gaussian log-likelihood, log N m 0 (t), m 0 (t), K t θ 0 , added to a corresponding trace term, -1 2 Tr K t K t θ 0</p><p>-1 . This function can be maximised independently from the other parameters, giving the first part of the results in Proposition 3.2 and Proposition 3.3.</p><p>Although the idea is analogous for the remaining hyper-parameters, we have to discriminate here regarding the assumption on the model. If each individual is supposed to have its own set {θ i , σ i }, which thus can be optimised independently from the observations and hyper-parameters of other individuals, we identify a sum of M Gaussian log-likelihoods, log N y i , m 0 (t i ), Ψ t i θ i ,σ 2 i , and the corresponding trace terms, -1 2 Tr( K t Ψ t i θ i ,σ 2 i -1 ). This property results in M independent maximisation problems on the corresponding functions, proving Proposition 3.2. Conversely, if we assume that all individuals in the model shares their hyper-parameters (i.e. θ, σ 2 = θ i , σ 2 i , ∀i ∈ I), we can no longer divide the problem into M sub-maximisations, and the whole sum on all individual should be optimised thanks to observations from all individuals. This case corresponds to the second part of Proposition 3.3.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1.</head><label/><figDesc>Draw a random working grid t ⊂ [ 0, 10 ] of N = 200 timestamps, and a number M of individuals. 2. Define a prior mean function : m 0 (t) = at + b, ∀t ∈ t, where a ∈ [ -2, 2 ] and b ∈ [ 0, 10 ] are drawn uniformly.3. Draw hyper-parameters uniformly for µ 0 's kernel : θ 0 = {v 0 , ℓ 0 }, where v 0 ∈ [ 1, exp(5) ] andℓ 0 ∈ [ 1, exp(2) ]. 4. Draw µ 0 (t) ∼ N m 0 (t), K t θ 0 . 5. ∀i ∈ I, draw v i ∈ [ 1, exp(5) ], ℓ i ∈ [ 1, exp(2)], and σ 2 i ∈ [ 0, 1 ] uniformly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Prediction curves (blue) of a new individual with associated 95% credible intervals (grey) for GP regression (left) and Magma (right). The dashed line represents the mean function m 0 , from the hyper-posterior p(µ 0 | {y i } i ). Observed data points are in black, and testing data points are in red. The colourful backward points are the observations from the training dataset, each colour corresponding to a different individual.</figDesc><graphic coords="16,85.04,85.04,425.19,141.58" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :1</head><label>4</label><figDesc>Figure 4: MSE prediction error on the 10 last testing points with respect to the increasing number N of observed timestamps, among the first 20 points (boxplots are displayed from 100 runs in each case).</figDesc><graphic coords="18,191.34,481.15,212.58,151.17" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Prediction curves (blue) for a testing individual with associated 95% credible intervals (grey) for GP regression (left) and Magma (right), for both women (top) and men (bottom). The dashed lines represent the mean functions m 0 , from the hyper-posterior p(µ 0 | {y i } i ). Observed data points are in black, and testing data points are in red. The colourful backward points are observations from the training datasets, each colour corresponding to a different individual.</figDesc><graphic coords="24,85.04,85.04,425.19,263.96" type="bitmap"/></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head/><label/><figDesc>p({yi } i , µ 0 (t) | Θ) = p({y i } i | µ 0 (t), Θ) p(µ 0 (t) | Θ) = M i=1 p(y i | µ 0 (t), θ i , σ 2 i ) p(µ 0 (t) | θ 0 ) = M i=1 N (y i ; µ 0 (t), Ψ θ i ,σ 2 i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head/><label/><figDesc>1 (X -b) = (m -b) ⊺ S -1 (m -b) + Tr(KS -1 ). Lemma 8.1. E = E X Tr(S -1 (X -b)(X -b) ⊺ ) = Tr(S -1 V X (X -b)) + Tr(S -1 (m -b)(m -b) ⊺ ) = (m -b) ⊺ S -1 (m -b) + Tr KS -1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Average MSE (standard deviation) and average CIC 95 (standard deviation) on 100 runs for GP, GPFDA and Magma. (⋆ : 99.6 (2.8), the measure of incertitude from the GPFDA package is not a genuine credible interval)</figDesc><table><row><cell cols="2">Prediction</cell><cell cols="2">Estimation µ 0</cell></row><row><cell>MSE</cell><cell>CIC 95</cell><cell>MSE</cell><cell>CIC 95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Average MSE (standard deviation) and average CIC 95 (standard deviation) on 100 runs for the different settings of Magma.</figDesc><table><row><cell cols="2">Prediction</cell><cell cols="2">Estimation of µ 0</cell></row><row><cell>MSE</cell><cell>CIC 95</cell><cell>MSE</cell><cell>CIC 95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Average (standard deviation) training time (in seconds) for Magma and GPFDA on 100 runs for different numbers M of individuals in the training dataset. The relative running time between Magma and GPFDA is provided on the line Ratio.</figDesc><table><row><cell>M =</cell><cell/><cell>5</cell><cell>10</cell><cell>50</cell><cell>100</cell></row><row><cell cols="6">Magma 5.2 (2.7) 7.6 (3.2) 24.2 (11.1) 42.8 (10)</cell></row><row><cell>GPFDA</cell><cell cols="5">1 (0.3) 2.1 (0.6) 10.7 (2.4) 23.1 (5.3)</cell></row><row><cell>Ratio</cell><cell/><cell>5.2</cell><cell>3.6</cell><cell>2.3</cell><cell>1.9</cell></row><row><cell cols="6">Table 4 Average (standard deviation) training and prediction time (in seconds) on 100</cell></row><row><cell cols="3">runs for different settings of Magma.</cell><cell/><cell/></row><row><cell/><cell/><cell/><cell/><cell>Train</cell><cell>Predict</cell></row><row><cell cols="2">Common HP</cell><cell cols="4">Common grid Uncommon grid 16.5 (11.4) 0.2 (0.1) 12.6 (3.5) 0.1 (0)</cell></row><row><cell cols="2">Different HP</cell><cell cols="2">Common grid Uncommon grid</cell><cell cols="2">42.6 (20.5) 0.6 (0.1) 40.2 (17) 0.6 (0.1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc>Average MSE (standard deviation) and average CIC 95 (standard deviation) for prediction on french swimmer testing datasets.</figDesc><table><row><cell/><cell>MSE</cell><cell>CIC 95</cell></row><row><cell>Women</cell><cell cols="2">Magma 3.8 (10.3) 95.3 (15.9) GP 25.3 (97.6) 72.7 (37.1)</cell></row><row><cell>Men</cell><cell cols="2">Magma 3.7 (5.3) 93.9 (15.3) GP 22.1 (94.3) 78.2 (30.4)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>,• the CI 95 coverage (CIC 95 ), i.e. the percentage of unobserved data points effec-</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors warmly thank <rs type="person">Andy Marc</rs>, <rs type="person">Olivier Dupas</rs>, <rs type="person">Richard Martinez</rs> and the <rs type="institution">French Swimming Federation</rs> for providing data and helping in the analysis of the results. <rs type="person">Benjamin Guedj</rs> acknowledges partial support by the <rs type="funder">U.S. Army Research Laboratory</rs> and the <rs type="funder">U.S. Army Research Office</rs>, and by the <rs type="funder">U.K. Ministry of Defence</rs> and the <rs type="funder">U.K. Engineering and Physical Sciences Research Council (EPSRC)</rs> under grant number <rs type="grantNumber">EP/R013616/1</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4dgEsak">
					<idno type="grant-number">EP/R013616/1</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Availability of data. The synthetic data and table of results are available at https: //github.com/ArthurLeroy/MAGMA/tree/master/Simulations. Code availability. The R code associated with the present work is available at https: //github.com/ArthurLeroy/MAGMA. The current version of the R package implementing an extended version of Magma is available at https://github.com/ArthurLeroy/ MagmaClustR.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Proofs</head><p>Note that the proof of Proposition 3.1 is a particular case of the proof below, where τ = t exactly (where τ is the set of timestamps the hyper-posterior is to be computed on). Moreover, in order to keep an analytical expression for µ 0 's hyper-posterior distribution, we discard the superfluous information contained in {y i } i at timestamps on which the hyper-posterior is not to be computed. Hence, the proof below states that the remaining data points are observed on subsets {τ i } i of τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Proof of Proposition 4.1</head><p>Let τ be a finite vector of timestamps, and {τ i } i such as ∀i ∈ I, τ i ⊂ τ . We define convenient notation:</p><p>• m τ 0 = m 0 (τ ),</p><p>• µ τ i 0 = µ 0 (τ i ), ∀i ∈ I,</p><p>•</p><p>Moreover, for a covariance matrix C, and u, v ∈ τ , we note [ C ] -1 uv the element of the inverse matrix at row associated with timestamp u, and column associated with timestamp v. We also ignore the conditionings over Θ, τ i and τ to maintain simple expressions. By construction of the models, we have:</p><p>The term</p><p>) associated with the hyper-posterior remains quadratic and we may find the corresponding Gaussian parameters by identification:</p><p>where we entirely decomposed the vector-matrix products. We factorise the expression according to the common timestamps between τ i and τ . Since for all i, τ i ⊂ τ , let us introduce a dummy indicator function</p><p>subsequently, we can gather the sums such as:</p><p>where the y i and Ψ i are completed by zeros:</p><p>By identification of the quadratic form, we reach:</p><p>p(µ τ 0 | {y τ i i } i ) = N µ τ 0 ; m 0 (τ ), K , with,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declarations</head><p>Funding. The authors received no financial support for the research, authorship, and/or publication of this article.</p><p>Conflict of interest. The authors declare no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Benjamin Guedj acknowledges partial support from the French National Agency for Research</title>
		<idno>ANR-18-CE40-0016-01 and ANR-18-CE23-0015-02</idno>
		<imprint/>
	</monogr>
	<note>Bibliography</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian Inference of Individualized Treatment Effects using Multi-task Gaussian Processes</title>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">M</forename><surname>Alaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3424" to="3432"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Computationally Efficient Convolved Multiple Output Gaussian Processes</title>
		<author>
			<persName><forename type="first">Mauricio</forename><forename type="middle">A</forename><surname>Álvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<idno type="ISSN">1533-7928</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">41</biblScope>
			<biblScope unit="page" from="1459" to="1500"/>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kernels for Vector-Valued Functions: A Review</title>
		<author>
			<persName><forename type="first">Mauricio</forename><forename type="middle">A</forename><surname>Álvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000036</idno>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<idno type="ISSN">1935-8237</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="195" to="266"/>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Choosing starting values for the EM algorithm for getting the highest likelihood in multivariate Gaussian mixture models</title>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Biernacki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Celeux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gérard</forename><surname>Govaert</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0167-9473(02)00163-9</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<idno type="ISSN">0167- 9473</idno>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="561" to="575"/>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pattern Recognition and Machine Learning</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Science and Statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-task Gaussian Process Prediction</title>
		<author>
			<persName><forename type="first">Kian</forename><forename type="middle">M</forename><surname>Edwin V Bonilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="153" to="160"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multitask Learning</title>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1007379606734</idno>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<idno type="ISSN">1573-0565</idno>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75"/>
			<date type="published" when="1997-07">July 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An Introduction to Empirical Bayes Data Analysis</title>
		<author>
			<persName><forename type="first">George</forename><surname>Casella</surname></persName>
		</author>
		<idno type="DOI">10.2307/2682801</idno>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<idno type="ISSN">0003-1305</idno>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="83" to="87"/>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lifelong Learning with Gaussian Processes</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clingerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Eaton</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-71246-8</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<editor>
			<persName><forename type="first">Michelangelo</forename><surname>Ceci</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jaakko</forename><surname>Hollmén</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ljupčo</forename><surname>Todorovski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Celine</forename><surname>Vens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sašo</forename><surname>Džeroski</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10535</biblScope>
			<biblScope unit="page" from="690" to="704"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bayesian Functional Data Analysis Using WinBUGS</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ciprian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Jeffrey</forename><surname>Crainiceanu</surname></persName>
		</author>
		<author>
			<persName><surname>Goldsmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical software</title>
		<idno type="ISSN">1548-7660</idno>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Maximum Likelihood from Incomplete Data via the EM Algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<idno type="ISSN">0035-9246</idno>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38"/>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Automatic Model Construction with Gaussian Processes</title>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-11">November 2014</date>
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Nonparametric Functional Data Analysis: Theory and Practice</title>
		<author>
			<persName><forename type="first">Frédéric</forename><surname>Ferraty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Vieu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-measuring Similarity for Multi-task Gaussian Process</title>
		<author>
			<persName><forename type="first">Kohei</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Takenouchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<idno type="DOI">10.1527/tjsai.27.103</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Japanese Society for Artificial Intelligence</title>
		<idno type="ISSN">1346-8030</idno>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="103" to="110"/>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The EM Algorithm and Extensions</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thriyambakam</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-11">November 2007</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Remark on algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound constrained optimization</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><surname>Nocedal</surname></persName>
		</author>
		<idno type="DOI">10.1145/2049662.2049669</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software</title>
		<idno type="ISSN">0098-3500</idno>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7"/>
			<date type="published" when="2011-12">December 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Continual Multi-task Gaussian Processes</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Moreno-Muñoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Artés-Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauricio</forename><forename type="middle">A</forename><surname>Álvarez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00002</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Collaborative multi-output Gaussian processes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Trung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwin</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Bonilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence, UAI'14</title>
		<meeting>the Thirtieth Conference on Uncertainty in Artificial Intelligence, UAI'14<address><addrLine>Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2014-07">July 2014</date>
			<biblScope unit="page" from="643" to="652"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Updating quasi-Newton matrices with limited storage</title>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Nocedal</surname></persName>
		</author>
		<idno type="DOI">10.1090/S0025-5718-1980-0572855-7</idno>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<idno type="ISSN">0025-5718, 1088-6842</idno>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">151</biblScope>
			<biblScope unit="page" from="773" to="782"/>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Approximation Methods for Gaussian Process Regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Quiñonero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</title>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Rakitsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Lippert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Stegle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1466" to="1474"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Functional Data Analysis</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">O</forename><surname>Ramsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gaussian Processes for Machine Learning</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Edward Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adaptive Computation and Machine Learning</title>
		<meeting><address><addrLine>Cambridge, Mass</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Estimating the Mean and Covariance Structure Nonparametrically When the Data are Curves</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">A</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Silverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<idno type="ISSN">0035-9246</idno>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="233" to="243"/>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning Gaussian Process Kernels via Hierarchical Bayes</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gaussian Process Functional Regression Modeling for Batch Data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Titterington</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1541-0420.2007.00758.x</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<idno type="ISSN">1541-0420</idno>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="714" to="723"/>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gaussian Process Function Data Analysis R Package 'GPFDA</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Manual of the GPFDA Package</title>
		<imprint>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Gaussian Process Regression Analysis for Functional Data</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taeryon</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical Gaussian process mixtures for regression</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murray-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Titterington</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11222-005-4787-7</idno>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<idno type="ISSN">0960-3174, 1573-1375</idno>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="41"/>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sparse Gaussian Processes using Pseudo-inputs</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Snelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-Task Bayesian Optimization</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2004" to="2012"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Bayesian Model for Sparse Functional Data</title>
		<author>
			<persName><forename type="first">Wesley</forename><forename type="middle">K</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Rosen</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1541-0420.2007.00829.x</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<idno type="ISSN">1541-0420</idno>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="63"/>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Variational Learning of Inducing Variables in Sparse Gaussian Processes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Michalis</surname></persName>
		</author>
		<author>
			<persName><surname>Titsias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deterministic annealing EM algorithm</title>
		<author>
			<persName><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryohei</forename><surname>Nakano</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0893-6080(97)00133-0</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<idno type="ISSN">0893-6080</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="271" to="282"/>
			<date type="published" when="1998-03">March 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-task Gaussian Process Learning of Robot Inverse Dynamics</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Klanke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sethu</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kian</forename><forename type="middle">M</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="265" to="272"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Smoothing and Mean-Covariance Estimation of Functional Data with a Bayesian Hierarchical Model</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taeryon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="DOI">10.1214/15-BA967</idno>
	</analytic>
	<monogr>
		<title level="j">Bayesian Analysis</title>
		<idno type="ISSN">1936-0975</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="649" to="670"/>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient Bayesian hierarchical functional data analysis with basis function approximations using Gaussian-Wishart processes</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Soo Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taeryon</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.1111/biom.12705</idno>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<idno type="ISSN">0006-341X</idno>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1082" to="1091"/>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning Gaussian Processes from Multiple Tasks</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Schwaighofer</surname></persName>
		</author>
		<idno type="DOI">10.1145/1102351.1102479</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd International Conference on Machine Learning, ICML '05</title>
		<meeting>the 22Nd International Conference on Machine Learning, ICML '05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1012" to="1019"/>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-task Sparse Gaussian Processes with Improved Multitask Sparsity Regularization</title>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-662-45646-06</idno>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="54" to="62"/>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>