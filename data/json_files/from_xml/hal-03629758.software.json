{"application": "software-mentions", "version": "0.8.0", "date": "2024-03-21T10:35+0000", "md5": "B25098B0FE9EBAC52F66AB900BFD44E0", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 0, "offsetEnd": 4}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "Wav2Vec2.0 contextual embeddings: Table 2. shows the results with the last but two-layer of contextual transformer block C of Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999674558639526}, "created": {"value": false, "score": 6.794929504394531e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 4, "offsetEnd": 8}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "The Wav2Vec2.0 showed remarkable improvement in ASR (Baevski et al. 2020), emotion detection (Pepino et al. 2021).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996102452278137}, "created": {"value": false, "score": 1.2874603271484375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 6, "offsetEnd": 10}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "Using Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 14, "offsetEnd": 18}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "We found that Wav2Vec2.0-based", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999841451644897}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 18, "offsetEnd": 22}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "Each layer of the Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.999925971031189}, "created": {"value": false, "score": 1.8596649169921875e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "offsetStart": 31, "offsetEnd": 38}, "context": "We extract embeddings from the PyTorch version of Wav2Vec2.0 ", "mentionContextAttributes": {"used": {"value": true, "score": 0.999750554561615}, "created": {"value": false, "score": 0.00032806396484375}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999610185623169}, "created": {"value": false, "score": 0.00032806396484375}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 42, "offsetEnd": 46}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "Moreover, the prior application of LDA on Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998830556869507}, "created": {"value": false, "score": 4.315376281738281e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 43, "offsetEnd": 47}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "Moreover, concatenating the ECAPA-TDNN and Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9947885274887085}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q1026367", "wikipediaExternalRef": 33490859, "lang": "en", "confidence": 0.7138, "software-name": {"rawForm": "Scikit-learn", "normalizedForm": "Scikit-learn", "wikidataId": "Q1026367", "wikipediaExternalRef": 33490859, "lang": "en", "confidence": 0.7138, "offsetStart": 46, "offsetEnd": 58}, "context": " set splits are not provided. So, we decided to create a publicly available protoc", "mentionContextAttributes": {"used": {"value": false, "score": 9.274482727050781e-05}, "created": {"value": true, "score": 0.9974719882011414}, "shared": {"value": false, "score": 0.25550174713134766}}, "documentContextAttributes": {"used": {"value": false, "score": 9.274482727050781e-05}, "created": {"value": true, "score": 0.9974719882011414}, "shared": {"value": false, "score": 0.25550174713134766}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 48, "offsetEnd": 52}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "Fusion: In addition, we fuse the ECAPA-TDNN and Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9968855977058411}, "created": {"value": false, "score": 0.0008834004402160645}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 50, "offsetEnd": 54}, "version": {"rawForm": "2.0", "normalizedForm": "2.0", "offsetStart": 57, "offsetEnd": 60}, "context": "We extract embeddings from the PyTorch version of Wav2Vec2.0 ", "mentionContextAttributes": {"used": {"value": true, "score": 0.999750554561615}, "created": {"value": false, "score": 0.00032806396484375}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 52, "offsetEnd": 56}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "By fine-tuning towards ASR, it is possible that the Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": false, "score": 0.108181893825531}, "created": {"value": false, "score": 6.139278411865234e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 54, "offsetEnd": 58}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "While computing the final score p from ECAPA-TDNN and Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999604225158691}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "offsetStart": 54, "offsetEnd": 61}, "context": "For splitting of the dataset, please see Table1. The experimental results mentioned are the", "mentionContextAttributes": {"used": {"value": true, "score": 0.9986067414283752}, "created": {"value": false, "score": 0.00015532970428466797}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999610185623169}, "created": {"value": false, "score": 0.00032806396484375}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 57, "offsetEnd": 61}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "We investigated ECAPA-TDNN-based speaker recognition and Wav2Vec2.0-based", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999666213989258}, "created": {"value": false, "score": 5.841255187988281e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "offsetStart": 72, "offsetEnd": 79}, "context": "e average of 10-fold cross validation experiments, and are compared to the baseline resu", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999610185623169}, "created": {"value": false, "score": 8.106231689453125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999610185623169}, "created": {"value": false, "score": 0.00032806396484375}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 74, "offsetEnd": 78}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "-We also provide an analysis of the impact of using different layers from Wav2Vec2.0 and their concatenation in SD and also investigate the impact of combining information from ECAPA-TDNN and Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.8269919753074646}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 74, "offsetEnd": 85}, "context": "speech recognition as two separate pre-text tasks trained on VoxCeleb and LibriSpeech, respectively. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.999407172203064}, "created": {"value": false, "score": 0.0001538991928100586}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999407172203064}, "created": {"value": false, "score": 0.008199036121368408}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 77, "offsetEnd": 81}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "From the results, we can observe that for SD, the contextual embeddings from Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999020099639893}, "created": {"value": false, "score": 1.5139579772949219e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UCLASS", "normalizedForm": "UCLASS", "offsetStart": 78, "offsetEnd": 84}, "context": "The models were trained using spectrogram input features on a small set of 24 UCLASS speakers.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999938011169434}, "created": {"value": false, "score": 0.00012874603271484375}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999938011169434}, "created": {"value": false, "score": 0.04697650671005249}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Howell et al. 2009)", "normalizedForm": "Howell et al. 2009", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 79, "offsetEnd": 83}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "In addition, the results show that the contextual layers from L6 to L12 of the Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999815225601196}, "created": {"value": false, "score": 1.0848045349121094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UCLASS", "normalizedForm": "UCLASS", "offsetStart": 82, "offsetEnd": 184}, "context": "Over the years, several stuttering datasets including: SEP-28k (Lea et al. 2021), UCLASS (Sheikh et al. 2021a), LibriStutter (Kourkounakis et al. 2021), and FluencyBank (Lea et al. 2021) have been developed for investigating different SD models. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002664923667907715}, "created": {"value": false, "score": 0.04697650671005249}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999938011169434}, "created": {"value": false, "score": 0.04697650671005249}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Howell et al. 2009)", "normalizedForm": "Howell et al. 2009", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 87, "offsetEnd": 91}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "We further improved SD performance by combining embedding from different layers of the Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997552037239075}, "created": {"value": false, "score": 0.0031015872955322266}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UCLASS", "normalizedForm": "UCLASS", "offsetStart": 90, "offsetEnd": 96}, "context": "The method is trained on mel-spectral and phoneme-based input features by mixing SEP-28k, UCLASS, and FluencyBank datasets.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8188705444335938}, "created": {"value": false, "score": 0.0003515481948852539}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999938011169434}, "created": {"value": false, "score": 0.04697650671005249}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Howell et al. 2009)", "normalizedForm": "Howell et al. 2009", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 126, "offsetEnd": 130}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "Wav2Vec2.0 contextual embeddings: Table 2. shows the results with the last but two-layer of contextual transformer block C of Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999674558639526}, "created": {"value": false, "score": 6.794929504394531e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UCLASS", "normalizedForm": "UCLASS", "offsetStart": 144, "offsetEnd": 150}, "context": "The classifier is trained with 20 Mel-frequency cepstral coefficient (MFCCs) (Huang et al. 2001) input features on a large set of more than 100 UCLASS (Howell et al. 2009) speakers.", "mentionContextAttributes": {"used": {"value": true, "score": 0.626361608505249}, "created": {"value": false, "score": 1.9788742065429688e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999938011169434}, "created": {"value": false, "score": 0.04697650671005249}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Howell et al. 2009)", "normalizedForm": "Howell et al. 2009", "refKey": 0, "offsetStart": 2261, "offsetEnd": 2281}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 159, "offsetEnd": 163}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "-We explore the use of speaker embeddings extracted from emphasized channel attention, propagation and aggregation (ECAPA)-TDNN (Desplanques et al. 2020), and Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": false, "score": 0.005730748176574707}, "created": {"value": false, "score": 0.09679895639419556}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 182, "offsetEnd": 186}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "In this work, use the Minkowski metric distance from eq. ( 3) with p = 2 (Euclidean) to fit the K-NN on the SEP-28k dataset using embeddings computed from pre-trained ECAPA-TDNN and Wav2Vec2.0.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9976208806037903}, "created": {"value": false, "score": 4.0531158447265625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 188, "offsetEnd": 199}, "context": "The authors have released several pre-trained feature embeddings with dimensions of 768 (base) and 1024 (large) and we are using the base one (768-dimensional) pre-trained on 960 hours of LibriSpeech dataset and then fine-tuned for ASR using CTC loss function by adding a linear layer on top of the contextual block.", "mentionContextAttributes": {"used": {"value": false, "score": 0.042430877685546875}, "created": {"value": false, "score": 0.008199036121368408}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999407172203064}, "created": {"value": false, "score": 0.008199036121368408}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 192, "offsetEnd": 196}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "-We also provide an analysis of the impact of using different layers from Wav2Vec2.0 and their concatenation in SD and also investigate the impact of combining information from ECAPA-TDNN and Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.8269919753074646}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 205, "offsetEnd": 209}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "-We provide a novel way for SD, which exploits the information from the fully connected (FC) layer of ECAPA-TDNN (Desplanques et al. 2020) and draws on speech information from several hidden layers of the Wav2vec2.0", "mentionContextAttributes": {"used": {"value": false, "score": 0.00011789798736572266}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561929702758789}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "offsetStart": 317, "offsetEnd": 324}, "context": "The experimental results mentioned are the average of 10-fold cross validation experiments, and are compared to the baseline results from(Sheikh et al. 2021b(Sheikh et al.  , 2022b;; Kourkounakis et al. 2020)   which is trained on MFCC features.4.3 ImplementationFor implementing the proposed pipeline for NN, we use PyTorch library(Paszke  et al. 2019), and for LDA, KNN, and NBC, we have used the Scikit-learn(Pedregosa et al. 2011)  toolkit.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999191761016846}, "created": {"value": false, "score": 5.245208740234375e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999610185623169}, "created": {"value": false, "score": 0.00032806396484375}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "offsetStart": 317, "offsetEnd": 324}, "context": "The experimental results mentioned are the average of 10-fold cross validation experiments, and are compared to the baseline results from(Sheikh et al. 2021b(Sheikh et al.  , 2022b;; Kourkounakis et al. 2020)   which is trained on MFCC features.4.3 ImplementationFor implementing the proposed pipeline for NN, we use PyTorch library(Paszke  et al. 2019), and for LDA, KNN, and NBC, we have used the Scikit-learn(Pedregosa et al. 2011)", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999191761016846}, "created": {"value": false, "score": 5.245208740234375e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999610185623169}, "created": {"value": false, "score": 0.00032806396484375}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "offsetStart": 317, "offsetEnd": 324}, "context": "The experimental results mentioned are the average of 10-fold cross validation experiments, and are compared to the baseline results from(Sheikh et al. 2021b(Sheikh et al.  , 2022b;; Kourkounakis et al. 2020)   which is trained on MFCC features.4.3 ImplementationFor implementing the proposed pipeline for NN, we use PyTorch library(Paszke  et al. 2019), and for LDA, KNN, and NBC, we have used the Scikit-learn", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999191761016846}, "created": {"value": false, "score": 5.245208740234375e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999610185623169}, "created": {"value": false, "score": 0.00032806396484375}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "offsetStart": 317, "offsetEnd": 324}, "context": "The experimental results mentioned are the average of 10-fold cross validation experiments, and are compared to the baseline results from(Sheikh et al. 2021b(Sheikh et al.  , 2022b;; Kourkounakis et al. 2020)   which is trained on MFCC features.4.3 ImplementationFor implementing the proposed pipeline for NN, we use PyTorch library(Paszke  et al. 2019)", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998610019683838}, "created": {"value": false, "score": 5.125999450683594e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999610185623169}, "created": {"value": false, "score": 0.00032806396484375}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "wikidataId": "Q47509047", "wikipediaExternalRef": 54022970, "lang": "en", "confidence": 0.7826, "offsetStart": 317, "offsetEnd": 324}, "context": "The experimental results mentioned are the average of 10-fold cross validation experiments, and are compared to the baseline results from(Sheikh et al. 2021b(Sheikh et al.  , 2022b;; Kourkounakis et al. 2020)   which is trained on MFCC features.4.3 ImplementationFor implementing the proposed pipeline for NN, we use PyTorch library", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998239874839783}, "created": {"value": false, "score": 9.775161743164062e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999610185623169}, "created": {"value": false, "score": 0.00032806396484375}, "shared": {"value": false, "score": 4.76837158203125e-07}}}], "references": [{"refKey": 0, "tei": "<biblStruct xml:id=\"b0\">\n\t<monogr>\n\t\t<title/>\n\t\t<idno>314327DE732C63AEDC18524018BB8838</idno>\n\t\t<imprint>\n\t\t\t<date></date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 10698, "id": "7fe1f93ad30ce0672228e9b63e7602d8aadf2c29", "metadata": {"id": "7fe1f93ad30ce0672228e9b63e7602d8aadf2c29"}, "original_file_path": "../../datalake/Samuel/TS2023/TS2023_xml/hal-03629758.grobid.tei.xml", "file_name": "hal-03629758.grobid.tei.xml"}