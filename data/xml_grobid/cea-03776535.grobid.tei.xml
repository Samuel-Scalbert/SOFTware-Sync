<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Disentangled Loss for Low-Bit Quantization-Aware Training</title>
				<funder ref="#_k6ks32V">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_P5TPWeD">
					<orgName type="full">French Agence Nationale de la Recherche</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thibault</forename><surname>Allenet</surname></persName>
							<email>thibault.allenet@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">CEA-LIST</orgName>
								<address>
									<settlement>Saclay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Briand</surname></persName>
							<email>david.briand@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">CEA-LIST</orgName>
								<address>
									<settlement>Saclay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Bichler</surname></persName>
							<email>olivier.bichler@cea.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">CEA-LIST</orgName>
								<address>
									<settlement>Saclay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Olivier</forename><surname>Sentieys</surname></persName>
							<email>olivier.sentieys@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Disentangled Loss for Low-Bit Quantization-Aware Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8AD69AD940CE9A66F36E06AFF24C7C73</idno>
					<idno type="DOI">10.1109/CVPRW56347.2022.00315</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-12T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Quantization-Aware Training (QAT) has recently showed a lot of potential for low-bit settings in the context of image classification. Approaches based on QAT are using the Cross Entropy Loss function which is the reference loss function in this domain. We investigate quantization-aware training with disentangled loss functions. We qualify a loss to disentangle as it encourages the network output space to be easily discriminated with linear functions. We introduce a new method, Disentangled Loss Quantization Aware Training, as our tool to empirically demonstrate that the quantization procedure benefits from those loss functions. Results show that the proposed method substantially reduces the loss in top-1 accuracy for low-bit quantization on CIFAR10, CIFAR100 and ImageNet. Our best result brings the top-1 Accuracy of a Resnet-18 from 63.1% to 64.0% with binary weights and 2-bit activations when trained on ImageNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many deep learning advances rely on increasing the number of parameters and computation power to achieve better performance. Also, the interest of deploying deep neural networks on edge mushroomed in the past few years. Critical applications with real-time constraints such as memory, latency, energy/power consumption, with specific scarce resource hardware or with privacy issues, cannot be inferred on Cloud. In this context, low-bit quantization is an elegant solution to allow significant memory footprint reduction, energy savings, and faster inference once engineered with hardware accelerators, while preserving performance and quality of results as close as possible to the floating-point reference.</p><p>The latest proposals present approaches to quantization aware training, where networks trained and quantized from scratch showed promising results for settings from 8 bits down to 2 bits <ref type="bibr">[4,</ref><ref type="bibr">9]</ref>. Those methods rely on the Cross Entropy Loss (CEL) function, i.e., a combination of softmax and negative log likelihood, as it is the reference loss function for classification. A variation of the softmax was proposed by Liu et al. to encourage more discriminating features for image classification <ref type="bibr" target="#b12">[13]</ref>. This research led to disruptive performance gains, especially in the face recognition domain <ref type="bibr">[12,</ref><ref type="bibr" target="#b17">18]</ref>, where the number of classes is an order of magnitude higher than academic image classification tasks. Also, Wan et al. used Gaussian Mixtures to formalize the classification space and encourage more discriminating features <ref type="bibr" target="#b16">[17]</ref>.</p><p>To date, the effect of those loss functions on quantization-aware training (QAT) remains unexplored. Our paper studies the quantization aware learning with disentangled loss functions for settings down to binary weights. We empirically show that training a model to output discriminative features improves its resilience to quantization. Results on CIFAR10, CIFAR100 and ImageNet datasets show the clear advantage of our approach, with significant performance gains, especially for very low-bit settings.</p><p>This paper is organized as follows. Section 2 presents some previous work on QAT as well as the foundation of disentangled loss functions. Section 3 introduces our method that takes advantage of both AMS and GML to improve the QAT procedure. Section 4 presents our experimental setup and the results obtained on relevant datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous Work</head><p>To better understand the intuition behind our approach, we first give a brief review of the state-of-the-art techniques on quantization-aware training and disentangled losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Quantization Aware Training</head><p>Given a network f : R n ⇒ R with its parameters p, an input x ∈ R n and its corresponding label y, we refer to quantization aware training (QAT) for classification as finding the non-differentiable quantization function q with the loss function L as</p><formula xml:id="formula_0">min p L[f (x, q(p)), y].<label>(1)</label></formula><p>Bengio et al. proposed the Straight-Through-Estimator (STE) to enable training with backpropagation <ref type="bibr" target="#b0">[1]</ref>. The STE method estimates the gradients of the quantized parameters assuming that the derivative of the quantization function q is the identity function. Such approximation error grows bigger as the bitwidth goes smaller hence decreasing the performance for low-bit settings. Esser et al. tackled this issue by scaling dynamically the gradients with a learnable step <ref type="bibr">[4]</ref>. Following their method, the gradient landscape is shaped to encourage the full precision parameters towards the quantized points. Doing so, the proposed Learned Step Size Quantization (LSQ) method implicitly reduces the approximation error introduced by the STE and shows substantially better results over the previous quantization techniques. Alternatively, the Scaled Adjust Training (SAT) method introduced by Jin et al. directly scales the weights instead of the gradients to control the training dynamics, which yields state-of-the-art results <ref type="bibr">[9]</ref>. We refer the interested readers to [9] for a detailed presentation of the quantization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Disentangled Losses</head><p>We qualify a loss to disentangle as it encourages the network output space to be easily discriminated with linear functions. Inspired by Large-Margin Softmax <ref type="bibr" target="#b12">[13]</ref> and Sphereface <ref type="bibr">[12]</ref>, Wang et al. proposed an intuitive formulation of the margin softmax loss function called Additive Margin Softmax (AMS) <ref type="bibr" target="#b17">[18]</ref>. The authors considered the propagation of features f i (from the i-th sample with target y i ) in the linear layer without bias as scalar products for each column j of the weight matrix W . They used the geometric definition of the scalar product of Eq. (2), coupled with feature and weight normalization to rewrite the loss function applying a margin m on the target logit W T yi f i and a scaling factor s, following Eq. (3).</p><formula xml:id="formula_1">f i • W j = ∥W j ∥∥f i ∥cos(θ j )<label>(2)</label></formula><formula xml:id="formula_2">L AM S = - 1 n n i=0 log e s•(cos θy i -m) e s•(cos θy i -m) + c j=1, j̸ =yi e s•(cos θj )</formula><p>(3) The softmax output probabilities can be interpreted as a vector of dimension n, n being the number of classes. The onehot vectors encoding the different classes are the orthogonal vectors that construct the canonical basis of R n . Here, the subtracted margin m acts as a classification boundary offset, forcing the network to output features that are closer to the orthogonal vector corresponding to their label, thus reducing the intra-class variance of each class cluster in the network.</p><p>Wan et al. proposed to model the classification layer with Gaussian mixtures <ref type="bibr" target="#b16">[17]</ref>. The Gaussian Mixture Loss (GML) draws the distances d k between features f and the learned means µ k to minimize the distance to the mean associated to the true label d zi . A positive margin factor α artificially inflates the distance d zi to help regulate the convergence of the network. Under the assumption that the covariance matrix is isotropic, the GML can be rewritten as</p><formula xml:id="formula_3">L GM = - 1 n n i=0 log e -dz i (1+α) e -dz i (1+α) + k=1, k̸ =zi e -d k (4) with d k = 1 2 (f -µ k ) 2<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Disentangled Loss Quantization Aware Training</head><p>Considering that features can be more discriminative than with CEL, we assume that low-bit quantization-aware training can benefit from a disentangled loss. Indeed, a smaller intra-class variance and a bigger inter-class difference should be more robust to the quantization noise. With CEL, the inter-class features are optimized to be orthogonal without constraint on their actual distance in the output space. While it is also true for AMS, it still allows for an additional margin on the orthogonality. On contrary, GML directly minimizes the distance between the features and their corresponding centroids, thus, minimizing the intraclass variance. The use of learned centroids instead of orthogonal features ensures that the distance between interclass features is constrained by the distance of their respective centroids, as the features are attracted to their corresponding centroids. To reformulate, while AMS loss encourages a smaller intra-class variance than CEL, GML ensures both a smaller intra-class variance and a bigger interclass difference than CEL. This is why our hypothesis is that there is a possibility to investigate the combination of several state-of-the-art methods: the presented disentangled loss functions with the SAT procedure <ref type="bibr">[9]</ref>. In order to assess our hypothesis, we introduce Disentangled Loss Quantization Aware Training (DL-QAT), a method applying the intuitive formulation of AMS or GML loss function with the quantization-aware training method SAT [9].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training setups</head><p>All experiments use a Resnet-18 <ref type="bibr">[7]</ref> with the CIFAR10, CIFAR100 <ref type="bibr" target="#b9">[10]</ref> and ILSVRC 2012 ImageNet dataset [3]. The batch size is 768 for CIFAR and 1024 for ImageNet. We use the same learning strategy as <ref type="bibr">[9]</ref>. When training on CIFAR, the learning rates are 0.01 for SAT using CEL &amp; DL-QAT using AMS loss and 0.2 for DL-QAT using GML. When training on ImagNet, the learning rate is 0.02 for both SAT using CEL and DL-QAT using GML. All networks are trained over 150 epochs. Finally, we use m = 0.35 from Eq. (3) and α = 0.7 from Eq. (4) for CIFAR and α = 0 for ImageNet as they give best results. As it is common practice in the previous quantization approaches [4, 9], the precision of filters from the first convolution, the weights of the last layer and the activation preceding the last layer are fixed to 8 bits. Also, all batch normalization layers and the bias in the linear layer are not quantized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results and analysis</head><p>To better visualize the contribution of the AMS loss and the GML during quantization, we performed dimension reduction with the t-sne algorithm <ref type="bibr" target="#b15">[16]</ref> over the input features of the linear classifier. The features fed to the t-sne algorithm are extracted from the converged Resnet-18 inferring with the same sets of 50 test images for each class. The 2D visualisations from full precision and 2-bit Resnet-18 for CEL, AMS loss and GML are plotted in Fig. <ref type="figure" target="#fig_0">1</ref>. As expected, the full precision Resnet-18 clusters with AMS loss (c) and GML (e) are more compact than with CEL (a). It is then manifest that separating the clusters thanks to straight lines modelled by the linear classifier will be made easier. Comparing full precision in Fig. <ref type="figure" target="#fig_0">1.(a-c-e</ref>) to 2-bit quantization in Fig. <ref type="figure" target="#fig_0">1.(b-d-f</ref>), the clusters with the quantized version are less compact, and we can interpret this as the effect of the quantization. Comparing Fig. <ref type="figure" target="#fig_0">1</ref>.(b) to (d) and (f), the plots show that the ambiguities caused by the quantization are reduced thanks to the disentangled losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">CIFAR10 &amp; CIFAR100</head><p>The top-1 test accuracies on CIFAR10 and CIFAR100 of the proposed DL-QAT method (Acc AM S ) and (Acc GM L ) compared to the SAT method (Acc CEL ) are reported in Tab. 1. The lines with 32 here corresponds to single-precision floating-point, which is considered as the full precision baseline. Tab. 1 also reports the ∆P loss quality metric to compare the QAT methods defined as ∆P loss measures the drop in top-1 accuracy between the full precision version and a quantized version of a network trained with the same loss function. Given ∆P GM L and ∆P CEL , we can better compare the quantization resilience between disentangled losses and CEL. One noticeable result is that Resnet-18 with binary weights and 2-bit activations trained with GML (71.3%) outperforms the full precision Resnet-18 trained with CEL (66.2%). We also want to emphasize that ∆P CEL &gt; ∆P AM S and ∆P CEL &gt; ∆P GM L for all settings. As the precision is reduced, the drop in top-   1 accuracy grows larger. Our approach especially well limits the drop in top-1 accuracy for low-bit settings. Hence, the discriminative features, enforced by the AMS loss or the GML, enable more resilient quantization-aware training than the CEL, especially for low-bit settings. Overall, a clear tendency appears where Acc CEL &lt; Acc AM S &lt; Acc GM L . Indeed, GML minimizes the intra-class variance and constraint the distances of inter-class features while the AMS loss only minimizes the intra-class variance. Those results confirms our hypothesis on the loss function that both intra-class variance and inter-class difference need to be constraint.</p><formula xml:id="formula_4">∆P loss = Acc f loat32 loss -Acc quant loss .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">ImageNet</head><p>In this section, we evaluate the performance of our method using the ImageNet dataset. Considering the results on CI-FAR and our hypothesis on the losses, we chose to focus on the GML for ImageNet experiments. We report the top-1 test accuracy on ImageNet of our method DL-QAT using the GML and the SAT method [9] using CEL and other state-of-the-art approaches in Tab. 2 for low-bit settings and in Tab. 3 for binary weights settings.</p><p>As we read Tab. 2 and Tab. 3 from left to right, the quantization is more and more aggressive. Considering our experimental results only (DL-QAT using GML and SAT using CEL), the gap between the disentangled loss GML and the CEL is getting bigger as the settings reach more extreme quantization. Ultimately, in the binary weights and 2-bit activation setting, our approach reaches an accuracy of 64, 0%, improving by 0.9% the CEL score of 63.1%.</p><p>When comparing our method to the other approaches, the version of Resnet-18 and the quantization method matter. Notably, the Resnet-18 results reported in Esser et al.</p><p>[4] use pre-activation quantization scaling and thus keep the residual connections in the same precision as the accumulation (i.e., 32 bits). While this significantly improves the final accuracy in low precision, the actual precision of the dataflow is not strictly the activation's precision. For this reason, we have chosen to keep Resnet-18 with postactivation for our experiments, which makes it however not fully comparable with the LSQ reported results. For 2bit weights, our method achieves substantial improvement over ABC-Net <ref type="bibr" target="#b10">[11]</ref> and INQ <ref type="bibr" target="#b21">[22]</ref>, while the setting is more constraining on the activations. One noticeable result over the binary weights experiments is that our method with 4bit activations reaches 67.2% and surpasses all other approaches with full precision or 8-bit activations. Looking at the stricter quantization setting with binary weights and 2-bit activations, our approach achieves the highest performance with 64% top-1 accuracy. Over all approaches, our method demonstrates the best performance on ImageNet for extreme quantization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we target very-low-bit settings and propose to study multiple losses to further reduce the gap in accuracy of quantization. We introduce DL-QAT, a method combining quantization-aware training and disentangled losses, as our tool to investigate the contribution of those different loss functions for extreme quantization. Preliminary experiments on CIFAR10 and CIFAR100 are conducted to visualise and lighten the advantage of our method. Further results on ImageNet show that our approach improves by nearly 1% the top-1 accuracy of Resnet-18 with binary weights and 2-bit activations. Overall, the experiments confirm our hypothesis and encourage future use and research of disentangled losses for Quantization Aware Training.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Dimension reduction with t-sne algorithm representing the input features of the linear classifier from CIFAR10 test data. The corresponding top-1 test accuracies are reported in Tab. 1. tsne performed over 1000 iterations and a perplexity of 30.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Top-1 Accuracy on vanilla Resnet-18</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>SAT [9]</cell><cell cols="2">DL-QAT (ours)</cell><cell>SAT [9]</cell><cell cols="2">DL-QAT (ours)</cell></row><row><cell>Dataset</cell><cell>W [bits]</cell><cell>A [bits]</cell><cell>Acc CEL</cell><cell>Acc AM S</cell><cell>Acc GM L</cell><cell>∆P CEL</cell><cell>∆P AM S</cell><cell>∆P GM L</cell></row><row><cell></cell><cell>32</cell><cell>32</cell><cell>89.4</cell><cell>91.7</cell><cell>93.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CIFAR10</cell><cell>2</cell><cell>2</cell><cell>76.5</cell><cell>89.1</cell><cell>91.3</cell><cell>12.9</cell><cell>2.6</cell><cell>1.7</cell></row><row><cell></cell><cell>binary</cell><cell>2</cell><cell>72.4</cell><cell>88.3</cell><cell>91.2</cell><cell>17.0</cell><cell>3.4</cell><cell>1.8</cell></row><row><cell></cell><cell>32</cell><cell>32</cell><cell>66.2</cell><cell>68.7</cell><cell>73.6</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>8</cell><cell>8</cell><cell>65.8</cell><cell>68.5</cell><cell>73.1</cell><cell>0.4</cell><cell>0.2</cell><cell>0.5</cell></row><row><cell></cell><cell>4</cell><cell>4</cell><cell>65.4</cell><cell>68.4</cell><cell>72.6</cell><cell>0.8</cell><cell>0.3</cell><cell>1.0</cell></row><row><cell></cell><cell>3</cell><cell>3</cell><cell>65.1</cell><cell>68.2</cell><cell>73.3</cell><cell>1.1</cell><cell>0.5</cell><cell>0.3</cell></row><row><cell>CIFAR100</cell><cell>2</cell><cell>2</cell><cell>61.1</cell><cell>66.1</cell><cell>71.9</cell><cell>5.1</cell><cell>2.6</cell><cell>1.7</cell></row><row><cell></cell><cell>binary</cell><cell>8</cell><cell>63.9</cell><cell>67.9</cell><cell>72.5</cell><cell>2.3</cell><cell>0.8</cell><cell>1.1</cell></row><row><cell></cell><cell>binary</cell><cell>4</cell><cell>63.2</cell><cell>67.1</cell><cell>72.5</cell><cell>3.0</cell><cell>1.6</cell><cell>1.1</cell></row><row><cell></cell><cell>binary</cell><cell>3</cell><cell>62.4</cell><cell>67.0</cell><cell>72.0</cell><cell>3.8</cell><cell>1.7</cell><cell>1.6</cell></row><row><cell></cell><cell>binary</cell><cell>2</cell><cell>59.0</cell><cell>65.5</cell><cell>71.3</cell><cell>7.2</cell><cell>3.2</cell><cell>2.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>For LSQ, the residual connections remain in the accumulation dynamic. SAT and DL-QAT results are obtained from our experiments, all the other results are reported from the original papers. DL-QAT, SAT → original Resnet-18. LSQ → full pre-activation Resnet-18.</figDesc><table><row><cell>Method</cell><cell>LSQ</cell><cell>HAWQ-</cell><cell>SAT</cell><cell>DL-QAT</cell><cell>ABC-</cell><cell>INQ</cell><cell>SAT</cell><cell>DL-QAT</cell><cell>LSQ</cell><cell>SAT</cell><cell>DL-QAT</cell></row><row><cell></cell><cell>[4]</cell><cell>V3 [20]</cell><cell>[9]</cell><cell>GML (ours)</cell><cell>Net [11]</cell><cell>[22]</cell><cell>[9]</cell><cell>GML (ours)</cell><cell>[4]</cell><cell>[9]</cell><cell>GML (ours)</cell></row><row><cell>W [bits]</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell>A [bits]</cell><cell cols="2">4/32 * 4</cell><cell>4</cell><cell>4</cell><cell>32</cell><cell>32</cell><cell>8</cell><cell>8</cell><cell cols="2">2/32 * 2</cell><cell>2</cell></row><row><cell>Top-1 Acc</cell><cell>71.1</cell><cell>68.5</cell><cell>70.0</cell><cell>70.1</cell><cell>63.7</cell><cell cols="2">66.0 67.4</cell><cell>67.9</cell><cell>67.6</cell><cell>63.1</cell><cell>64.0</cell></row></table><note><p>* </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>ImageNet Top-1 Accuracy for low-bit quantization settings of Resnet-18. SAT and DL-QAT results are obtained from our experiments, all the other results are reported from the original papers. DL-QAT, SAT → original Resnet-18. PACT → full pre-activation Resnet-18. LQ-Net → Resnet-18 type-A. BWN → Resnet-18 type-B.</figDesc><table><row><cell>Method</cell><cell>BWN</cell><cell>ABC-</cell><cell>BWNH</cell><cell>DSQ</cell><cell>Q-Nets</cell><cell>IR-Net</cell><cell>SYQ</cell><cell>PACT</cell><cell>LQ-</cell><cell></cell><cell>SAT [9]</cell><cell></cell><cell cols="3">DL-QAT GML (ours)</cell></row><row><cell></cell><cell>[15]</cell><cell>Net [11]</cell><cell>[8]</cell><cell>[6]</cell><cell>[19]</cell><cell>[14]</cell><cell>[5]</cell><cell>[2]</cell><cell>Net [21]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W [bits]</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>A [bits]</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>32</cell><cell>8</cell><cell>2</cell><cell>2</cell><cell>8</cell><cell>4</cell><cell>2</cell><cell>8</cell><cell>4</cell><cell>2</cell></row><row><cell>Top-1 Acc</cell><cell cols="2">60.8 62.8</cell><cell>64.3</cell><cell cols="2">63.7 66.5</cell><cell>66.5</cell><cell cols="2">62.9 62.9</cell><cell>62.6</cell><cell cols="6">67.5 66.9 63.1 67.5 67.2 64.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>ImageNet Top-1 Accuracy for binary weights settings of Resnet-18.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="funder">French Agence Nationale de la Recherche</rs> grant <rs type="grantNumber">AdequatDL</rs> <rs type="grantNumber">ANR-18-CE23-0012</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_P5TPWeD">
					<idno type="grant-number">AdequatDL</idno>
				</org>
				<org type="funding" xml:id="_k6ks32V">
					<idno type="grant-number">ANR-18-CE23-0012</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pact: Parameterized clipping activation for quantized neural networks</title>
		<author>
			<persName><forename type="first">Jungwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swagath</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Jen</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayalakshmi</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kailash</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><surname>Gopalakrishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06085</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learned step size quantization</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">L</forename><surname>Steven K Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepika</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rathinakumar</forename><surname>Bablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharmendra</forename><forename type="middle">S</forename><surname>Appuswamy</surname></persName>
		</author>
		<author>
			<persName><surname>Modha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Syq: Learning symmetric quantization for efficient deep neural networks</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Faraone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaela</forename><surname>Blott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip Hw</forename><surname>Leong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4300" to="4309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Differentiable soft quantization: Bridging full-precision and low-bit neural networks</title>
		<author>
			<persName><forename type="first">Ruihao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazhen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4852" to="4861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From hashing to cnns: Training binary weight networks via hashing</title>
		<author>
			<persName><forename type="first">Qinghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peisong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Towards efficient training for neural network quantization</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Liao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10207</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Towards accurate binary convolutional neural network</title>
		<author>
			<persName><forename type="first">Xiaofan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11294</idno>
		<idno>. 4</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Forward and backward information retention for accurate binary neural networks</title>
		<author>
			<persName><forename type="first">Ruihao</forename><surname>Haotong Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingkuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2250" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rethinking feature distribution for loss functions in image classification</title>
		<author>
			<persName><forename type="first">Weitao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiansheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haijun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quantization networks</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7308" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hawq-v3: Dyadic neural network quantization</title>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangcheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiali</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qijing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mahoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lq-nets: Learned quantization for highly accurate and compact deep neural networks</title>
		<author>
			<persName><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongqiangzi</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="365" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anbang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03044</idno>
		<title level="m">Incremental network quantization: Towards lossless cnns with low-precision weights</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
